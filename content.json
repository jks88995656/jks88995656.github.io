{"meta":{"title":"一只柴犬","subtitle":"","description":"","author":"凯凯超人","url":"http://example.com","root":"/"},"pages":[{"title":"","date":"2021-08-02T14:36:04.948Z","updated":"2021-08-02T14:36:04.948Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"同济子豪兄 之 yolov1 详解","slug":"同济子豪兄 之 yolov1 详解","date":"2021-09-26T14:01:01.000Z","updated":"2021-09-26T15:31:31.888Z","comments":true,"path":"2021/09/26/同济子豪兄 之 yolov1 详解/","link":"","permalink":"http://example.com/2021/09/26/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov1%20%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"@TOC yolo 是一个典型的 将 目标检测 转化为 回归问题的方法。yolo与其他网络不同在，他的测试与训练方法不同，所以接下来我们主要 分预测以及训练两个阶段去介绍yolov1。 包括 预测阶段（以及后处理），训练阶段来讲。 目标检测的基础知识目标检测是什么？在计算机视觉领域，图像任务主要分为：图像分类，图像检测 和 图像分割（语义分割和实例分割）等语义分割和实例分割的区别在于：语义分割是我对每个像素分类，我不管这个像素是属于哪几个物体的，只管他是属于什么类别的（也就是我只分类，同一个类别的不同实例不区分）；而实例分割是要把同一个类别的不同实例给区分开来 目标检测主流的数据集来源yolo 是在 PASCAL-2007 和 MS-COCO上做的评测 目标检测的发展目前这个主要的流派由两种，上面是单阶段模型 yolo 系列 下面是两阶段模型 RCNN系列。 两阶段就是先从图像中提取若干候选框，再逐一的对这些候选框进行分类、甄别以及调整它们的坐标最后得出结果。 单阶段就是 我不提取候选框，我直接把全图喂到算法里面，能直接输出出来目标检测的结果。是一个统一的端到端的系统 对于 RCNN系列的话，他比较慢但是 正确率比较高 R-CNN 使用region proposal先提取候选框，再使用卷积神经网络逐一的对每个候选款进行甄别，对bouningbox位置调整和回归和分类 Fast R -CNN 是把所有的图片用卷积神经网络过一遍，在生成的feature map上找候选框 投影到feature map上面，再进行甄别 Faster R-CNN 使用了 RPN 网络，也就是找候选框这个事情由 RPN 这个专业户干了 对于 yolo系列的话，他的优势在于速度，正确率相比没有那么高（待补充） yolov1 的缺点在于，单个grid cell 只能识别一种类别。识别小目标或者密集目标 能力不足，例如羊群、人群这种。 yolov2 是在 yolov1 的基础上 yolov3 yolov4 从v4开始 作者变了，前面的大作因为感慨yolo技术被用来干坏事而退出了计算机视觉领域。 yolov5 yolo v1 的 预测阶段yolo v1 网络架构输出的 7×7×30 这个tensor 就是我们预测阶段所需要的 yolo v1 预测阶段 内容详解 Q：为什么 输出的是 7×7×30 呢？因为在 yolo v1 中图像被划分成了7×7的网格。每个格子叫做 grid cell ，也就是由49个grid cell。 这里的 每个 grid cell 又能预测 B个 bounding box （也就是预测框） yolo v1中 B=2 也就是每个 grid cell 可以预测 2个 bounding box。这两个预测框可能很大也可能很小 也就是这个框是啥样的不一定。（这里我有个问题，就是这个bounding box 是怎么生成的呢？）。这个框覆盖其他的grid cell 是很正常的，只要这个 bounding box 的中心点 是落在这个 grid cell 里就ok。 每个grid cell 预测 B 个 bounding box。 在yolo v1 的实验中 B=2，也就是 每个 grid cell 由 2 个 bounding box。bounding box 的由 5个 数值来表示 （x,y,h,w,c） x，y 表示中心点的位置 h，w表示这个 bounding box 的 高 和 宽 c 表示这个 bounding box 框的置信度。在图像中 一般用 框线的粗细来表示置信度的大小。 在yolo v1的实验中，因为由 49 个 grid cell，所以 有 98 个 bounding box 如下图所示（粗细表示置信度高低） grid cell 输出所有类别的条件概率每个 grid cell 还能生成所有类别的 条件概率，并且选择最高的一个概率表明，预测的是这个类别。这也就是 yolo v1 最大的弊端，每个 grid cell 只能预测一个类别，那一个 49个cell 最大只能预测 49个类别；并且如果类别对象很密集，一个 grid cell 中有多个不同的小对象的话，识别效果会很差。 注意这里的概率是 条件概率 比如 p(cat|object) 是在有存在类别对象的情况下 是 猫的概率。下图可以表示 49个cell 预测的类别情况（也就是各cell 选择的最高类别的 条件概率） 7×7×30 这个30是怎么来的呢每个grid cell 有 2个bounding box。1个bounding box 中有5个数值 那就是10个数值，然后 yolo v1 预测了20个类别的物体，所以有20个类别的概率 所以是 30个输出。然后有 7×7个 grid cell 所以 输出的 是 7×7×30 的 tensor。 Q：那 类别真正的概率怎么算出来呢？只有将 对应的 bounding box 置信度 这个grid cell 的类别条件概率才是 这个类别真正的概率值 将这个grid cell 的两个 bouding box 都赋予 这个grid cell 选择的最高类别。再进行一系列的后处理（指的是 置信度很低的先给过滤掉，非极大值抑制（NMS））选择最佳 bounding box 7×7×30这个tensor中包含了 98个bounding box 49个grid cell 每个grid cell 的类别 bounding box的 5个数值 进行解析和后处理 最后得到了结果以上我们说的都是 预测阶段 也就是 参数啥的已经调好了 我只要跑一跑 拿到个结果。 预测阶段 的后处理部分一个 grid cell 可以得到 下图右侧中两列向量。每个向量 都表示 这个 grid cell 在其一个 bounding box 中 20个类别的全概率。一个98个bounding box。 这个是由 每个bounding box 的置信度 乘以 tensor 中 输出的 该 grid cell 对20个类别的条件概率得到的。 现在我们得到如下的框框图像，一共有 98个 bounding box。不同颜色表示不同类别。 我们把刚才 每个bounding box 输出的 20个分类的全概率向量拿出来。 假如第一行的是对狗的预测。整个的过程如下图所示。 NMS方法详解假如 我们比较下图中98个bounding box 狗这一行 的 前两个值， 也就是左边橙色和绿色连的bounding box ，我们计算一下 IoU 这边要预先设定一个门槛值（他这里是0.5，越小排异性越高），如果IoU超过0.5的话，表明这两个框预测的都是同一类别（这里也就是狗）。这样的话，保留值高的那个，低的那个全概率改为0，把这个bounding box 干掉。 下图这个就是 IoU不到门槛值 说明两个 bounding box 预测的不是一个类别 两者都保留不动。 然后第一个和所有比完了 再从剩下里面最高的 再比 找出 对应框的索引. 注意奥 我们讲的是只是对预测阶段 需要 。在训练阶段是不需要NMS的。 因为每个框不管他是要被打入冷宫的还是负责预测物体的有用框 都要在损失函数中占据一席之地。里面的所有框的一举一动都会影响损失函数。所以不能随随便便在训练阶段 用NMS 把没用的框去掉，或者把概率抹零。 训练阶段首先奥这是个典型的 监督学习。所以我们是需要 ground truth。比如下图就是一个正确的标签： 这里有个问题 每个grid cell 都有两个 bounding box 谁来 拟合 ground truth 做损失函数呢? 答：看谁和 ground truth的 IoU 更大。比如这里 就是由 外面这个大框 负责拟合 ground truth，让其尽量的逼近 调整成 ground truth 的样子，那么另外一个框就被打入冷宫了 ，它什么都不用做，尽量让他置信度降为0。 如果没有ground truth 中心点落下 的 grid cell 的两个 bounding box 全部打入冷宫。 yolo v1 的 损失函数 (待补充与推理)下面介绍一下 yolov1损失函数误差。主要分为5个部分。 参考文献牛逼的子豪兄 yolo v1详解yolo 损失函数的详解","categories":[{"name":"深度学习基础","slug":"深度学习基础","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"yolo","slug":"yolo","permalink":"http://example.com/tags/yolo/"}]},{"title":"吴恩达deeplearning.ai学习  之 目标检测","slug":"吴恩达deeplearning.ai学习  之 目标检测","date":"2021-09-25T08:37:01.000Z","updated":"2021-09-25T08:38:28.991Z","comments":true,"path":"2021/09/25/吴恩达deeplearning.ai学习  之 目标检测/","link":"","permalink":"http://example.com/2021/09/25/%E5%90%B4%E6%81%A9%E8%BE%BEdeeplearning.ai%E5%AD%A6%E4%B9%A0%20%20%E4%B9%8B%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/","excerpt":"","text":"@TOC 目标定位我们对图像的学习过程应该由 图像分类 -&gt; 图像单目标定位 -&gt; 图像多目标检测。 图像分类在图像分类中，例如，输入一张图片到多层卷积神经网络，它会输出一个特征向量，并反馈给 softmax 单元来预测图片类型。比如你正在构建汽车自动驾驶系统，你面前的对象可能包括以下4类：行人、汽车、摩托车和背景，这时 softmax 单元有四个输出（概率高的为判断的类）。如果图片里没有前三类的话，输出结果会是背景。 图像分类基础上 发展图像内的单目标定位在图像分类的基础上，我们可以对图像内的单目标定位（注意：这边的基础是图像里只有一个目标可以被检测，例如上图的车）。实现定位，需要让神经网络多输出4个数字，标记为$b{x},b{y},b{h},b{w}$用于表示输出的边界框。前两者表示边界框的中心点，后两者表示的为边界框的高度和宽度。注意：我们定义左上角为坐标原点(0,0)，右下角为终点(1,1) 例如下图所示：还是这个例子，神经网络的输出改变为四个边界框描述数字$b{x},b{y},b{h},b{w}$ ，是否含有对象 $p{c}$ ，含有对象属于哪个类标签。可以定义输出 $y$ 为 $[p{c},b{x},b{y},b{h},b{w},c{1},c{2},c_{3}]^{T}$ 注意我们此时假设的图片中只含有一个对象，是单目标的分类定位问题。 损失函数神经网络的损失函数，其参数为类别$\\hat{y}$ 和网络输出 $y$ ，如果采用MSE损失函数，则 L(\\hat{y},y)==(\\hat{y_{1}} - y_{1})^{2}+(\\hat{y_{2}} - y_{2})^{2}+...+(\\hat{y_{8}} - y_{8})^{2}损失值等于每个元素相应差值的平方和。​ 如果图片内有对象（即 $p_{c}=1$），损失值就是不同元素的平方和 如果图片内没有对象（即 $p{c}=0$），我们不用考虑其它元素，只需要关注神经网络输出 $p{c}$ 的准确度。 图像内的单目标定位基础上 发展多目标定位yolo 将图像拆分成多个子图 特征点检测（Landmark detection）例如我想做人脸的情感识别，可以根据眼睛的特征点，根据嘴部的关键点输出值来确定嘴的形状，从而判断人物是在微笑还是皱眉，也可以提取鼻子周围的关键特征点。为了便于说明，你可以设定特征点的个数，假设脸部有64个特征点，有些点甚至可以帮助你定义脸部轮廓或下颌轮廓。选定特征点个数，并生成包含这些特征点的标签训练集，然后利用神经网络输出脸部关键特征点的位置。具体的想法是，准备一个卷积网络和一些特征点集，将人脸图片输入到卷积网络，输出1或0（1表示有人脸，0表示没有人脸），然后输出 64个特征点坐标位置 $(l{1x},l{1y})$ … $(l{64x},l{64y})$。理论上应该有129个输出（64个特征点 64×2+ 1（表示是否有人脸））。另外在人体姿态检测上也可以使用这样的特征点方法。 Q：注意点 特征点1的特性在所有图片中必须保持一致特征点1始终是右眼的外眼角，特征点2是右眼的内眼角，特征点3是左眼内眼角，特征点4是左眼外眼角等等。所以标签在所有图片中必须保持一致，假如你雇用他人或自己标记了一个足够大的数据集，那么神经网络便可以输出上述所有特征点，你可以利用它们实现其他有趣的效果，比如判断人物的动作姿态，识别图片中的人物表情等等。 滑动窗口目标检测法 （复杂过时）这个方法简单的说就两步 先构建一个卷积神经网络，能够识别出这个对象 滑动窗口（每次扩大窗口），选定合适的步长，从左上角开始裁剪，将其输入该卷积网络，输出是否为该对象。这样从左上角开始滑动窗口遍历整个图像。这个方法有很明显的缺点就是。如果步长太大，误差会很大，不够精细。单如果步长太精细的话，因为我每滑动一次窗口即裁剪一次图像就要输入卷积网络输出是否是车。卷积网络时间代价很高，这么多次输入，那这个方法需要的时间成本就非常的高。 Q：那为什么这个办法之前是可用的？因为之前神经网络还没兴起，每个裁剪对象判断都是由SVM来完成的。线性分类器速度比较快一些。 解决的办法是不要使用滑动窗口，使用卷积来实现滑动窗口的效果。详见下章节。 滑动窗口的卷积实现（Convolutional implementation of sliding windows）首先你要了解，如何将全神经网络转化为用卷积网络来实现，如下图：那我们现在来看 为什么要可以使用卷积来简化滑动窗口呢？如上图所示，假如输入给卷积网络的图像大小为14×14×3，测试集图片是16×16×3。现在给这个输入图片加上黄色条块（padding = 2），在最初的滑动窗口算法中，设置窗口大小为14×14，步长为2，可以滑出4个窗口，对应输入到卷积网络中，输出4个标签。 可以发现，这4次卷积操作中由很多像素点的计算都是重复的。 所以执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算。如果我们使用如上的卷积方式的话，最后输出的4个方块，刚好就对应我们的4个窗口。如上面绿色的小框框，假设你剪切出这块区域（编号1），传递给卷积网络，第一层的激活值就是这块区域（编号2），最大池化后的下一层的激活值是这块区域（编号3），这块区域对应着后面几层输出的右上角方块（编号4，5，6）。但是正如在前一页所看到的，我们不能依靠连续的卷积操作来识别图片中的汽车，比如，我们可以对大小为28×28的整张图片进行卷积操作，一次得到所有预测值，如果足够幸运，神经网络便可以识别出汽车的位置。28×28的图像按照 图像14×14的来划定窗口，那么将有 64个窗口（行列均为8 8×8）所以最后输出的也是个 8×8。说白了这个办法，就是用卷积，将最后的输出每个格子表示当初划定的一个窗口。这个算法，效率是提高了，但是仍然存在一个缺点，就是边界框的位置可能不够准确。可以Bounding Box预测解决，详见下一章。 Bounding Box预测（Bounding box predictions）如何能精准边界框呢。比较出名的一个算法就是 Yolo 算法。Yolo 算法是这样的，简单的说就是 （如下图）比如你输入的图像是100×100的，然后在图像上放一个3×3网格（自己定应该更精细一点）。一共9个网格，每个网格都采取我们之前说的图像分类再定位的算法。 对于9个格子中的每一个指定一个标签 $y$ ，$y$ 是8维的。$y=[p{c},b{x},b{y},b{h},b{w},c{1},c{2},c{3}]^{T}$。 看这个九宫格的第一个格子，里面没有检测的目标（也就是只有背景），所以左上格子的标签向量 $y=[0,?,?,?,?,?,?,?]^{T}$，同样右边的两个格子也是一样的。 再看第二层，有两辆车，也就是有2个对象。YOLO算法做的就是，取两个对象的中点，然后将这个对象分配给包含对象中点的格子。所以左边的汽车就分配到左边这个格子上（编号4），然后这辆长条车中点在这里，分配给右侧这个格子（编号6）。所以即使中心格子（编号5）同时有两辆车的一部分，我们就假装中心格子没有任何我们感兴趣的对象，所以对于中心格子，的输出标签向量 就是$y=[0,?,?,?,?,?,?,?]^{T}$。而左右编号为4和6的格子，输出标签向量均为$y=[1,b{x},b{y},b{h},b{w},0,1,0]^{T}$ 由此得到，最后这张图片的目标输出为 3×3×8 （因为这里有3×3格子，然后对于每个格子，你都有一个8维向量，所以目标输出尺寸是3×3×8。） 怎么得到框的精确位置呢？因为我们是将每个样本格子都作为考虑对象，所以每个格子的左上角均为（0，0），右下角为（1，1）。如下图所示，右侧长条车为栗子： 可以看到橙色为对象的中点处，而后$b{x},b{y},b{h},b{w}$单位是相对于格子尺寸的比例，所以$b{x},b{y}$必须在0和1之间；$b{h},b{w}$ 可能会大于1，特别是如果有一辆汽车的边界框是这样的（左下大红框），那么边界框的宽度和高度有可能大于1。 交并比（Intersection over union）业界都用交并比（IOU）来衡量 框框打的对不对。如下图所示： IOU = \\frac{∩ size}{∪size}一般规定≥0.5即可，这个是人为定的，你也可以严格一点，选0.6及其以上。 非极大值抑制（NMS）减少同一对象的 多个检测结果到目前为止你们学到的对象检测中的一个问题是，你的算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。 非极大值抑制这个方法可以确保你的算法对每个对象只检测一次，我们讲一个例子。 就比如上面这个图。在上面放个19×19网格，理论上这辆车只有一个中点，所以它应该只被分配到一个格子里，左边的车子也只有一个中点，所以理论上应该只有一个格子做出有车的预测。实践中当你运行对象分类和定位算法时，对于每个格子都运行一次，所以这个格子（编号1）可能会认为这辆车中点应该在格子内部，这几个格子（编号2、3）也会这么认为。对于左边的车子也一样，所以不仅仅是这个格子，如果这是你们以前见过的图像，不仅这个格子（编号4）会认为它里面有车，也许这个格子（编号5）和这个格子（编号6）也会，也许其他格子也会这么认为，觉得它们格子内有车。 我们分步介绍一下非极大抑制是怎么起效的，因为你要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的 $p_{c}$ ，我这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。所以当你运行算法的时候，最后可能会对同一个对象做出多次检测，所以非极大值抑制做的就是清理这些检测结果。这样一辆车只检测一次，而不是每辆车都触发多次检测。 所以具体上，这个算法做的是，首先看看每次报告每个检测结果相关的概率$p{c}$，实际上$p{c}$ 是 $p{c}=1$ 乘以 $c{1}、c{2}$或 $c{3}$得到的。首先看概率最大的那个，这个例子（右边车辆）中是0.9，然后就说这是最可靠的检测，所以我们就用高亮标记，就说我这里找到了一辆车。这么做之后，非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边框有很高交并比，高度重叠的其他边界框，那么这些输出就会被抑制。因为其他两个矩形 $p_{c}$ 分别是0.6和0.7，这两个矩形和 0.9矩形重叠程度很高，所以会被抑制而变暗。接下来，逐一审视剩下的矩形，找出概率最高，最高的一个，在这种情况下是0.8，我们就认为这里检测出一辆车（左边车辆），然后非极大值抑制算法就会去掉其他 loU 值很高的矩形。所以现在每个矩形都会被高亮显示或者变暗，如果你直接抛弃变暗的矩形，那就剩下高亮显示的那些，这就是最后得到的两个预测结果。 简单的说就是：先消除$p_{c} ≤ 0.6$的框，当多个检测框重叠面积IOU占据最大框面积的比例超过了这个设定的非最大值抑制这个值的时候，那么就只保留置信度（概率）最高的那个框，冗余的框都去掉。 Anchor Boxes为什么要使用 anchor box到目前为止，对象检测中存在一个问题就是每个格子只能预测一个对象，如果想让一个格子检测出多个对象，你可以这么做，就是使用anchor box这个概念。 假设我们有这样一张图片，对于这个例子，我们使用3x3的网格，可以观察到，行人和汽车的中心几乎在同一个网格，然而我们以前的方法一个格子只能预测一个对象，而且对于 y 输出的向量 $y=[p{c},b{x},b{y},b{h},b{w},c{1},c{2},c{3}]^{T}$ ，你可以检测这三个类别，行人、汽车和摩托车，它将无法输出检测结果，所以我必须从两个检测结果中选一个，这便影响了模型性能，导致一些对象被丢弃无法检测出来。 anchor box 的引入和使用我们按以下图片的方式，重新定义输出即可：（右边的话是因为 anchor box2 的IoU更高）所以，总的来说，anchor box是这么来做的，现在每个对象和以前一样根据中心点分配到一个格子中，然后看和每个anchor box的IoU（交并比），选择IoU最高的那个，用这个anchor box来进行预测。 如何选择 anchor box？ 一般手工指定anchor box形状，根据要检测的对象，指定有针对性地anchor box，可选择5-10个anchor box，使其尽可能覆盖到不同形状。 使用K-means聚类算法获得anchor box。 anchor box + NMS 的总过程","categories":[{"name":"深度学习基础","slug":"深度学习基础","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"argparse模块用法实例详解","slug":"argparse模块用法实例详解","date":"2021-09-22T15:31:01.000Z","updated":"2021-09-23T05:13:24.980Z","comments":true,"path":"2021/09/22/argparse模块用法实例详解/","link":"","permalink":"http://example.com/2021/09/22/argparse%E6%A8%A1%E5%9D%97%E7%94%A8%E6%B3%95%E5%AE%9E%E4%BE%8B%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"@TOC argparse是python的命令行解析的标准模块，内置于python，不需要安装。这个库可以让我们直接在命令行（这边指的是 python的命令行 或者是 Anaconda Prompt）中就可以向程序中传入参数并让程序运行。其实 argparse 就是一个键值对存储的方式。 栗子一：传入一个参数并输出新建一个文件（如叫：arg_study），在该文件夹中新建一个python文件（如：demo.py） 1234567891011import argparseparser = argparse.ArgumentParser(description=&#x27;命令行中传入一个数字&#x27;)# type是要传入的参数的数据类型 help是该参数的提示信息# integers 相当于键parser.add_argument(&#x27;integers&#x27;, type=str, help=&#x27;传入的数字&#x27;)args = parser.parse_args()#获得传入的参数print(args) 查看帮助提示命令行中输入python demo.py -h或者 python demo.py --help 12python demo.py -hpython demo.py --help 运行结果如下： 123456789usage: demo.py [-h] integers命令行中传入数字positional arguments: integers 传入的数字optional arguments: -h, --help show this help message and exit 输入参数并输出如输入51python demo.py 5得到的结果print(args)为 1Namespace(integers=&#x27;5&#x27;) 如何获取其中的数据呢？Namespace(integers=&#39;5&#39;) 其实是一个类似于python字典的数据类型。我们可以是哟个 arg.参数名 来提取这个参数 12#获得integers参数print(args.integers) 栗子二：传入多个参数并输出nargs是用来说明传入的参数个数，’+’ 表示传入至少一个参数。这时候再重新在命令行中运行。 12345678import argparseparser = argparse.ArgumentParser(description=&#x27;命令行中传入一个数字&#x27;)# nargs是用来说明传入的参数个数，&#x27;+&#x27; 表示传入至少一个参数。parser.add_argument(&#x27;integers&#x27;, type=str, nargs=&#x27;+&#x27;,help=&#x27;传入的数字&#x27;)args = parser.parse_args()print(args.integers) 这时候再重新在命令行中运行python demo.py 1 2 3 4得到1[&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;] 栗子三：改变数据类型add_argument中有type参数可以设置传入参数的数据类型。我们看到代码中有type这个关键词，该关键词可以传入list, str, tuple, set, dict等。例如我们把上面的type=str，改成type=int,这时候我们就可以进行四则运算。 12345678import argparseparser = argparse.ArgumentParser(description=&#x27;命令行中传入一个数字&#x27;)parser.add_argument(&#x27;integers&#x27;, type=int, nargs=&#x27;+&#x27;,help=&#x27;传入的数字&#x27;)args = parser.parse_args()#对传入的数据进行加总print(sum(args.integers)) 在命令行中输入 python demo.py 1 2 3 4, 运行结果为 110 栗子四：位置参数在命令行中传入参数时候，传入的参数的先后顺序不同，运行结果往往会不同，这是因为采用了位置参数,例如： 123456789import argparseparser = argparse.ArgumentParser(description=&#x27;姓名&#x27;)parser.add_argument(&#x27;param1&#x27;, type=str,help=&#x27;姓&#x27;)parser.add_argument(&#x27;param2&#x27;, type=str,help=&#x27;名&#x27;)args = parser.parse_args()#打印姓名print(args.param1+args.param2) 输出 张三 ：在命令行中分别输入 1python demo.py 张 三 使用可选参数为了在命令行中避免上述位置参数的bug（容易忘了顺序），可以使用可选参数，这个有点像关键词传参，但是需要在关键词前面加—，例如： 123456789import argparseparser = argparse.ArgumentParser(description=&#x27;姓名&#x27;)parser.add_argument(&#x27;--family&#x27;, type=str,help=&#x27;姓&#x27;)parser.add_argument(&#x27;--name&#x27;, type=str,help=&#x27;名&#x27;)args = parser.parse_args()#打印姓名print(args.family+args.name) 在命令行中输入： 1python demo.py --family=张 --name=三 结果为 张三 。可选参数虽然写法比较繁琐，但是增加了命令行中的可读性，不容易因为参数传入顺序导致数据错乱。 设置默认值add_argument中有一个default参数。有的时候需要对某个参数设置默认值，即如果命令行中没有传入该参数的值，程序使用默认值。如果命令行传入该参数，则程序使用传入的值。具体请看下面的例子： 123456789import argparseparser = argparse.ArgumentParser(description=&#x27;姓名&#x27;)parser.add_argument(&#x27;--family&#x27;, type=str, default=&#x27;张&#x27;,help=&#x27;姓&#x27;)parser.add_argument(&#x27;--name&#x27;, type=str, default=&#x27;三&#x27;, help=&#x27;名&#x27;)args = parser.parse_args()#打印姓名print(args.family+args.name) 设置该参数一定要传入add_argument有一个required参数可以设置该参数是否必需。 12345678910import argparseparser = argparse.ArgumentParser(description=&#x27;姓名&#x27;)parser.add_argument(&#x27;--family&#x27;, type=str, help=&#x27;姓&#x27;)# name 必须传入parser.add_argument(&#x27;--name&#x27;, type=str, required=True, default=&#x27;&#x27;, help=&#x27;名&#x27;)args = parser.parse_args()#打印姓名print(args.family+args.name) 参考博客argparse模块用法实例详解 【非常的详细】python中argparse模块用法实例详解 【比较粗糙】","categories":[{"name":"python工具类","slug":"python工具类","permalink":"http://example.com/categories/python%E5%B7%A5%E5%85%B7%E7%B1%BB/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"argparse","slug":"argparse","permalink":"http://example.com/tags/argparse/"}]},{"title":"Pytorch GoogleNet中的Inception","slug":"Pytorch GoogleNet中的Inception","date":"2021-09-21T15:40:01.000Z","updated":"2021-09-21T15:24:29.376Z","comments":true,"path":"2021/09/21/Pytorch GoogleNet中的Inception/","link":"","permalink":"http://example.com/2021/09/21/Pytorch%20GoogleNet%E4%B8%AD%E7%9A%84Inception/","excerpt":"","text":"@TOC GoogleNet 的概念是基于AlexNet，VGG 后的模型 特殊点：Inception为什么要提出 Inception一般来说，提升网络性能最直接的办法是增加网络深度和宽度，但一味地增加，会带来诸多问题： 参数太多，如果训练数据集有限，很容易产生过拟合； 网络越大、参数越多，计算复杂度越大，难以应用； 网络越深，容易出现梯度消失问题（梯度越往后穿越容易消失），难以优化模型。 梯度消失和梯度爆炸 是什么？查看文章 如何理解梯度消失和梯度爆炸以及本博客文章 梯度消失和梯度爆炸的理解 我们希望在增加网络深度和宽度的同时减少参数，为了减少参数，自然就想到将全连接变成稀疏连接。但是在实现上，全连接变成稀疏连接后实际计算量并不会有质的提升，因为大部分硬件是针对密集矩阵计算优化的，稀疏矩阵虽然数据量少，但是计算所消耗的时间却很难减少。在这种需求和形势下，Google研究人员提出了Inception的方法。 Inception 模块的结构Inception 模块 是GoogleNet 重复使用的重要部分。其最大的特点是 引入了 1×1 的卷积核。其目的是用于 缩小通道数，将像素信息融合，也叫做 通道压缩。同时其可以减少卷积核的参数数量 例如:如下的操作数对比（28×28表示 卷积的时候图片像素点也要乘的啊）Inception 模块的内容如下图所示： Pytorch 实现 Inception 模块下图中为每个部分的代码模块。 每个部分的 上侧是 pytorch中网络初始化部分，下侧是 pytorch中网络前馈实现的部分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445import torchimport torch.nn.functional as Fclass InceptionA(torch.nn.Module): def __init__(self, in_channels): super(InceptionA, self).__init__() # 第一个部分 self.branch_pool = torch.nn.Conv2d(in_channels, 24, kernel_size=1) # 第二个部分 self.branch1x1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1) # 第三个部分 self.branch5x5_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1) self.branch5x5_2 = torch.nn.Conv2d(16, 24, kernel_size=5, padding=2) # 第四个部分 self.branch3x3_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1) self.branch3x3_2 = torch.nn.Conv2d(16, 24, kernel_size=3, padding=1) self.branch3x3_3 = torch.nn.Conv2d(24, 24, kernel_size=3, padding=1) def forward(self, x): # 第一个部分 branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1) branch_pool = self.branch_pool(branch_pool) # 第二个部分 branch1x1 = self.branch1x1(x) # 第三个部分 branch5x5 = self.branch5x5_1(x) branch5x5 = self.branch5x5_2(branch5x5) # 第四个部分 branch3x3 = self.branch3x3_1(x) branch3x3 = self.branch3x3_2(branch3x3) branch3x3 = self.branch3x3_3(branch3x3) # 通道整合 outputs = [branch_pool, branch1x1, branch5x5, branch3x3] # 整合通道 通道的位置在1处 （batch,通道,宽度,长度） return torch.cat(outputs, dim=1) GoogleNet 网络的整体模型 使用Mnist 使用Inception构建网络123456789101112131415161718192021class Net(torch.Module): def __init__(self): super(Net, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5) self.conv2 = torch.nn.Conv2d(88, 20, kernel_size=5) self.incep1 = InceptionA(in_channels=10) self.incep2 = InceptionA(in_channels=20) self.mp = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(1408, 10) def forward(self, x): in_size = x.size(0) x = F.relu(self.mp(self.conv1(x))) x = self.incep1(x) x = F.relu(self.mp(self.conv2(x))) x = self.incep2(x) # 变成列向量 x = x.view(in_size, -1) x = self.fc(x) return x Mnist 数据集 训练 整体代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143import torchimport torch.nn.functional as Ffrom torchvision import transformsfrom torchvision import datasetsfrom torch.utils.data import DataLoaderimport torch.optim as optim# 第一步 准备数据batch_size = 64transform = transforms.Compose([ transforms.ToTensor(), # 用均值和方差进行归一化 transforms.Normalize((0.1307,), (0.3081,))])train_dataset = datasets.MNIST(root=&#x27;../dataset/mnist/&#x27;, train=True, download=True, transform=transform)test_dataset = datasets.MNIST(root=&#x27;../dataset/mnist/&#x27;, train=False, download=True, transform=transform)train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)class InceptionA(torch.nn.Module): def __init__(self, in_channels): super(InceptionA, self).__init__() # 第一个部分 self.branch_pool = torch.nn.Conv2d(in_channels, 24, kernel_size=1) # 第二个部分 self.branch1x1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1) # 第三个部分 self.branch5x5_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1) self.branch5x5_2 = torch.nn.Conv2d(16, 24, kernel_size=5, padding=2) # 第四个部分 self.branch3x3_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1) self.branch3x3_2 = torch.nn.Conv2d(16, 24, kernel_size=3, padding=1) self.branch3x3_3 = torch.nn.Conv2d(24, 24, kernel_size=3, padding=1) def forward(self, x): # 第一个部分 branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1) branch_pool = self.branch_pool(branch_pool) # 第二个部分 branch1x1 = self.branch1x1(x) # 第三个部分 branch5x5 = self.branch5x5_1(x) branch5x5 = self.branch5x5_2(branch5x5) # 第四个部分 branch3x3 = self.branch3x3_1(x) branch3x3 = self.branch3x3_2(branch3x3) branch3x3 = self.branch3x3_3(branch3x3) # 通道整合 outputs = [branch_pool, branch1x1, branch5x5, branch3x3] return torch.cat(outputs, dim=1)class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5) self.conv2 = torch.nn.Conv2d(88, 20, kernel_size=5) self.incep1 = InceptionA(in_channels=10) self.incep2 = InceptionA(in_channels=20) self.mp = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(1408, 10) def forward(self, x): in_size = x.size(0) x = F.relu(self.mp(self.conv1(x))) x = self.incep1(x) x = F.relu(self.mp(self.conv2(x))) x = self.incep2(x) # 变成列向量 x = x.view(in_size, -1) x = self.fc(x) return x# 实例化这个网络模型model = Net()# 第三步 定义损失函数和优化器criterion = torch.nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)# 定义测试函数def test(): test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: outputs = model(data) _, predicted = torch.max(outputs, dim=1) correct += predicted.eq(target.view_as(predicted)).sum().item() test_loss /= len(test_loader.dataset) print(&quot;\\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%) \\n&quot;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset) )) \\ \\ def train(epochs): for epoch in range(epochs): for batch_idx, data in enumerate(train_loader, 0): # prepare data inputs, labels = data # 前馈 y_predict = model(inputs) loss = criterion(y_predict, labels) # 反馈 optimizer.zero_grad() loss.backward() # 更新 optimizer.step() if (batch_idx + 1) % 30 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))if __name__ == &#x27;__main__&#x27;: for epoch in range(10): train(epoch) test() 参考博客深度学习|经典网络：GoogLeNet（一）GoogLeNet","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"},{"name":"GoogleNet","slug":"GoogleNet","permalink":"http://example.com/tags/GoogleNet/"}]},{"title":"梯度消失和梯度爆炸的理解","slug":"梯度消失和梯度爆炸的理解","date":"2021-09-20T11:31:01.000Z","updated":"2021-09-21T15:06:43.194Z","comments":true,"path":"2021/09/20/梯度消失和梯度爆炸的理解/","link":"","permalink":"http://example.com/2021/09/20/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E7%90%86%E8%A7%A3/","excerpt":"","text":"@TOC 根本原因梯度消失和梯度爆炸的根本原因是由于深度神经网络过长的链，在反向传播通过链式法则求导过程中产生的。 换句话说，就是反向传播先天就有一定的毛病。 根本原因的理解左上角为 正向反馈的步骤（σ 表示 激活函数 $sigmoid$ ）。 Loss 使用的是 MSE损失函数。 根据反向传播，我们可以看到最后 loss 对 b1 参数的梯度。 可以看到 连乘的情况。 看这个式子里的 $w{i}$ 中，一般我们初始化权重参数 $w{i}$ 时，通常都小于1。 激活函数 $sigmoid$ 的求导如下所示 激活函数的导数 图像如下图所示： 所以$|σ’(z) × w| ≤ 0.25$，多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说梯度几乎为0，导致参数基本不更新，这就是梯度消失的根本原因。梯度爆炸的原因，也就是说如果$|σ’(z) × w| ≥ 1$，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。但 $sigmoid$ 的函数是不可能大于1了，上图看的很清楚，那只能是参数 $w{i}$了，故只有当 $abs(w)&gt;4$ 时才可能出现梯度爆炸，这也就是经常看到别人博客里的一句话，初始权重过大。但梯度爆炸的情况一般不会发生，对于$sigmoid$ 函数来说，$σ’(z)$的大小也与 $w{i}$ 有关。其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。 如何解决梯度消失的问题（待补充） 用 $ReLU、LeakyRelu、Elu$ 等激活函数激活函数取代 $sigmoid$ 激活函数。将输出不要固定在0-1之间。$sigmoid$函数的梯度随着 $x$ 的增大或减小和消失，而 $ReLU$ 不会。 Batch Normalization 就是通过对每一层的输出规范为均值和方差一致的方法，消除了$w$带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。 ResNet残差结构具体待补充完善，查看 ResNet LSTM结构LSTM不太容易发生梯度消失，主要原因在于LSTM内部复杂的“门（gates）”，具体看LSTM基本原理解析 预训练加finetunning此方法来自Hinton在06年发表的论文上，其基本思想是每次训练一层隐藏层节点，将上一层隐藏层的输出作为输入，而本层的输出作为下一层的输入，这就是逐层预训练。训练完成后，再对整个网络进行“微调（fine-tunning）”。此方法相当于是找全局最优，然后整合起来寻找全局最优，但是现在基本都是直接拿imagenet的预训练模型直接进行finetunning。 梯度剪切、正则这个方案主要是针对梯度爆炸提出的，其思想是设值一个剪切阈值，如果更新梯度时，梯度超过了这个阈值，那么就将其强制限制在这个范围之内。这样可以防止梯度爆炸。另一种防止梯度爆炸的手段是采用权重正则化，正则化主要是通过对网络权重做正则来限制过拟合，但是根据正则项在损失函数中的形式：可以看出，如果发生梯度爆炸，那么权值的范数就会变的非常大，反过来，通过限制正则化项的大小，也可以在一定程度上限制梯度爆炸的发生。 参考博客 神经网络训练中的梯度消失与梯度爆炸 梯度消失和梯度爆炸问题详解","categories":[{"name":"深度学习基础","slug":"深度学习基础","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Keras 实现 Kaggle 数据集 Titanic 预测","slug":"Keras 实现 Kaggle 数据集 Titanic 预测","date":"2021-09-17T12:24:01.000Z","updated":"2021-09-17T12:25:41.234Z","comments":true,"path":"2021/09/17/Keras 实现 Kaggle 数据集 Titanic 预测/","link":"","permalink":"http://example.com/2021/09/17/Keras%20%E5%AE%9E%E7%8E%B0%20Kaggle%20%E6%95%B0%E6%8D%AE%E9%9B%86%20Titanic%20%E9%A2%84%E6%B5%8B/","excerpt":"","text":"@TOC 问题描述 使用乘客数据(如姓名、年龄、性别、社会经济阶层等)，建立一个模型预测泰坦尼克号沉船上哪些乘客能够幸存。数据被分成训练集和测试集两组，它们分别在train.csv和test.csv文档中。我们的模型将基于训练集的乘客的性别和阶级等特征建立。在测试集中每个乘客是否幸存的信息是缺省的，其将由我们模型预测出来作为答案提交。 加载本地下载的 Titanic 数据集这里使用的 是 pandas 的 read_csv() 方法。 读取的格式为 DataFrame。12345# xlsx训练数据导入train_filepath = r&quot;../dataset/Titanic/train.csv&quot;train_data = pd.read_csv(train_filepath)test_filepath = r&quot;../dataset/Titanic/test.csv&quot;test_data = pd.read_csv(test_filepath)同样可以用 shape函数查看 12#(891,12)print(train_data.shape) 数据分析与预处理在预处理数据前，首先整体分析各项数据对预测模型的重要性 （1）PassengerID：乘客的ID（2）Survived：乘客是否幸存，取值为0或1，是我们预测/分类的目标。（3）Pclass：客舱等级，可能蕴含着乘客的阶层、乘客客舱的位置等信息，比较重要。（4）Name： 姓名，是无关信息。（5）Sex：性别。灾难来临时常让妇女儿童先走，而同等条件女性体力普遍弱于男性，这些因素都会影响到一名乘客幸存的可能性，因此比较重要。（6）Age：年龄，较为重要，理由同上。（7）Parch：直系亲友数目，比较重要。（8）SibSp：旁系亲友数目，比较重要。（9）Ticket：票编号，是无关信息。（10）Fare：票价，可能会反映乘客的社会阶层等。（11）Cabin：客舱编号，可能会反映客舱位置等，但由于缺省太多，数据量很小不具有代表性，可以视为噪音剔除。（12）Embarked：上船的港口编号。 在剔除了一些数据后，是否会因信息损失而降低模型的准确度？例如乘客的姓名可能暗含船上乘客之间家庭的关系。实际上我们的模型本来就是建立在不完全观测上（比如我们不知道船上的一对男女乘客有没有发生像Jack和Rose那样的故事），不确定性是必然存在的。把握主要矛盾，舍弃噪音信息是建立模型的一个好思路。 训练数据预处理方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 数据预处理# 训练数据预处理def PreprocessTrainData(train_data): # 预处理1：筛除无关特征 # 无关的有 乘客的ID 姓名 票编号客舱编号（数据量太少，当噪声剔除） # 是否幸存 客舱等级 性别 年龄 旁系亲友数目 直系亲友数目 票价 上船港口编号 cols=[&#x27;Survived&#x27;, &#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;SibSp&#x27;, &#x27;Parch&#x27;, &#x27;Fare&#x27;, &#x27;Embarked&#x27;] # colums表示列名 index 表示行名 train_data = pd.DataFrame(train_data, columns=cols) #(891,8) print(train_data.shape) # 预处理2：填充缺失特征并标准化特征 age_mean = train_data[&#x27;Age&#x27;].mean() # fillna 为 无值的数据填充 train_data[&#x27;Age&#x27;] = train_data[&#x27;Age&#x27;].fillna(age_mean) fare_mean = train_data[&#x27;Fare&#x27;].mean() train_data[&#x27;Fare&#x27;] = train_data[&#x27;Fare&#x27;].fillna(fare_mean) # 预处理3：性别编码0-1 将&#123;&#x27;female&#x27;: 0, &#x27;male&#x27;: 1&#125; train_data[&#x27;Sex&#x27;]= train_data[&#x27;Sex&#x27;].map(&#123;&#x27;female&#x27;: 0, &#x27;male&#x27;: 1&#125;).astype(int) # 预处理4：登港地点转换为one-hot编码 # 这是将 Embarked这一列 分成 one-hot形式 共有三个港口 所以分成了3列 x_OneHot_df = pd.get_dummies(data=train_data,columns=[&quot;Embarked&quot;]) ndarray = x_OneHot_df.values print(&#x27;ndarray&#x27;,ndarray) #(891,10) print(ndarray.shape) # 预处理5：全体特征标准化，标签向量化 # &#x27;Survived&#x27; label = ndarray[:,:1] # label的shape： (891,1) print(&quot;label的shape：&quot;,label.shape) # 除了&#x27;Survived&#x27; 其他全部特征 features = ndarray[:,1:] # features的shape： (891, 9) print(&quot;features的shape：&quot;, features.shape) # 求一个所有列的平均值 mean = features.mean(axis=0) features -= mean # 求一个所有列的方差 std = features.std(axis=0) features /= std return features,label 测试数据预处理方法本质上与训练数据相同，只是少了一列 标签 12345678910111213141516171819202122232425262728# 测试数据预处理def PreprocessTestData(test_data): # 预处理1：筛除无关特征 # 客舱等级 性别 年龄 旁系亲友数目 直系亲友数目 票价 上船港口编号 cols=[ &#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;SibSp&#x27;, &#x27;Parch&#x27;, &#x27;Fare&#x27;, &#x27;Embarked&#x27;] test_data = test_data[cols] # 预处理2：填充缺失特征并标准化特征 age_mean = test_data[&#x27;Age&#x27;].mean() test_data[&#x27;Age&#x27;] = test_data[&#x27;Age&#x27;].fillna(age_mean) fare_mean = test_data[&#x27;Fare&#x27;].mean() test_data[&#x27;Fare&#x27;] = test_data[&#x27;Fare&#x27;].fillna(fare_mean) # 预处理3：性别编码0-1 test_data[&#x27;Sex&#x27;]= test_data[&#x27;Sex&#x27;].map(&#123;&#x27;female&#x27;: 0, &#x27;male&#x27;: 1&#125;).astype(int) # 预处理4：登港地点转换为one-hot编码 x_OneHot_df = pd.get_dummies(data=test_data,columns=[&quot;Embarked&quot;]) ndarray = x_OneHot_df.values # 预处理5：全体特征标准化，标签向量化 features = ndarray mean = features.mean(axis=0) features -= mean std = features.std(axis=0) features /= std return features 拿到处理后的数据12x_train, y_train = PreprocessTrainData(train_data)x_test = PreprocessTestData(test_data) 构建网络模型构建网络时需要注意控制网络的大小。模型中容量（模型可学习的参数）不足可能导致欠拟合；但模型也不是越大越好，因为模型过大可能导致过拟合，泛化能力下降。其他降低过拟合的方法包括添加dropout正则化、权重正则化等。此外还需要在评估模型（将在下文阐述）的过程中尝试不同的超参数（学习率等）以找到最佳配置。123456789101112131415# 第三步 构建网络def TitanicModel(): # 构建网络-模型定义 model = models.Sequential() model.add(layers.Dense(input_dim=9,units=64, kernel_regularizer=regularizers.l1_l2(l1=0.001,l2=0.001), activation=&#x27;relu&#x27;)) model.add(layers.Dropout(0.5)) model.add(layers.Dense(units=64, kernel_regularizer=regularizers.l1_l2(l1=0.001,l2=0.001), activation=&#x27;relu&#x27;)) model.add(layers.Dropout(0.5)) model.add(layers.Dense(units=1, activation=&#x27;sigmoid&#x27;)) # 构建网络-编译模型 model.compile(optimizer=&#x27;rmsprop&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;]) return model 训练模型（带验证集）划分测试集和验证集12345678910111213# 留出验证集num_val = 300# 将训练集样本顺序 随机打乱np.random.shuffle([x_train,y_train])# 取出 前300个 训练样本作为 验证集x_val = x_train[:num_val]# 其他的部分作为 真实的训练集partial_x_train = x_train[num_val:]# 取出前三百个的标签 作为 验证集y_val = y_train[:num_val]# 其他的 作为真实的训练集partial_y_train = y_train[num_val:] 训练模型123# 训练模型model = TitanicModel()model.fit(partial_x_train, partial_y_train, epochs = 150, batch_size=16, validation_data=(x_val, y_val)) 预测测试样本并评估预测测试样本12y_test2 = model.predict_classes(x_test)print(y_test2) 读取正确答案123456789real_label_filepath = r&quot;../dataset/Titanic/gender_submission.csv&quot;real_label = pd.read_csv(real_label_filepath)# 读取正确数值一列onehot = pd.get_dummies(data=real_label)xarray = onehot.valuesreal = xarray[:,1:]print(real.shape,y_test2.shape) 输出正确率与保存预测答案123456789101112count = 0for (y_pre,y_rel) in zip(y_test2,real): if y_pre == y_rel: count = count+1print(&quot;count:&quot;,count)print(&quot;这个模型的正确率是：&quot; ,count/y_test2.shape[0])with open(r&quot;../dataset/Titanic/gender_submission_predict.csv&quot;,&#x27;w+&#x27;,newline=&#x27;&#x27;) as f: csv_file = csv.writer(f) csv_file.writerows(y_test2) 这个模型的优点这个模型 来自 [https://zhuanlan.zhihu.com/p/278057962?utm_source=wechat_session&amp;ivk_sa=1024320u]此模型在kaggle上排名前12%。总结其优点如下： （1）几乎完全没有人工干预。我们并不需要深入理解和分析每种因素对乘客幸存可能性的影响，而只需将数据几乎交由机器自己来学习便能得到准确度极高的预测结果。 （2）几乎没有引入数据集以外的新信息。引入新信息的行为包括将已知的乘客生存信息填入预测结果（kaggle上实现100%准确率的来源）等。此模型仅在数据处理阶段，引入部分常识判断的信息。 （3）模型泛化能力强。这里的“泛化”是指在模型建立过程中没有对该问题“过拟合”。实质上一味追求此问题的预测准确率是没有意义的。过度分析并设计复杂的特征工程也许可以提高测试集的准确率，但实质上很可能是对该问题的过拟合，不能在其他类似问题上泛化。","categories":[{"name":"Keras","slug":"Keras","permalink":"http://example.com/categories/Keras/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/tags/Keras/"},{"name":"刘二","slug":"刘二","permalink":"http://example.com/tags/%E5%88%98%E4%BA%8C/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://example.com/tags/Kaggle/"}]},{"title":"Pytorch 封装函数","slug":"Pytorch 封装函数","date":"2021-09-17T11:40:01.000Z","updated":"2021-09-17T12:23:25.342Z","comments":true,"path":"2021/09/17/Pytorch 封装函数/","link":"","permalink":"http://example.com/2021/09/17/Pytorch%20%E5%B0%81%E8%A3%85%E5%87%BD%E6%95%B0/","excerpt":"","text":"@TOC torch.tensor.viewTensor.view(*shape) → Tensor view()的作用相当于numpy中的reshape，重新定义矩阵的形状。 示例： 1234567891011121314151617181920a = torch.range(1,30)print(a)&gt;&gt;a:tensor([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30.])b = a.view(2,3,5)print(b)&gt;&gt;b:tensor([[[ 1., 2., 3., 4., 5.], [ 6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.]], [[16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], [26., 27., 28., 29., 30.]]])print(b.view(b.size(0),-1))print(b.view(b.size(1),-1))print(b.view(b.size(2),-1)) b是一个2组3行5列，b.size(0)就是留下2组，后面3行5列拉直成15个数，行成2行15列；b.size(1)就是留下3行，2组5列拉成10个数，行成3行10列；b.size(2)就是留下5列，2组3行拉成6个数，行成5行6列","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"}]},{"title":"数据集加载的各种方式方法","slug":"数据加载的各种方式方法","date":"2021-09-17T11:37:01.000Z","updated":"2021-09-17T12:21:53.678Z","comments":true,"path":"2021/09/17/数据加载的各种方式方法/","link":"","permalink":"http://example.com/2021/09/17/%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E7%9A%84%E5%90%84%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%96%B9%E6%B3%95/","excerpt":"","text":"@TOC np.loadtxt 读取本地 csv 文件读入本地 csv 文件内容。 12345678# 读取数据&#x27;&#x27;&#x27; numpy读取本地文件 delimiter 数据的分割号 dtype 读取数据类型 一般机器学习类的都是float32 因为显卡一般他内核里面是按32位工作的&#x27;&#x27;&#x27;xy = np.loadtxt(&#x27;./dataset/diabetes.csv&#x27;,delimiter=&quot;,&quot;,dtype=np.float32) 用Pytorch使用1234# numpy 转 张量 最后一组样本我们当测试样本吧 其余的当训练样本x_train = torch.from_numpy(xy[:-1,:-1])print(x_train.shape)y_train = torch.from_numpy(xy[:-1,[-1]]) 批次读取 本地 csv 文件使用 DataLoader+Dataset Step1：引入包1234# Dataset 是个抽象类，所以其不可以被实例化# Dataset 可以为其他的子类所继承的from torch.utils.data import Datasetfrom torch.utils.data import DataLoader Step2：定义一个自己的Dataset类 并继承原有的抽象类Dataset1234567891011121314151617181920212223# DiabetesDataset 继承 Datasetclass DiabetesDataset(Dataset): def __init__(self, filepath): xy = np.loadtxt(filepath, delimiter=&#x27;,&#x27;, dtype=np.float32) # 一共有几个样本 self.len = xy.shape[0] # numpy 转 张量 最后一组样本我们当测试样本吧 其余的当训练样本 self.x_train = torch.from_numpy(xy[:, :-1]) self.y_train = torch.from_numpy(xy[:, [-1]]) # 继承Dataset的方法并重写 # 其实用来帮助找索引位置的 dataset[index] # 函数功能是根据index索引去返回数据样本以及标签label def __getitem__(self, index): return self.x_train[index], self.y_train[index] # 函数功能是用来查看数据的长度，也就是 dataset 样本的数量 def __len__(self): return self.len# 实例化这个类diabetesdataset = DiabetesDataset(&#x27;../dataset/diabetes.csv&#x27;) Step3：使用DataLoader其返回的是 对应索引的数据123456789# 定义一个 loader&#x27;&#x27;&#x27; batch_size 就是批次大小 shuffle True的话就表示要打乱数据 num_workers 读取Mni-batch的时候要多进程 2就表示由2个进程进行读取&#x27;&#x27;&#x27;# 返回 （x,y）train_loader = DataLoader(dataset=diabetesdataset, batch_size=32, num_workers=2, shuffle=True, drop_last=False) Step4：用迭代的方式拿到数据12345678910111213if __name__ == &#x27;__main__&#x27;: for epoch in range(100): for i, data in enumerate(train_loader, 0): # 1. Prepare data inputs, labels = data # 2. Forward y_pred = FullLeanerModel(inputs) loss = criterion(y_pred, labels) # 3. Backward optimizer.zero_grad() # 梯度清零 loss.backward() # 反向传播 # 4. Update optimizer.step() # 更新参数 用 pandas 的 read_csv 方法12train_filepath = r&quot;../dataset/Titanic/train.csv&quot;train_data = pd.read_csv(train_filepath) pandas需要使用 DataFrame 的形式可用 pandas.DataFrame()来转换成 DataFrame 格式 1234# 是否幸存 客舱等级 性别 年龄 旁系亲友数目 直系亲友数目 票价 上船港口编号cols=[&#x27;Survived&#x27;, &#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Age&#x27;, &#x27;SibSp&#x27;, &#x27;Parch&#x27;, &#x27;Fare&#x27;, &#x27;Embarked&#x27;]# colums表示列名 index 表示行名 选择需要的列train_data = pd.DataFrame(train_data, columns=cols) 可以用 ndarray 将DataFrame 转为 普通的numpy 读取出来。如下： 1234# &#x27;Survived&#x27;label = ndarray[:,:1]# 除了&#x27;Survived&#x27; 其他全部特征features = ndarray[:,1:]","categories":[{"name":"数据集加载","slug":"数据集加载","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"数据集","slug":"数据集","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"}]},{"title":"Pytorch CNN概念性解决","slug":"Pytorch CNN概念性解决","date":"2021-09-16T12:40:01.000Z","updated":"2021-09-17T12:19:07.363Z","comments":true,"path":"2021/09/16/Pytorch CNN概念性解决/","link":"","permalink":"http://example.com/2021/09/16/Pytorch%20CNN%E6%A6%82%E5%BF%B5%E6%80%A7%E8%A7%A3%E5%86%B3/","excerpt":"","text":"@TOC需要知道的几点 卷积核里的参数都是超参数，是通过训练进行调整的 CNN的流程input——&gt;convolution——&gt;max pooling——&gt;…——&gt;flatten（压平）——&gt;fully connected network——&gt;output CNN的卷积操作运算单通道图像 多通道图像 如三通道RGB用三个卷积核，分别对3个通道各自卷积，然后再相加整合输出。 如何看待卷积核的个数和通道数记住两条规律就好： 卷积核通道数 = 输入通道数 卷积核个数 = 输出通道数 注意卷积层一般都是4维的 包括：批次，输入通道数，卷积核大小（宽，高）可以参考下面的卷积程序 1234567891011121314151617181920212223242526import torch# 输入通道数为5 输出通道数为10in_channels,out_channels = 5, 10# 初始化图像高度width,height = 100,100# 卷积核的大小kernel_size = 3# 批次batch_size = 1# 随机生成100×100的 图像input = torch.randn(batch_size,in_channels,width,height)# 设置卷积层 输入维度，输出维度，卷积核大小conv_layer = torch.nn.Conv2d(in_channels,out_channels,kernel_size=kernel_size)# 将图片放入卷积层 输出结果output = conv_layer(input)# torch.Size([1, 5, 100, 100])# 分别为 batch_size,图片通道数，图像宽100，图像高100print(input.shape)# torch.Size([1, 10, 98, 98])# 分别为 batch_size,输出通道数10（其实就是卷积核的个数），图像卷积后（由于卷积核是3×3 所以减去2）print(output.shape)# torch.Size([10, 5, 3, 3])# 输出通道（卷积核数量） 输入通道（卷积核通道数） 卷积核大小print(conv_layer.weight.shape) 如何看待 paddingpadding的目的是，为了规定输出图像的大小。例如 原来是 5×5 的原图，通过卷积核为 3×3 那 输出的图 是 3×3 的但如果设置 padding 为1，那么相当于把原图扩充为 7×7 了。可以参考一下如下的代码： 1234567891011121314151617181920212223242526272829303132import torch# 假设的灰度图片像素值 图片为5×5input = [3,4,6,5,7, 2,4,6,8,2, 1,6,7,8,4, 9,7,4,6,2, 3,7,5,4,1]# 将输入的图片变成Tensor类型，并且reshape一下&#x27;&#x27;&#x27; 参数从左到右分别为 batch_size,in_channels,width,height&#x27;&#x27;&#x27;input = torch.Tensor(input).view(1,1,5,5)# 卷积核的大小kernel_size = 3&#x27;&#x27;&#x27; 从左到右分别为 in_channels,out_channels,kernel_size&#x27;&#x27;&#x27;conv_layer = torch.nn.Conv2d(1,1,3,padding=1,bias=False)&#x27;&#x27;&#x27; 自定义一个tensor向量 再将其 reshape batch_size,in_channels,width,height&#x27;&#x27;&#x27;kernel = torch.Tensor([1,2,3,4,5,6,7,8,9]).view(1,1,3,3)# 初始化卷积层conv_layer.weight.data = kernel.dataoutput = conv_layer(input)# torch.Size([1, 1, 5, 5])print(output.shape)print(output) 如何看待步长 stride设置一下 stride就可以了1conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, stride=2, bias=False)改变步长的意义，其实就是为了缩小输出尺寸 如何看待Max Pooling可以参考如下的代码: 1234567891011121314151617181920212223242526import torchinput = [ 3,4,6,5, 2,4,6,8, 1,6,7,8, 9,7,4,6]# 输入的 batch_size为1 通道数为1 也就是灰度图像 大小是4×4input = torch.Tensor(input).view(1,1,4,4)# 默认的步长 stride也是2maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2)output = maxpooling_layer(input)print(output.shape)print(output)&#x27;&#x27;&#x27; 这里只进行了一个 maxpooling操作 input = [ 3,4,6,5, 2,4,6,8, 1,6,7,8, 9,7,4,6] 变为 [4,8 9,8]&#x27;&#x27;&#x27; 设计一个网络并实现如图所示为 要设计的网络模型 转换成流程图 Pytorch 代码实现这个网络12345678910111213141516171819202122# 设计网络class CNNNet(torch.nn.Module): def __init__(self): super(CNNNet, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5) self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5) self.pooling = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(320, 10) def forward(self, x): # Flatten data from (n, 1, 28, 28) to (n, 784) batch_size = x.size(0) x = self.pooling(torch.relu(self.conv1(x))) x = self.pooling(torch.relu(self.conv2(x))) # flatten (n,320) x = x.view(batch_size, -1) x = self.fc(x) return x # 实例化这个网络CNNmodel = CNNNet()","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"CNN","slug":"CNN","permalink":"http://example.com/tags/CNN/"},{"name":"刘二","slug":"刘二","permalink":"http://example.com/tags/%E5%88%98%E4%BA%8C/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"}]},{"title":"Pytorch 多分类问题的解决","slug":"Pytorch 多分类问题的解决","date":"2021-09-15T11:40:01.000Z","updated":"2021-09-17T12:20:57.433Z","comments":true,"path":"2021/09/15/Pytorch 多分类问题的解决/","link":"","permalink":"http://example.com/2021/09/15/Pytorch%20%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/","excerpt":"","text":"@TOC 多分类问题 激活函数的选择 Softmax选择 Softmax 函数的关键点是： 确保输出的每一个概率都是 ≥ 0 的 所有的概率之和应该为 1 Softmax 激活函数完美解决了这个问题： 举个简单的例子 CrossEntropyLoss 和 NLLLoss区别NLLLoss全称是Negative Log Likelyhood Loss，负对数似然损失函数。softmax + NLLLoss = CrossEntropyLoss 在Pytorch中 CrossEntropyLoss可以直接接到模型结果之后，直接得出交叉熵损失。 NLLLoss需要在模型结果后先接一个Softmax，将模型结果变成概率，再用NLLLoss求预测损失。 实现梯度下降Step1：准备数据12x_data = [1.0, 2.0, 3.0]y_data = [2.0, 4.0, 6.0] Step2：初始化参数w12# 初始值w为1w = 1.0 Step3：定义模型12def forward(x_data): return x_data * w Step4：定义损失函数 123456def loss(x_data, y_data): loss = 0 for x, y in zip(x_data, y_data): y_pred = forward(x) loss = loss + (y_pred - y) ** 2 return loss / len(x_data) Step5：定义梯度 12345678910111213# 求梯度 也就是 损失函数对w的偏导def gradient(x_data, y_data): grad = 0 &#x27;&#x27;&#x27; x = [1, 2, 3] y = [4, 5, 6, 7] xy = zip(x, y) print xy 运行的结果是： [(1, 4), (2, 5), (3, 6)] &#x27;&#x27;&#x27; for x, y in zip(x_data, y_data): grad = grad + 2 * x * (x * w - y) return grad / len(x_data) Step6：训练并更新参数 w 1234567891011# 没训练过的时候 w是初始值print(&#x27;Predict (before training)&#x27;, 4, forward(4))# 这批数据样本 训练 99次for epoch in range(100): # 每次都是计算一整个数据集的 平均loss loss_val = loss(x_data, y_data) grad_val = gradient(x_data, y_data) w = w - 0.01 * grad_val print(&#x27;Epoch:&#x27;, epoch, &#x27;w=&#x27;, w, &#x27;loss=&#x27;, loss_val)print(&#x27;Predict (after training)&#x27;, 4, forward(4)) 实现多分类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import torchfrom torchvision import transformsfrom torchvision import datasetsfrom torch.utils.data import DataLoaderimport torch.nn.functional as Fimport torch.optim as optim# 第一步 准备数据batch_size = 64transform = transforms.Compose([ transforms.ToTensor(), # 用均值和方差进行归一化 transforms.Normalize((0.1307,), (0.3081,))])train_dataset = datasets.MNIST(root=&#x27;../dataset/mnist/&#x27;, train=True, download=True, transform=transform)test_dataset = datasets.MNIST(root=&#x27;../dataset/mnist/&#x27;, train=False, download=True, transform=transform)train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)# 第二步 设计模型class Net(torch.nn.Module): def __init__(self): # 这个是必须写的 super(Net, self).__init__() # 构建层次模型 self.l1 = torch.nn.Linear(784, 512) self.l2 = torch.nn.Linear(512, 256) self.l3 = torch.nn.Linear(256, 128) self.l4 = torch.nn.Linear(128, 64) self.l5 = torch.nn.Linear(64, 10) def forward(self, x): x = x.view(-1,784) x = F.relu(self.l1(x)) x = F.relu(self.l2(x)) x = F.relu(self.l3(x)) x = F.relu(self.l4(x)) return self.l5(x)# 实例化这个网络模型model = Net()# 第三步 定义损失函数和优化器criterion = torch.nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)# 第四步 训练模型def train(epoch): running_loss = 0.0 # 这个0 的意思表示 索引从0开始计 for batch_idx, data in enumerate(train_loader, 0): inputs, target = data optimizer.zero_grad() # forward + backward + update outputs = model(inputs) loss = criterion(outputs, target) loss.backward() optimizer.step() running_loss += loss.item() if batch_idx % 300 == 299: print(&#x27;[%d, %5d] loss: %.3f&#x27; % (epoch + 1, batch_idx + 1, running_loss / 300)) running_loss = 0.0# 第五步 测试并检验def test(): correct = 0 total = 0 with torch.no_grad(): for data in test_loader: images, labels = data outputs = model(images) #输出每行最大的那个 _, predicted = torch.max(outputs.data, dim=1) total += labels.size(0) correct += (predicted == labels).sum().item() print(&#x27;Accuracy on test set: %d %%&#x27; % (100 * correct / total))if __name__ == &#x27;__main__&#x27;: for epoch in range(10): train(epoch) test()","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"},{"name":"分类问题","slug":"分类问题","permalink":"http://example.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"}]},{"title":"Pytorch 数据读取 DataLoader与Dataset 概念","slug":"Pytorch 数据读取 DataLoader与Dataset 概念","date":"2021-09-13T13:17:01.000Z","updated":"2021-09-13T13:17:51.173Z","comments":true,"path":"2021/09/13/Pytorch 数据读取 DataLoader与Dataset 概念/","link":"","permalink":"http://example.com/2021/09/13/Pytorch%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%20DataLoader%E4%B8%8EDataset%20%E6%A6%82%E5%BF%B5/","excerpt":"","text":"@TOC 在机器学习中，我们对数据的处理主要分为4个阶段，如下图所示： 第一步，收集需要的数据，数据包括原始样本 和 对应标签； 第二步，对数据集进行划分，把数据集划分为 训练集、验证集和测试集 ；训练集用于训练模型，验证集用于验证模型是否过拟合（也可以理解为用验证集挑选模型的超参数），测试集用于测试模型的性能，测试模型的泛化能力； 第三步，从本地读取数据，要按 Mini-batch 分批训练。一起训练，内存不够。使用的方法为 DataLoader ，其又分为两个部分： Sample 用于生成索引，即样本的序号； Dataset 是根据索引去读取图片以及对应的标签； 第四步，数据预处理，把数据读取进来往往还需要对数据进行一系列的图像预处理，比如说数据的中心化，标准化，旋转或者翻转等等。Pytorch 中数据预处理是通过 transforms 进行处理的； DataLoader 和 DatasetDataLoader torch.utils.data.DataLoader 功能：构建可迭代的数据装载器；dataset: Dataset类，决定数据从哪里读取及如何读取；batchsize：批次样本数量大小；num_works:是否多进程读取数据； 可以设置为几个进程shuffle：每个epoch是否乱序；drop_last：当样本数不能被 batchsize 整除时，是否舍弃最后一批数据； Epoch，Iteration，Batchsize的区别 Epoch：所有训练样本都已输入到模型中，称为一个EpochIteration：一批样本输入到模型中，称之为一个Iteration；Batchsize：批次样本数量大小，决定一个Epoch中有多少个Iteration； Iteration = Epoch ➗ Batchsize123样本总数：87，Batchsize=8 （样本不能被Batchsize整除）1 Epoch = 10 Iteration，drop_last = True1 Epoch = 11 Iteration， drop_last = False用法例如： 1train_loader = DataLoader(dataset=diabetesdataset,batch_size=32,num_workers=2,shuffle=True,drop_last=False) Dataset torch.utils.data.DatasetDataset是用来定义数据从哪里读取，以及如何读取的问题； 功能：Dataset抽象类，所有自定义的Dataset需要继承它，并且重写getitem()； getitem ()：接收一个索引，返回一个样本 例如： 123456789101112131415161718# DiabetesDataset 继承 Datasetclass DiabetesDataset(Dataset): def __init__(self,filepath): xy = np.loadtxt(filepath,delimiter=&#x27;,&#x27;,dtype=np.float32) # 一共有几个样本 self.len = xy.shape[0] # numpy 转 张量 最后一组样本我们当测试样本吧 其余的当训练样本 self.x_train = torch.from_numpy(xy[:, :-1]) self.y_train = torch.from_numpy(xy[:, [-1]]) # 继承Dataset的方法并重写 # 其实用来帮助找索引位置的 dataset[index] # 函数功能是根据index索引去返回数据样本以及标签label def __getitem__(self, index): return self.x_train[index],self.y_train[index] # 函数功能是用来查看数据的长度，也就是 dataset 样本的数量 def __len__(self): return self.len Dataloader的运行机制数据读取的三个问题：1、读哪些数据；2、从哪读数据；3、怎么读数据？ 从代码中可以发现，index 是从 sampler.py 中输出的，所以读哪些数据是由sampler得到的； 从代码中看，是从Dataset中的 文件地址参数 告诉我们 Pytorch 是从硬盘中的哪一个文件夹获取数据； 从代码中可以发现，Pytorch是从Dataset的getitem()中具体实现的，根据索引去读取数据； DataLoader数据读取流程简单描述一下流程图，首先在for循环中去使用DataLoader，进入DataLoader之后是否采用多进程进入DataLoaderlter，进入DataLoaderIter之后会使用sampler去获取Index，拿到索引之后传输到DatasetFetcher，在DatasetFetcher中会调用Dataset，Dataset根据给定的Index，在getitem中从硬盘里面去读取实际的Img和Label，读取了一个batch_size的数据之后，通过一个collate_fn将数据进行整理，整理成batch_Data的形式，接着就可以输入到模型中训练； 读哪些是由Sampler决定的，从哪读是由Dataset决定的，怎么读是由getitem决定的 详细原文转载https://blog.csdn.net/qq_37388085/article/details/102663166 实验代码糖尿病案例。 数据读取采用 批处理 DataLoader。理论上 测试集和训练集 都应该分别有一个 DataLoader 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import torchimport numpy as np# Dataset 是个抽象类，所以其不可以被实例化# Dataset 可以为其他的子类所继承的from torch.utils.data import Datasetfrom torch.utils.data import DataLoaderxy = np.loadtxt(&#x27;../dataset/diabetes.csv&#x27;, delimiter=&quot;,&quot;, dtype=np.float32)# (759,9) 759行 9列# 当测试样本 的 一组数据x_test = torch.from_numpy(xy[-1:, :-1])y_test = torch.from_numpy(xy[-1:, [-1]])# DiabetesDataset 继承 Datasetclass DiabetesDataset(Dataset): def __init__(self, filepath): xy = np.loadtxt(filepath, delimiter=&#x27;,&#x27;, dtype=np.float32) # 一共有几个样本 self.len = xy.shape[0] # numpy 转 张量 最后一组样本我们当测试样本吧 其余的当训练样本 self.x_train = torch.from_numpy(xy[:, :-1]) self.y_train = torch.from_numpy(xy[:, [-1]]) # 继承Dataset的方法并重写 # 其实用来帮助找索引位置的 dataset[index] # 函数功能是根据index索引去返回数据样本以及标签label def __getitem__(self, index): return self.x_train[index], self.y_train[index] # 函数功能是用来查看数据的长度，也就是 dataset 样本的数量 def __len__(self): return self.len# 实例化这个类diabetesdataset = DiabetesDataset(&#x27;../dataset/diabetes.csv&#x27;)# 定义一个 loader&#x27;&#x27;&#x27; batch_size 就是批次大小 shuffle True的话就表示要打乱数据 num_workers 读取Mni-batch的时候要多线程 2就表示由2个线程进行读取&#x27;&#x27;&#x27;# 返回 （x,y）train_loader = DataLoader(dataset=diabetesdataset, batch_size=32, num_workers=2, shuffle=True, drop_last=False)# 第二部 定义模型# 这个其实有点类似与 神经网络的结构了 每个线性层后面都加了一个sigmoid函数 做了次非线性变换class FullLeanerModel(torch.nn.Module): # 定义多层线性模型的结构 def __init__(self): super(FullLeanerModel, self).__init__() # 第一个线性转换 将8维转换为6维 self.linear1 = torch.nn.Linear(8, 6) self.linear2 = torch.nn.Linear(6, 4) self.linear3 = torch.nn.Linear(4, 1) self.sigmoid = torch.nn.Sigmoid() # 前馈 运行 def forward(self, x): # 用一个变量比较 简单 x = self.sigmoid(self.linear1(x)) x = self.sigmoid(self.linear2(x)) x = self.sigmoid(self.linear3(x)) return x# 实例化模型FullLeanerModel = FullLeanerModel()# 第三步 定义 损失函数和优化器criterion = torch.nn.BCELoss(reduction=&#x27;mean&#x27;)optimizer = torch.optim.SGD(FullLeanerModel.parameters(), lr=0.1)# 第四步 训练&#x27;&#x27;&#x27; 前两步 就是正向传播 forward 1. 预测 标签 2. 预测 与 实际 算出损失值 3. 反向传播 backward 优化参数 4. 更新参数&#x27;&#x27;&#x27;#if __name__ == &#x27;__main__&#x27;: for epoch in range(100): for i, data in enumerate(train_loader, 0): # 1. Prepare data inputs, labels = data # 2. Forward y_pred = FullLeanerModel(inputs) loss = criterion(y_pred, labels) # 3. Backward optimizer.zero_grad() # 梯度清零 loss.backward() # 反向传播 # 4. Update optimizer.step() # 更新参数 # 第五步 评估模型 # 输出各层 权重w 和 偏置b for weight, bias in FullLeanerModel.state_dict().items(): # param is weight or bias(Tensor) print(weight, bias) # 预测 y_yuce = FullLeanerModel(x_test) print(&quot;测试样本的预测值为&quot;, y_yuce.data, &quot;实际样本的标签值为&quot;, y_test.data) 实验结果","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"刘二","slug":"刘二","permalink":"http://example.com/tags/%E5%88%98%E4%BA%8C/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"},{"name":"DataLoader","slug":"DataLoader","permalink":"http://example.com/tags/DataLoader/"}]},{"title":"Pytorch 实现 多维特征的输入——糖尿病预测","slug":"Pytorch 实现 多维特征的输入——糖尿病预测","date":"2021-09-13T03:30:01.000Z","updated":"2021-09-13T13:24:52.355Z","comments":true,"path":"2021/09/13/Pytorch 实现 多维特征的输入——糖尿病预测/","link":"","permalink":"http://example.com/2021/09/13/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E7%9A%84%E8%BE%93%E5%85%A5%E2%80%94%E2%80%94%E7%B3%96%E5%B0%BF%E7%97%85%E9%A2%84%E6%B5%8B/","excerpt":"","text":"@TOC 整体的设计思路大致的设计步骤 分为5步 如下所示：第五步：是进行 评估模型并预测 12import torchimport numpy as np Step1：准备数据数据格式包含759个样本，其中有8个特征已经其是否会加剧糖尿病的预测标签（1或0）。我们将759个样本，分为758个训练样本以及1个测试样本。 先读入本地 csv 文件内容 12345678# 读取数据&#x27;&#x27;&#x27; numpy读取本地文件 delimiter 分割号 dtype 读取数据类型 一般机器学习类的都是float32 因为显卡一般他内核里面是按32位工作的&#x27;&#x27;&#x27;xy = np.loadtxt(&#x27;./dataset/diabetes.csv&#x27;,delimiter=&quot;,&quot;,dtype=np.float32) 可以查看一下 读入的数据维度情况 12# (759,9) 759行 9列print(xy.shape) 其中758个样本作为测试集 1234# numpy 转 张量 最后一组样本我们当测试样本吧 其余的当训练样本x_train = torch.from_numpy(xy[:-1,:-1])print(x_train.shape)y_train = torch.from_numpy(xy[:-1,[-1]]) 最后一个样本作为训练集，用于预测结果 1234# 当测试样本 的 一组数据x_test = torch.from_numpy(xy[-1:,:-1])print(x_test.shape)y_test = torch.from_numpy(xy[-1:,[-1]]) Step2：定义模型同样每一层都为 逻辑回归模型。但这边有8个特征，所以导入的应该是个矩阵。 torch.nn.Linear(in_features,out_features,bias=True) 方法下面这个函数 使得8维度线性变换为6维度注：1.整个模型都是以 列向量为操作单位的（这么做其实是为了利用计算机的并行计算能力加快训练速度），所以维度指的是有多少列，比如左下角这个维度就是8。2.激活函数的引入，其实就是为了引入非线性的因素。这样就可以使得我们可以非线性的变换矩阵维度。下面是我们 构建的整体的 线性网络模型 1234567891011121314151617181920# 第二部 定义模型# 这个其实有点类似与 神经网络的结构了 每个线性层后面都加了一个sigmoid函数 做了次非线性变换class FullLeanerModel(torch.nn.Module): # 定义多层线性模型的结构 def __init__(self): super(FullLeanerModel,self).__init__() # 第一个线性转换 将8维转换为6维 self.linear1 = torch.nn.Linear(8,6) self.linear2 = torch.nn.Linear(6,4) self.linear3 = torch.nn.Linear(4,1) self.sigmoid = torch.nn.Sigmoid() # 前馈 运行输出结果 # 会被自动调用 是方法的重写 def forward(self,x): # 用一个变量比较 简单 x = self.sigmoid(self.linear1(x)) x = self.sigmoid(self.linear2(x)) x = self.sigmoid(self.linear3(x)) return x 实例化这个模型 12# 实例化模型FullLeanerModel = FullLeanerModel() Step3：定义损失函数和优化器123# 第三步 定义 损失函数和优化器criterion = torch.nn.BCELoss(reduction=&#x27;mean&#x27;)optimizer = torch.optim.SGD(FullLeanerModel.parameters(),lr=0.1) Step4：训练模型训练模型100次 12345678910111213141516# 第四步 训练&#x27;&#x27;&#x27; 前两步 就是正向传播 forward 1. 预测 标签 2. 预测 与 实际 算出损失值 3. 反向传播 backward 优化参数 4. 更新参数&#x27;&#x27;&#x27;for epoch in range(101): y_pred = FullLeanerModel(x_train) loss = criterion(y_pred,y_train) print(&#x27;迭代次数:&#x27;,epoch,&quot; 损失值:&quot;,loss.item()) optimizer.zero_grad() # 梯度清零 loss.backward() # 反向传播 optimizer.step() #更新参数 评估模型并预测输出所有层次的 权重和偏置123 # 输出各层 权重w 和 偏置bfor weight, bias in FullLeanerModel.state_dict().items(): # param is weight or bias(Tensor)print( weight,bias) 预测123# 预测y_yuce = FullLeanerModel(x_test)print(&quot;测试样本的预测值为&quot;,y_yuce.data,&quot;实际样本的标签值为&quot;,y_test.data) 完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import numpy as npimport torch# 读取数据&#x27;&#x27;&#x27; numpy读取本地文件 delimiter 分割号 dtype 读取数据类型 一般机器学习类的都是float32 因为显卡一般他内核里面是按32位工作的&#x27;&#x27;&#x27;xy = np.loadtxt(&#x27;./dataset/diabetes.csv&#x27;,delimiter=&quot;,&quot;,dtype=np.float32)# (759,9) 759行 9列print(xy.shape)# numpy 转 张量 最后一组样本我们当测试样本吧 其余的当训练样本x_train = torch.from_numpy(xy[:-1,:-1])print(x_train.shape)y_train = torch.from_numpy(xy[:-1,[-1]])# 当测试样本 的 一组数据x_test = torch.from_numpy(xy[-1:,:-1])print(x_test.shape)y_test = torch.from_numpy(xy[-1:,[-1]])# 第二部 定义模型# 这个其实有点类似与 神经网络的结构了 每个线性层后面都加了一个sigmoid函数 做了次非线性变换class FullLeanerModel(torch.nn.Module): # 定义多层线性模型的结构 def __init__(self): super(FullLeanerModel,self).__init__() # 第一个线性转换 将8维转换为6维 self.linear1 = torch.nn.Linear(8,6) self.linear2 = torch.nn.Linear(6,4) self.linear3 = torch.nn.Linear(4,1) self.sigmoid = torch.nn.Sigmoid() # 前馈 运行 def forward(self,x): # 用一个变量比较 简单 x = self.sigmoid(self.linear1(x)) x = self.sigmoid(self.linear2(x)) x = self.sigmoid(self.linear3(x)) return x# 实例化模型FullLeanerModel = FullLeanerModel()# 第三步 定义 损失函数和优化器criterion = torch.nn.BCELoss(reduction=&#x27;mean&#x27;)optimizer = torch.optim.SGD(FullLeanerModel.parameters(),lr=0.1)# 第四步 训练&#x27;&#x27;&#x27; 前两步 就是正向传播 forward 1. 预测 标签 2. 预测 与 实际 算出损失值 3. 反向传播 backward 优化参数 4. 更新参数&#x27;&#x27;&#x27;for epoch in range(101): y_pred = FullLeanerModel(x_train) loss = criterion(y_pred,y_train) print(&#x27;迭代次数:&#x27;,epoch,&quot; 损失值:&quot;,loss.item())![在这里插入图片描述](https://img-blog.csdnimg.cn/f600a0b6fbe24b60b35a0f2bc26f9444.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center) optimizer.zero_grad() # 梯度清零 loss.backward() # 反向传播 optimizer.step() #更新参数# 第五步 评估模型 # 输出各层 权重w 和 偏置bfor weight, bias in FullLeanerModel.state_dict().items(): # param is weight or bias(Tensor) print( weight,bias)# 预测y_yuce = FullLeanerModel(x_test)print(&quot;测试样本的预测值为&quot;,y_yuce.data,&quot;实际样本的标签值为&quot;,y_test.data) 实验结果可以看到 最后一个样本模型预测概率是 0.6529 。&gt;0.5 我们可以推测其标签就是1，而实际标签也是1，所以这个模型预测结果目前看是正确的。","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"刘二","slug":"刘二","permalink":"http://example.com/tags/%E5%88%98%E4%BA%8C/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"},{"name":"多维度特征","slug":"多维度特征","permalink":"http://example.com/tags/%E5%A4%9A%E7%BB%B4%E5%BA%A6%E7%89%B9%E5%BE%81/"}]},{"title":"Pytorch 实现 简易逻辑回归模型 —— 刘二","slug":"Pytorch 实现 简易逻辑回归模型 —— 刘二","date":"2021-09-12T03:30:01.000Z","updated":"2021-09-13T13:25:10.303Z","comments":true,"path":"2021/09/12/Pytorch 实现 简易逻辑回归模型 —— 刘二/","link":"","permalink":"http://example.com/2021/09/12/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%AE%80%E6%98%93%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/","excerpt":"","text":"@TOC 用Pytorch 实现 简单的逻辑回归。整个流程图可以如下图所示： 大致的设计步骤 分为5步 如下所示：第五步：是进行 评估模型并预测 1import torch Step1：准备数据 123# 准备数据x_data = torch.Tensor([[1.0], [2.0], [3.0]])y_data = torch.Tensor([[0], [0], [1]]) Step2：设计模型内涵的线性模型比较简单 为 $y=Wx+b$ 只有两个超参数 W 和 b 1234567891011121314# 第二步 设计模型# 构建一个线性模型类 所有的模型类都必须继承torch.nn.Moduleclass LogisticRegressionModel(torch.nn.Module): def __init__(self): # 调用父类的构造 这步必须得有 super(LogisticRegressionModel, self).__init__() # Linear 是一个模型类 这边实例化他给 linear # w 权重 = 1 b 偏置 = 1 self.linear = torch.nn.Linear(1, 1) def forward(self, x): # 线性模型之后 在外套sigmoid激活函数 y_pred = torch.sigmoid(self.linear(x)) return y_pred torch.nn.Linear(in_features,out_features,bias=True) 方法 实例化这个模型为 model 12# 实例化这个模型model = LogisticRegressionModel() Step3：构建损失函数和优化器12345# 第三步 构建损失函数和优化器# BCELoss Binary Cross Entropycriterion = torch.nn.BCELoss(reduction=&#x27;mean&#x27;) # size_average = True 的话 就 乘以 1/N 默认为true#model.parameters() 可以找到模型所有需要训练的参数optimizer = torch.optim.SGD(model.parameters(),lr=0.01) torch.nn.BCELoss(size_average=False) 方法用于创建一个 BCE 损失函数 torch.optim.SGD(…) 方法优化器选择 SGD 可调整学习率$w^{*} = w - α\\frac{\\partial L}{\\partial W}$ Step4: 训练模型 前两步 就是正向传播 forward 预测 标签 预测 与 实际 算出损失值 反向传播 backward 优化参数 更新参数 123456789# 第四步 训练for epoch in range(100): y_pred = model(x_data) loss = criterion(y_pred,y_data) print(&#x27;迭代次数:&#x27;,epoch,&quot; 损失值:&quot;,loss.item() ) optimizer.zero_grad() # 梯度清零 loss.backward() # 反向传播 optimizer.step() #更新参数 Step5: 评估模型并预测这边没有准备 测试集及其标签 输出 超参数 权重w 和 偏置b1234# 输出 超参数 权重w 和 偏置bprint(&#x27;w = &#x27;,model.linear.weight.item() )print(&#x27;b = &#x27;,model.linear.bias.item()) 预测 1234# 预测 x_test = torch.Tensor([[4.0]])y_test = model(x_test)print(&#x27;x为4.0 预测的 y值为：&#x27;,y_test.data) 实验结果 这边少了 怎么输出 准确率？ 可以画一下图12345678910111213import numpy as npimport matplotlib.pyplot as pltx = np.linspace(0, 10, 200)x_t = torch.Tensor(x).view((200, 1))y_t = model(x_t)y = y_t.data.numpy()plt.plot(x, y)plt.plot([0, 10], [0.5, 0.5], c=&#x27;r&#x27;)plt.xlabel(&#x27;Hours&#x27;)plt.ylabel(&#x27;Probability of Pass&#x27;)plt.grid()plt.show() 实现源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import torch# 第一步 载入数据x_data = torch.Tensor([[1.0], [2.0], [3.0]])y_data = torch.Tensor([[0], [0], [1]])# 第二步 设计模型# 构建一个线性模型类 所有的模型类都必须继承torch.nn.Moduleclass LogisticRegressionModel(torch.nn.Module): def __init__(self): # 调用父类的构造 这步必须得有 super(LogisticRegressionModel, self).__init__() # Linear 是一个模型类 这边实例化他给 linear # w 权重 = 1 b 偏置 = 1 self.linear = torch.nn.Linear(1, 1) def forward(self, x): # 线性模型之后 在外套sigmoid激活函数 y_pred = torch.sigmoid(self.linear(x)) return y_pred# 实例化这个模型model = LogisticRegressionModel()# 第三步 构建损失函数和优化器# BCELoss Binary Cross Entropycriterion = torch.nn.BCELoss(reduction=&#x27;mean&#x27;) # size_average = True 的话 就 乘以 1/N 默认为trueoptimizer = torch.optim.SGD(model.parameters(),lr=0.01) #model.parameters() 可以找到# 模型所有需要训练的参数# 第四步 训练&#x27;&#x27;&#x27; 前两步 就是正向传播 forward 1. 预测 标签 2. 预测 与 实际 算出损失值 3. 反向传播 backward 优化参数 4. 更新参数&#x27;&#x27;&#x27;for epoch in range(100): y_pred = model(x_data) loss = criterion(y_pred,y_data) print(&#x27;迭代次数:&#x27;,epoch,&quot; 损失值:&quot;,loss.item() ) optimizer.zero_grad() # 梯度清零 loss.backward() # 反向传播 optimizer.step() #更新参数# 第五步 评估模型# 输出 超参数 权重w 和 偏置bprint(&#x27;w = &#x27;,model.linear.weight.item() )print(&#x27;b = &#x27;,model.linear.bias.item())# 预测x_test = torch.Tensor([[4.0]])y_test = model(x_test)print(&#x27;x为4.0 预测的 y值为：&#x27;,y_test.data)import numpy as npimport matplotlib.pyplot as pltx = np.linspace(0, 10, 200)x_t = torch.Tensor(x).view((200, 1))y_t = model(x_t)y = y_t.data.numpy()plt.plot(x, y)plt.plot([0, 10], [0.5, 0.5], c=&#x27;r&#x27;)plt.xlabel(&#x27;Hours&#x27;)plt.ylabel(&#x27;Probability of Pass&#x27;)plt.grid()plt.show()","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"刘二","slug":"刘二","permalink":"http://example.com/tags/%E5%88%98%E4%BA%8C/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://example.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"}]},{"title":"神经网络与深度学习 邱锡鹏 学习笔记（机器学习）","slug":"神经网络与深度学习 邱锡鹏 学习笔记（机器学习）","date":"2021-09-11T07:46:01.000Z","updated":"2021-09-13T04:58:39.371Z","comments":true,"path":"2021/09/11/神经网络与深度学习 邱锡鹏 学习笔记（机器学习）/","link":"","permalink":"http://example.com/2021/09/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E9%82%B1%E9%94%A1%E9%B9%8F%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%89/","excerpt":"","text":"@TOC 机器学习机器学习看作一个从有限、高维、有噪声的数据上得到更一般性规律的泛化问题． 机器学习的三个基本要素 模型 学习准则 优化算法 模型我们不知道 输入和输出是如何对应联系的，也就是数学函数究竟是什么。 我们假设一个可能的函数集合 $ℱ$ ，称为 假设空间 。 我们根据 观测 各函数在测试集 $𝒟$ 的表现，从中选择更理想的假设模型。常用的假设空间分为 线性和非线性 两种 学习准则令训练集 是由 $𝑁$ 个独立同分布的样本组成，即每个样本 (𝒙, 𝑦) ∈ 𝒳 × 𝒴 是从 𝒳 和 𝒴 的联合空间中按照某个未知分布 $𝑝_{r}(𝒙, 𝑦)$ 独立地随机产生的 。 学习准则包括： 经验风险最小化 结构风险最小化 最大似然估计 最大后验估计 损失函数用来量化模型预测和真实标签之间的差异 0-1损失函数平方损失函数经常用在预测标签 $𝑦$ 为实数值的任务中 L(y,f(x;θ)) = \\frac{1}{2}(y-f(x;θ))平方损失函数一般不适用于分类问题 交叉熵损失函数一般用于分类问题 L(y,f(x;θ)) =-\\sum_{c=1}^{C}y_{c}\\log f_{c}(x;θ)例如，对于一个三分类的问题，一个样本的标签向量为 $𝒚 = [0, 0, 1]^{T}$，模型预测的标签分布为 $f(x;θ) = [0.3, 0.3, 0.4]^{T}$。则它们的交叉熵为 $−(0 × log(0.3) + 0 ×log(0.3) + 1 × log(0.4)) = − log(0.4)$ 过拟合和欠拟合 过拟合模型在训练集上错误率 很低，但是在未知数据上错误率很高，这就是所谓的过拟合。过拟合的 3种 方法： 参数在过拟合之前就停止更新；正则化Regularization；dropout 欠拟合模型不能很好地拟合训练数据，在训练集上的错误率比较高．欠拟合一般是由于模型能力不足造成的。 优化算法参数与超参数 参数：模型 𝑓(𝒙; 𝜃)中的 𝜃 称为模型的参数，可以通过优化算法进行学习 超参数：用来定义模型结构或优化策略的。 常见的超参数有：聚类算法中的类别个数、梯度下降法中的步长、正则化项的系数、神经网络的层数、支持向量机中的核函数等 参数的优化一般都是由优化器优化，而超参数优化是机器学习的 一个经验性很强的技术 ，通常是按照人的经验设定，或者通过搜索的方法对一组超参数组合进行不断试错调整。 批量梯度下降法缺点在于 局部最优问题用下面的迭代公式来计算计算训练集 𝒟 上风险函数的最小值：𝛼 一般称为学习率（Learning Rate）． 批量梯度下降法在 每次迭代时需要计算每个样本（也就是所有样本）上损失函数的梯度并求和。当训练集中的样本数量 𝑁 很大时，空间复杂度比较高，每次迭代的计算开销也很大． 随机梯度下降法缺点在于 局部最优问题，但相比于批量，更容易脱离局部最优 为了减少每次迭代的计算复杂度，我们也可以在每次迭代时只采集一个样本，计算这个样本损失函数的梯度并更新参数，即随机梯度下降法。当经过足够次数的迭代时，随机梯度下降 也可以收敛到局部最优解。 小批量梯度下降法现在 大规模的机器学习 常用这个。利用了计算机的并行计算能力每次迭代时，我们随机选取一小部分训练样本来计算梯度并更新参数，这样既可以兼顾随机梯度下降法的优点，也可以提高训练效率。 机器学习的简单示例——线性回归自变量数量 为1时称为 简单回归， 自变量数量大于1时称为 多元回归． 𝑓(𝒙; 𝒘) = 𝒘^{T}𝒙 不明白 先跳过偏差- 方差分解其用在，对如何对模型的拟合能力和复杂度之间取得一个良好的平衡，偏差与方差起这很好的分析和指导作用。 对于单个样本 𝒙，不同训练集 𝒟 得到模型 $𝑓_{D}(𝒙)$ 和最优模型 $𝑓^{*}(𝒙)$ 的期望 差距为： 偏差：指一个模型在 不同训练集 上的平均性能和最优模型的差异，可以用来衡量一个模型的拟合能力 方差：一个模型在不同训练集上的差异 ，可以用来衡量一个模型是否容易过拟合 ． 每个图的中心点为最优模型 $𝑓^{*}(𝒙)$，黑点为不同训练集𝐷 上得到的模型 $𝑓_{D}(𝒙)$ 高偏差低方差的情况，表示模型的泛化能力很好，但拟合能力不足 低偏差高方差的情况，表示模型的拟合能力很好，但泛化能力比较差．当训练数据比较少时会导致过拟合 总结的来看：模型在训练集上的错误率比较高时，说明模型的拟合能力不够，偏差比较高。这种情况可以通过 增加数据特征 提高模型复杂度 减小正则化系数 模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高．这种情况可以通过 降低模型复杂度 加大正则化系数 引入先验 此外，还有一种有效降低方差的方法为集成模型，即通过多个高方差模型的平均来降低方差． 学习算法分类按照训练样本提供的信息以及反馈方式的不同，将机器学习算法分为以下几类： 监督学习机器学习的特征 𝒙 和标签 𝑦 之间可以用 数学模型表示出来，并且训练集中每个样本都有标签。（数据集标签一般都需要由人工进行标注，成本很高）根据标签的类型还可以分为： 回归。 标签 𝑦 与 模型的输出 都是连续值 分类。 标签 𝑦 是离散的类别（符号）。这种模型又叫做 分类器 。 分类问题可以分为 二分类 多分类。 结构化学习。 特殊的分类问题。标签 𝒚 通常是结构化的对象，比如序列、树或图等。 由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将 𝒙 , 𝒚 映射为该空间中的联合特征向量 𝜙(𝒙, 𝒚)，预测模型可以写为 其中 Gen(𝒙) 表示输入 𝒙 的所有可能的输出目标集合．计算 arg max 的过程也称为解码（Decoding）过程，一般通过动态规划的方法来计算。 无监督学习 是指从不包含目标标签的训练样本中自动学习到一些有价值的信息。典型的无监督学习问题有 聚类 密度估计 特征学习 降维 强化学习 是一类通过交互来学习的机器学习算法．在强化学习中，智能体根据环境的状态做出一个动作，并得到即时或延时的奖励．智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报． 弱监督和半监督弱监督学习和半监督学习的方法，希望从大规模的无标注数据中充分挖掘有用的信息，降低对标注样本数量的要求。 强化学习和监督学习的不同在于，强化学习不需要显式地以“输入/输出对”的方式给出训练样本，是一种在线的学习机制。 数据的特征表示 需要将这些不同类型的数据转换为向量表示 。 如何选取有效的特征，具体可分为两种：特征选择和特征抽取。 传统的是和预测模型的学习分离的。 特征选择特征选择就是保留有用特征，移除冗余或无关的特征。方法包括 子集搜索和 L1正则化 子集搜索 过滤式方法：不依赖具体机器学习模型的特征选择方法。每次增加最有信息量的特征，或删除最没有信息量的特征。 包裹式方法（Wrapper Method）是使用后续机器学习模型的准确率作为评价来选择一个特征子集的方法．每次增加对后续机器学习模型最有用的特征，或删除对后续机器学习任务最无用的特征。 L1正则化由于 L1 正则化会导致稀疏特征，因此间接实现了特征选择． 特征抽取 构造一个新的特征空间，并将原始特征投影在新的空间中得到新的表示。 其方法分为 有监督和无监督两类。 监督的特征学习的目标是抽取对一个特定的预测任务最有用的特征，比如线性判别分析。 无监督的特征学习和具体任务无关，其目标通常是减少冗余信息和噪声，比如主成分分析PCA和自编码器 AE。 总结特征选择和特征抽取的优点是可以用较少的特征来表示原始特征中的大部分相关信息，去掉噪声信息，并进而提高计算效率和减小维度灾难。对于很多没有正则化的模型，特征选择和特征抽取非常必要。 经过特征选择或特征抽取后，特征的数量一般会减少，因此特征选择和特征抽取。 也经常称为维数约减或降维。 模型的评价指标 准确率（Accuracy） 错误率（Error Rate）： 1 - 准确率 查准率（Precision） 查全率（Recall） F值（F Measure） 如何理解 精确率、召回率和F值模型在测试集上的结果可以分为以下四种情况：比如我们现在 预测标签 C 真正例（TP）：样本的预测与实际标签相同 假负例（FN）：样本实际标签为C，模型预测错了 假正例（FP）：样本实际标签不是C，但模型预测成C了 真负例（TN）：样本实际为其他类，模型也预测为其他类 查准率类别 𝑐 的查准率 是所有预测为 类别C 的样本中 预测正确 的比例精确率 $P_{C}$ 的计算 公式为： P_{C} = \\frac{TP_{c}}{TP_{c}+FP_{c}}查全率类别𝑐的查全率 是所有真实标签为 类别𝑐 的样本中预测正确的比例： R_{C} = \\frac{TP_{c}}{TP_{c}+FN_{c}}F值F值（F Measure）是一个综合指标，为精确率和召回率的调和平均： F_{C} = \\frac{(1+β^{2})×P_{C}×R_{C}}{β^{2}×P_{C}+R_{C}}其中 𝛽 用于平衡精确率和召回率的重要性，一般取值为1．𝛽 = 1时的F值称为 F1 值，是精确率和召回率的调和平均． 宏平均和微平均为了计算分类算法在所有类别上的总体查准率、查全率和 F1值，经常使用两种平均方法，分别称为 宏平均 和 微平均 宏平均是每一类的性能指标的算术平均值 微平均是每一个样本的性能指标的算术平均值 理论和定理PAC学习理论：指该学习算法能够在多项式时间内从合理数量的训练数据中学习到一个近似正确的𝑓(𝒙)． 没有免费午餐定理没有免费午餐定理 就是不存在一种机器学习算法适合于任何领域或任务。 奥卡姆剃刀原理简单的模型泛化能力更好．如果有两个性能相近的模型，我们应该选择更简单的模型。因此，在机器学习的学习准则上，我们经常会引入参数正则化来限制模型能力，避免过拟合． 丑小鸭定理世界上不存在相似性的客观标准，一切相似性的标准都是主观的 归纳偏置预测模型前，先假设。比如在最近邻分类器中，我们会假设在特征空间中，一个小的局部区域中的大部分样本同属一类。在朴素贝叶斯分类器中，我们会假设每个特征的条件概率是互相独立的。","categories":[{"name":"深度学习基础-邱锡鹏","slug":"深度学习基础-邱锡鹏","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%82%B1%E9%94%A1%E9%B9%8F/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"邱锡鹏","slug":"邱锡鹏","permalink":"http://example.com/tags/%E9%82%B1%E9%94%A1%E9%B9%8F/"},{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"神经网络与深度学习 学习笔记（第一章）","slug":"神经网络与深度学习 学习笔记（第一章）","date":"2021-09-10T08:42:01.000Z","updated":"2021-09-13T04:58:33.835Z","comments":true,"path":"2021/09/10/神经网络与深度学习 学习笔记（第一章）/","link":"","permalink":"http://example.com/2021/09/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%89/","excerpt":"","text":"@TOC 机器学习浅层学习 学习一个预测模型．一般需要首先将数据表示为一组特征（Feature），特征的表示形式可以是连续的数值、离散的符号或其他形式．然后将这些特征输入到预测模型，并输出预测结果．这类机器学习可以看作浅层学习（Shallow Learning） 机器学习的步骤 数据预处理：对数据的原始形式进行初步的数据清理（比如去掉一些有缺失特征的样本，或去掉一些冗余的数据特征等）和加工（对数值特征进行缩放和归一化等），并构建成可用于训练机器学习模型的数据集． 特征提取：从数据的原始特征中提取一些对特定机器学习任务有用的高质量特征．比如在图像分类中提取边缘、尺度不变特征变换（Scale InvariantFeature Transform，SIFT）特征，在文本分类中去除停用词等． 特征转换：对特征进行进一步的加工，比如降维和升维． 很多特征转换方法也都是机器学习方法．降维包括特征抽取（Feature Extraction）和特征选择（FeatureSelection）两种途径．常用的特征转换方法有主成分分析（Principal Components Analysis，PCA）、 线性判别分析（Linear Discriminant Analysis，LDA）等． 预测：机器学习的核心部分，学习一个函数并进行预测． 注：很多的机器学习问题变成了特征工程（Feature Engineering）问题．开发一个机器学习系统的主要工作量都消耗在了预处理、特征提取以及特征转换上。 表示学习在表示学习中，有两个核心问题： 一是“什么是一个好的表示”；即表示 需要包含更高层的语义信息 二是“如何学习到好的表示”． 传统的特征学习一般是通过人为地设计一些准则，然后根据这些准则来选取有效的特征。 所以 特征的学习是和最终预测模型的学习分开进行的，因此学习到的特征不一定可以提升最终模型的性能． 概念将输入信息转换为有效的特征。如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作表示学习（Representation Learning）． 语义鸿沟语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性。 比如车，图片中每辆车的颜色和形状等属性都不尽相同，因此不同图片在像素级别上的表示（即底层特征）差异性也会非常大．但是我们理解这些图片是建立在比较抽象的高层语义概念上的 表示特征的方式 局部表示：例如，one-hot向量 表示颜色。 缺点在于多个颜色就多个列或者行 分布式表示：RGB 表示颜色 嵌入嵌入通常指将一个度量空间中的一些对象映射到另一个低维的度量空间中，并尽可能保持不同对象之间的拓扑关系．比如自然语言中词的分布式表示，也经常叫作词嵌入 例如：3维one-hot向量空间和一个2维嵌入空间的对比在低维的嵌入空间中，每个样本都不在坐标轴上，样本之间可以计算相似度． 深度学习什么是深度“深度”是指原始数据进行非线性特征转换的次数 深度学习的优点深度学习，其主要目的是从数据中自动学习到有效的特征表示 其抽象在 数据通过多层的特征转换，学习到的表示可以代替人工设计的特征，从而避免“特征工程”其是 一种 端到端的学习方式 在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标．在端到端学习中，一般不需要明确地给出不同模块或阶段的功能，中间过程不需要人为干预 深度学习的关键问题深度学习需要解决的关键问题是 贡献度分配问题，即一个系统中不同的 组件 或其 参数 对最终系统输出结果的贡献或影响 目前，深度学习采用的模型主要是神经网络模型，其主要原因是神经网络模型可以使用 误差反向传播算法 ，从而可以比较好地解决贡献度分配问题 深度学习相关的学术会议 国际表示学习会议 ICLR ：主要聚焦于深度学习 神经信息处理系统年会 NeurIPS ：交叉学科会议，但偏重于机器学习 国际机器学习会议 ICML：机器学习顶级会议 国际人工智能联合会议 IJCAI ：人工智能领域最顶尖的综合性会议 美国人工智能协会年会 AAAI ：人工智能领域的顶级会议 计算机视觉领域 计算机视觉与模式识别大会 CVPR 国际计算机视觉会议 ICCV 自然语言处理领域 计算语言学年会 ACL 自然语言处理实证方法大会 EMNLP","categories":[{"name":"深度学习基础-邱锡鹏","slug":"深度学习基础-邱锡鹏","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%82%B1%E9%94%A1%E9%B9%8F/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"邱锡鹏","slug":"邱锡鹏","permalink":"http://example.com/tags/%E9%82%B1%E9%94%A1%E9%B9%8F/"},{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"Pytorch 实现 线性回归模型 —— 刘二","slug":"Pytorch 实现 线性回归模型 —— 刘二","date":"2021-09-08T02:40:01.000Z","updated":"2021-09-17T12:28:47.682Z","comments":true,"path":"2021/09/08/Pytorch 实现 线性回归模型 —— 刘二/","link":"","permalink":"http://example.com/2021/09/08/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/","excerpt":"","text":"@TOC 用Pytorch 实现 线性模型。整个流程图可以如下图所示：大致的设计步骤 分为5步 如下所示：第五步：是进行 评估模型并预测 1import torch Step1：准备数据 123# 准备数据x_data = torch.Tensor([[1.0],[2.0],[3.0]])y_data = torch.Tensor([[2.0],[4.0],[6.0]]) Step2：设计模型这边的线性模型比较简单 为 $y=Wx+b$ 只有两个超参数 W 和 b要通过输入的维度和 输出的维度，才能明确 $W$ 和 $b$ 的维度 1234567891011121314# 构建一个线性模型类 所有的模型类都必须继承torch.nn.Moduleclass LinearModel(torch.nn.Module): # 构造方法 def __init__(self): # 调用父类的构造 这步必须得有 super(LinearModel,self).__init__() # Linear 是一个模型类 这边实例化他给 对象linear # w 权重 = 1 b 偏执 = 1 self.linear = torch.nn.Linear(1,1) # 方法重写 def forward(self,x): # 预测值 y_pred = self.linear(x) return y_pred torch.nn.Linear(in_features,out_features,bias=True) 方法 实例化这个模型为 model 12# 实例化这个模型model = LinearModel() Step3：构建损失函数和优化器1234# 损失函数：mean squared errorcriterion = torch.nn.MSELoss(reduction=&#x27;sum&#x27;) # size_average = True 的话 就 乘以 1/N 默认为true# model.parameters() 可以找到模型所有需要训练的参数 优化器：SGDoptimizer = torch.optim.SGD(model.parameters(),lr=0.01) torch.nn.MSELoss(size_average=False) 方法用于创建一个MSE损失函数 torch.optim.SGD(…) 方法优化器选择 SGD 可调整学习率$w^{*} = w - α\\frac{\\partial L}{\\partial W}$ Step4: 训练模型 前两步 就是正向传播 forward 预测 标签 预测 与 实际 算出损失值 反向传播 backward 优化参数 更新参数 123456789for epoch in range(100): #数据跑 99次 range是不到那个数字的 y_pred = model(x_data) loss = criterion(y_pred,y_data) print(&#x27;迭代次数:&#x27;,epoch,&quot; 损失值:&quot;,loss) optimizer.zero_grad() # 梯度清零 loss.backward() # 反向传播 optimizer.step() #更新参数 Step5: 评估模型并预测这边没有准备 测试集及其标签 输出 超参数 权重w 和 偏置b1234# 输出 超参数 权重w 和 偏置bprint(&#x27;w = &#x27;,model.linear.weight.item() )print(&#x27;b = &#x27;,model.linear.bias.item()) 预测 1234# 预测 x_test = torch.Tensor([[4.0]])y_test = model(x_test)print(&#x27;x为4.0 预测的 y值为：&#x27;,y_test.data) 实验结果 这边少了 怎么输出 准确率？ 实现源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import torch# 准备数据x_data = torch.Tensor([[1.0],[2.0],[3.0]])y_data = torch.Tensor([[2.0],[4.0],[6.0]])# 第二步 设计模型class LinearModel(torch.nn.Module): def __init__(self): # 调用父类的构造 这步必须得有 super(LinearModel,self).__init__() # Linear 是一个模型类 这边实例化他给 linear # w 权重 = 1 b 偏执 = 1 self.linear = torch.nn.Linear(1,1) def forward(self,x): y_pred = self.linear(x) return y_pred# 实例化这个模型model = LinearModel()# 第三步 构建损失函数和优化器# mean squared errorcriterion = torch.nn.MSELoss(reduction=&#x27;sum&#x27;) # size_average = True 的话 就 乘以 1/N 默认为trueoptimizer = torch.optim.SGD(model.parameters(),lr=0.01) #model.parameters() 可以找到# 模型所有需要训练的参数# 第四步 训练&#x27;&#x27;&#x27; 前两步 就是正向传播 forward 1. 预测 标签 2. 预测 与 实际 算出损失值 3. 反向传播 backward 优化参数 4. 更新参数&#x27;&#x27;&#x27;for epoch in range(100): y_pred = model(x_data) loss = criterion(y_pred,y_data) print(&#x27;迭代次数:&#x27;,epoch,&quot; 损失值:&quot;,loss) optimizer.zero_grad() # 梯度清零 loss.backward() # 反向传播 optimizer.step() #更新参数# 第五步 评估模型# 输出 超参数 权重w 和 偏置bprint(&#x27;w = &#x27;,model.linear.weight.item() )print(&#x27;b = &#x27;,model.linear.bias.item())# 预测x_test = torch.Tensor([[4.0]])y_test = model(x_test)print(&#x27;x为4.0 预测的 y值为：&#x27;,y_test.data)","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"刘二","slug":"刘二","permalink":"http://example.com/tags/%E5%88%98%E4%BA%8C/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"}]},{"title":"Keras  RNN 实现 MNIST 手写数字识别","slug":"Keras  RNN 实现 MNIST 手写数字识别","date":"2021-09-04T02:40:01.000Z","updated":"2021-09-17T12:25:56.550Z","comments":true,"path":"2021/09/04/Keras  RNN 实现 MNIST 手写数字识别/","link":"","permalink":"http://example.com/2021/09/04/Keras%20%20RNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/","excerpt":"","text":"@TOC 我们就以 MNIST数据集的手写识别 为例子 导入需要的包首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便） 1234567891011import numpy as npfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.layers.recurrent import SimpleRNNfrom tensorflow.keras.optimizers import Adamimport matplotlib.pyplot as pltimport matplotlibmatplotlib.use(&#x27;TkAgg&#x27;)print(matplotlib.get_backend()) 载入MNIST 数据该数据集一共有训练集 6w 张，测试集 1w 张1(train_images,train_labels),(test_images,test_labels)= mnist.load_data() 可以查看一下图像和标签 是什么 12345678# 数字5print(train_images[0])# 标签5print(train_labels[0])# 数字7print(test_images[0])# 标签7print(test_labels[0]) 可以 打印 一下图片 看看是什么样子1234plt.imshow(train_images[0])plt.show()plt.imshow(test_images[0])plt.show() 图片数据处理查看图片原有 shape1234# train_images (60000, 28, 28)print(&#x27;train_images&#x27;, train_images.shape)# train_labels (60000,)print(&#x27;train_labels&#x27;, train_labels.shape) 图片数据处理：归一化除以255.0 是为了归一化，使得元素点 全部变为在 0-1 之间123456789# 数据处理# (60000, 28, 28) train_images_scale = train_images/255.0test_images_scale = test_images/255.0# train_images变换后 (60000, 28, 28)print(&#x27;train_images变换后&#x27;, train_images_scale.shape)# test_images变换后 (10000, 28, 28)print(&#x27;test_images变换后&#x27;, test_images_scale.shape) 标签数据处理：转换成 one hot 格式12345678910# 换 one hot 格式 共十个分类# np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。train_labels_hot = np_utils.to_categorical(train_labels,num_classes=10)test_labels_hot = np_utils.to_categorical(test_labels,num_classes=10)# train_labels (60000, 10)print(&#x27;train_labels&#x27;,train_labels_hot.shape)# test_labels (10000, 10)print(&#x27;test_labels&#x27;,test_labels_hot.shape) 构建模型先定义 RNN 所需的参数 12345678# 定义rnn 的参数# 数据长度 一行一共有28个元素input_size = 28# 序列长度 一共有28个序列 也就是28行time_steps = 28# 隐藏层cell个数cell_size = 50 再定义 RNN 模型123456789model = Sequential()#循环神经网络model.add(SimpleRNN( units=cell_size, #输出 input_shape=(time_steps,input_size) #输入))# 输出层model.add(Dense(input_dim=cell_size,units=10,activation=&quot;softmax&quot;))优化器使用 Adam， 损失函数 选择 交叉熵 并编译 1234# 定义优化器 10的 -4次方adam = Adam(learning_rate=1e-4)model.compile(optimizer= adam,loss=&#x27;categorical_crossentropy&#x27;,metrics=&quot;accuracy&quot;) 训练模型 一共 60000张训练图片 按批次训练 一批次64张 一共6w/64 个批次训练完一轮6w张，表示一个epoch123# 一共 60000张训练图片 按批次训练 一批次64张 # 训练完一轮6w张，表示一个epochmodel.fit(train_images_scale,train_labels_hot,batch_size=64,epochs=1) 评估模型就是在 测试集上的表现1loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)也可以看一下 在训练集上的表现 1loss,accuracy = model.evaluate(train_images_scale,train_labels_hot) 下面是 结果（上训练集 下测试集） 预测数据**有问题 输入的 数据维度不对 不知道错哪了？？？** 123456# 看下输入的形状# # 预测数据print(test_images_scale[0].shape)print(model.predict((test_images_scale[0]/255.0)))print(np.argmax(model.predict((test_images_scale[0]/255.0))))print(test_labels[0]) 输入的图像 变换后形状模型预测 数据 输出的概率分类结果选出其中最大的 以及 实际图片标签 均为 数字7","categories":[{"name":"Keras","slug":"Keras","permalink":"http://example.com/categories/Keras/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://example.com/tags/Tensorflow/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/tags/Keras/"},{"name":"RNN","slug":"RNN","permalink":"http://example.com/tags/RNN/"}]},{"title":"Keras  CNN 实现 MNIST 手写数字识别","slug":"Keras  CNN 实现 MNIST 手写数字识别","date":"2021-09-03T02:40:01.000Z","updated":"2021-09-17T12:26:02.683Z","comments":true,"path":"2021/09/03/Keras  CNN 实现 MNIST 手写数字识别/","link":"","permalink":"http://example.com/2021/09/03/Keras%20%20CNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/","excerpt":"","text":"@TOC 我们就以 MNIST数据集的手写识别 为例子 导入需要的包首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便） 12345678910import numpy as npfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flattenfrom tensorflow.keras.optimizers import Adamimport matplotlib.pyplot as pltimport matplotlibmatplotlib.use(&#x27;TkAgg&#x27;)print(matplotlib.get_backend()) 载入MNIST 数据该数据集一共有训练集 6w 张，测试集 1w 张1(train_images,train_labels),(test_images,test_labels)= mnist.load_data() 可以查看一下图像和标签 是什么 12345678# 数字5print(train_images[0])# 标签5print(train_labels[0])# 数字7print(test_images[0])# 标签7print(test_labels[0]) 可以 打印 一下图片 看看是什么样子1234plt.imshow(train_images[0])plt.show()plt.imshow(test_images[0])plt.show() 图片数据处理查看图片原有 shape1234# train_images (60000, 28, 28)print(&#x27;train_images&#x27;, train_images.shape)# train_labels (60000,)print(&#x27;train_labels&#x27;, train_labels.shape) 图片数据处理：变成四维 ，并归一化变维度的 -1 是个通配符，系统会自动完成应该变成多少除以255.0 是为了归一化，使得元素点 全部变为在 0-1 之间1234567891011# 数据处理# CNN 输入的是一张图片# 将 (60000, 28, 28) -&gt; (60000, 28, 28, 1) 变成四维# 第四个 1 表示的为深度 黑白图像为 1 彩色图像为 3train_images_scale = train_images.reshape(-1, train_images.shape[1] ,train_images.shape[2],1)/255.0test_images_scale = test_images.reshape(-1,test_images.shape[1], test_images.shape[2],1)/255.0# train_images变换后 (60000, 28, 28, 1)print(&#x27;train_images变换后&#x27;, train_images_scale.shape)# test_images变换后 (10000, 28, 28, 1)print(&#x27;test_images变换后&#x27;, test_images_scale.shape) 标签数据处理：转换成 one hot 格式12345678910# 换 one hot 格式 共十个分类# np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。train_labels_hot = np_utils.to_categorical(train_labels,num_classes=10)test_labels_hot = np_utils.to_categorical(test_labels,num_classes=10)# train_labels (60000, 10)print(&#x27;train_labels&#x27;,train_labels_hot.shape)# test_labels (10000, 10)print(&#x27;test_labels&#x27;,test_labels_hot.shape) 构建模型思路是 卷积 再池化 重复步骤 再压平给 全身神经网络12345678910111213141516171819202122232425262728293031model = Sequential()#第一个卷积层&#x27;&#x27;&#x27; input shape 激入平面 filters 卷积核/过滤器 个数 kernel_size 卷积窗口大小 strides 步长 padding（边界填充）padding方式 same/valid activation 激活函数&#x27;&#x27;&#x27;# 用same 保持整个还是 28 × 28model.add(Convolution2D( input_shape=(28,28,1),filters=32,kernel_size=5,strides=1,padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;))#第一个池化层model.add(MaxPooling2D( pool_size=2,strides=2,padding=&#x27;same&#x27;))#第二卷积层model.add(Convolution2D(64,5,strides=1,padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;))#第二个池化层model.add(MaxPooling2D( pool_size=2,strides=2,padding=&#x27;same&#x27;))#把第二个池化层的前出扁平化为1维model.add(Flatten())#第一个全连接层model.add(Dense(units=1024,activation=&#x27;relu&#x27;))#Dropoutmodel.add(Dropout(0.5))#第二个全连接层model.add(Dense(units=10,input_dim=1024,activation=&#x27;softmax&#x27;)) 优化器使用 Adam， 损失函数 选择 交叉熵 并编译 1234# 定义优化器 10的 -4次方adam = Adam(learning_rate=1e-4)model.compile(optimizer= adam,loss=&#x27;categorical_crossentropy&#x27;,metrics=&quot;accuracy&quot;) 训练模型 一共 60000张训练图片 按批次训练 一批次64张 一共6w/64 个批次训练完一轮6w张，表示一个epoch123# 一共 60000张训练图片 按批次训练 一批次64张 # 训练完一轮6w张，表示一个epochmodel.fit(train_images_scale,train_labels_hot,batch_size=64,epochs=1)上图只跑了一次 epoch，因为笔记本太慢了注意： 最好用 GPU 来跑 ，不然笔记本非常的慢 评估模型就是在 测试集上的表现1loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)也可以看一下 在训练集上的表现 1loss,accuracy = model.evaluate(train_images_scale,train_labels_hot) 下面是 结果（上训练集 下测试集） 预测数据12345# 看下输入的形状print(test_images_scale[0].reshape(-1,28,28,1).shape)print(model.predict((test_images_scale[0].reshape(-1,28,28,1))))print(np.argmax(model.predict((test_images_scale[0].reshape(-1,28,28,1)))))print(test_labels[0]) 输入的图像 变换后形状模型预测 数据 输出的概率分类结果选出其中最大的 以及 实际图片标签 均为 数字7","categories":[{"name":"Keras","slug":"Keras","permalink":"http://example.com/categories/Keras/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://example.com/tags/Tensorflow/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/tags/Keras/"},{"name":"CNN","slug":"CNN","permalink":"http://example.com/tags/CNN/"}]},{"title":"Keras MNIST 过拟合问题解决：Dropout 与 正则化","slug":"Keras MNIST 过拟合问题解决：Dropout 与 正则化","date":"2021-09-02T02:40:01.000Z","updated":"2021-09-17T12:26:09.961Z","comments":true,"path":"2021/09/02/Keras MNIST 过拟合问题解决：Dropout 与 正则化/","link":"","permalink":"http://example.com/2021/09/02/Keras%20MNIST%20%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%EF%BC%9ADropout%20%E4%B8%8E%20%E6%AD%A3%E5%88%99%E5%8C%96/","excerpt":"","text":"@TOC 我们就以 MNIST数据集的手写识别 为例子做 过拟合问题的应用 包括 Dropout 和 正则化 导入需要的包首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便） 使用Dropout 需要导入的包需要 导入 另一个包 keras.12345678910import numpy as npfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Densefrom tensorflow.keras.optimizers import SGDimport matplotlib.pyplot as pltimport matplotlibmatplotlib.use(&#x27;TkAgg&#x27;)print(matplotlib.get_backend()) 使用 正则化 需要导入的包layers 层中 引入 keras.regularizers 中 l2 范式1234567891011import numpy as npfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Densefrom tensorflow.keras.optimizers import SGDfrom tensorflow.keras.regularizers import l2import matplotlib.pyplot as pltimport matplotlibmatplotlib.use(&#x27;TkAgg&#x27;)print(matplotlib.get_backend()) 载入MNIST 数据该数据集一共有训练集 6w 张，测试集 1w 张1(train_images,train_labels),(test_images,test_labels)= mnist.load_data() 可以查看一下图像和标签 是什么 12345678# 数字5print(train_images[0])# 标签5print(train_labels[0])# 数字7print(test_images[0])# 标签7print(test_labels[0]) 可以 打印 一下图片 看看是什么样子1234plt.imshow(train_images[0])plt.show()plt.imshow(test_images[0])plt.show() 图片数据处理查看图片原有 shape1234# train_images (60000, 28, 28)print(&#x27;train_images&#x27;, train_images.shape)# train_labels (60000,)print(&#x27;train_labels&#x27;, train_labels.shape) 图片数据处理：将图片压平123456789# 数据处理# 将 (60000, 28, 28) -&gt; (60000, 784) 压平图片train_images_scale = train_images.reshape(train_images.shape[0], train_images.shape[1] * train_images.shape[2])/255.0test_images_scale = test_images.reshape(test_images.shape[0], test_images.shape[1] * test_images.shape[2])/255.0# train_images变换后 (60000, 784)print(&#x27;train_images变换后&#x27;, train_images_scale.shape)# test_images变换后 (10000, 784)print(&#x27;test_images变换后&#x27;, test_images_scale.shape) 标签数据处理：转换成 one hot 格式12345678910# 换 one hot 格式 共十个分类# np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。train_labels_hot = np_utils.to_categorical(train_labels,num_classes=10)test_labels_hot = np_utils.to_categorical(test_labels,num_classes=10)# train_labels (60000, 10)print(&#x27;train_labels&#x27;,train_labels_hot.shape)# test_labels (10000, 10)print(&#x27;test_labels&#x27;,test_labels_hot.shape) 构建模型添加 Dropout12345678910model = Sequential()# 输入压平图像 维度为784 输出为 10分类# 加个隐层model.add(Dense(units=200,input_dim=784,bias_initializer=&quot;one&quot;,activation=&quot;tanh&quot;))# 上层40%的神经元不工作model.add(Dropout(0.4))model.add(Dense(units=100,input_dim=200,bias_initializer=&quot;one&quot;,activation=&quot;tanh&quot;))# 上层40%的神经元不工作model.add(Dropout(0.4))model.add(Dense(units=10,input_dim=100,bias_initializer=&quot;one&quot;,activation=&quot;softmax&quot;)) 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵 1234# 重新定义 sgd 优化器 加速一下学习率sgd = SGD(learning_rate=0.2)# 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵model.compile(optimizer= sgd,loss=&#x27;categorical_crossentropy&#x27;,metrics=&quot;accuracy&quot;) 添加 正则化项123456model = Sequential()# 输入压平图像 维度为784 输出为 10分类# 加个隐层model.add(Dense(units=200,input_dim=784,bias_initializer=&quot;one&quot;,activation=&quot;tanh&quot;,kernel_initializer=l2(0.003)))model.add(Dense(units=100,input_dim=200,bias_initializer=&quot;one&quot;,activation=&quot;tanh&quot;,kernel_initializer=l2(0.003)))model.add(Dense(units=10,input_dim=100,bias_initializer=&quot;one&quot;,activation=&quot;softmax&quot;,kernel_initializer=l2(0.003))) 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵 1234# 重新定义 sgd 优化器 加速一下学习率sgd = SGD(learning_rate=0.2)# 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵model.compile(optimizer= sgd,loss=&#x27;categorical_crossentropy&#x27;,metrics=&quot;accuracy&quot;) 训练模型 一共 60000张训练图片 按批次训练 一批次32张 一共6w/32 = 1875 个批次训练完一轮6w张，表示一个epoch123# 一共 60000张训练图片 按批次训练 一批次32张 一共6w/32 = 1875 个批次# 训练完一轮6w张，表示一个epochmodel.fit(train_images_scale,train_labels_hot,batch_size=32,epochs=10) 评估模型Dropout的结果就是在 测试集上的表现1loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)也可以看一下 在训练集上的表现 1loss,accuracy = model.evaluate(train_images_scale,train_labels_hot) 下面是 结果（上训练集 下测试集） 正则化的结果就是在 测试集上的表现1loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)也可以看一下 在训练集上的表现 1loss,accuracy = model.evaluate(train_images_scale,train_labels_hot) 下面是 结果（上训练集 下测试集） 出现了错误 不知道是哪里的问题哎？ 1TypeError: __call__() got an unexpected keyword argument &#x27;dtype&#x27; 预测数据12345678# 看下输入的形状print(test_images_scale[0].reshape(-1,784).shape)# 模型预测 输出每个分类的 概率print(model.predict((test_images_scale[0].reshape(-1,784))))# 选取最大的那个 就是预测的标签print(np.argmax(model.predict((test_images_scale[0].reshape(-1,784)))))# 实际该图片的 标签print(test_labels[0]) 输入的图像 变换后形状模型预测 数据 输出的概率分类结果选出其中最大的 以及 实际图片标签 均为 数字7","categories":[{"name":"Keras","slug":"Keras","permalink":"http://example.com/categories/Keras/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://example.com/tags/Tensorflow/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/tags/Keras/"}]},{"title":"Keras  MNIST（数字识别）数据集分类（普通全神经网络）","slug":"Keras  MNIST（数字识别）数据集分类（普通全神经网络）","date":"2021-08-30T02:40:01.000Z","updated":"2021-09-17T12:26:21.379Z","comments":true,"path":"2021/08/30/Keras  MNIST（数字识别）数据集分类（普通全神经网络）/","link":"","permalink":"http://example.com/2021/08/30/Keras%20%20MNIST%EF%BC%88%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB%EF%BC%88%E6%99%AE%E9%80%9A%E5%85%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89/","excerpt":"","text":"@TOC 导入需要的包首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便） 12345678910import numpy as npfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Densefrom tensorflow.keras.optimizers import SGDimport matplotlib.pyplot as pltimport matplotlibmatplotlib.use(&#x27;TkAgg&#x27;)print(matplotlib.get_backend()) 载入MNIST 数据该数据集一共有训练集 6w 张，测试集 1w 张1(train_images,train_labels),(test_images,test_labels)= mnist.load_data() 可以查看一下图像和标签 是什么 12345678# 数字5print(train_images[0])# 标签5print(train_labels[0])# 数字7print(test_images[0])# 标签7print(test_labels[0]) 可以 打印 一下图片 看看是什么样子1234plt.imshow(train_images[0])plt.show()plt.imshow(test_images[0])plt.show() 图片数据处理查看图片原有 shape1234# train_images (60000, 28, 28)print(&#x27;train_images&#x27;, train_images.shape)# train_labels (60000,)print(&#x27;train_labels&#x27;, train_labels.shape) 图片数据处理：将图片压平123456789# 数据处理# 将 (60000, 28, 28) -&gt; (60000, 784) 压平图片train_images_scale = train_images.reshape(train_images.shape[0], train_images.shape[1] * train_images.shape[2])/255.0test_images_scale = test_images.reshape(test_images.shape[0], test_images.shape[1] * test_images.shape[2])/255.0# train_images变换后 (60000, 784)print(&#x27;train_images变换后&#x27;, train_images_scale.shape)# test_images变换后 (10000, 784)print(&#x27;test_images变换后&#x27;, test_images_scale.shape) 标签数据处理：转换成 one hot 格式12345678910# 换 one hot 格式 共十个分类# np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。train_labels_hot = np_utils.to_categorical(train_labels,num_classes=10)test_labels_hot = np_utils.to_categorical(test_labels,num_classes=10)# train_labels (60000, 10)print(&#x27;train_labels&#x27;,train_labels_hot.shape)# test_labels (10000, 10)print(&#x27;test_labels&#x27;,test_labels_hot.shape) 构建模型只有输入和输出层， 输入压平图像 维度为784 输出为 10分类123model = Sequential()# 输入压平图像 维度为784 输出为 10分类model.add(Dense(units=10,input_dim=784,bias_initializer=&quot;one&quot;,activation=&quot;softmax&quot;))优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵 1234# 重新定义 sgd 优化器 加速一下学习率sgd = SGD(learning_rate=0.2)# 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵model.compile(optimizer= sgd,loss=&#x27;categorical_crossentropy&#x27;,metrics=&quot;accuracy&quot;) 训练模型 一共 60000张训练图片 按批次训练 一批次32张 一共6w/32 = 1875 个批次训练完一轮6w张，表示一个epoch123# 一共 60000张训练图片 按批次训练 一批次32张 一共6w/32 = 1875 个批次# 训练完一轮6w张，表示一个epochmodel.fit(train_images_scale,train_labels_hot,batch_size=32,epochs=10) 评估模型就是在 测试集上的表现1loss,accuracy = model.evaluate(test_images_scale,test_labels_hot) 预测数据12345678# 看下输入的形状print(test_images_scale[0].reshape(-1,784).shape)# 模型预测 输出每个分类的 概率print(model.predict((test_images_scale[0].reshape(-1,784))))# 选取最大的那个 就是预测的标签print(np.argmax(model.predict((test_images_scale[0].reshape(-1,784)))))# 实际该图片的 标签print(test_labels[0]) 输入的图像 变换后形状模型预测 数据 输出的概率分类结果选出其中最大的 以及 实际图片标签 均为 数字7","categories":[{"name":"Keras","slug":"Keras","permalink":"http://example.com/categories/Keras/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://example.com/tags/Tensorflow/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/tags/Keras/"}]},{"title":"keras 包方法集合","slug":"keras 包方法集合","date":"2021-08-29T02:18:01.000Z","updated":"2021-09-17T12:26:15.853Z","comments":true,"path":"2021/08/29/keras 包方法集合/","link":"","permalink":"http://example.com/2021/08/29/keras%20%E5%8C%85%E6%96%B9%E6%B3%95%E9%9B%86%E5%90%88/","excerpt":"","text":"@TOC keras.util 库 np_utilsnp_utils.to_categoricalnp_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。 1train_labels_hot = np_utils.to_categorical(train_labels,num_classes=10) 如将 $[1,2,3,……4]$ 转化成：这样的形态。","categories":[{"name":"Keras","slug":"Keras","permalink":"http://example.com/categories/Keras/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/tags/Keras/"}]},{"title":"各类 优化器(调参工具) 详解与选择","slug":"各类 优化器(调参工具) 详解与选择","date":"2021-08-28T02:40:01.000Z","updated":"2021-08-30T02:52:24.871Z","comments":true,"path":"2021/08/28/各类 优化器(调参工具) 详解与选择/","link":"","permalink":"http://example.com/2021/08/28/%E5%90%84%E7%B1%BB%20%E4%BC%98%E5%8C%96%E5%99%A8(%E8%B0%83%E5%8F%82%E5%B7%A5%E5%85%B7)%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/","excerpt":"","text":"@TOC sgdadam","categories":[{"name":"优化器","slug":"优化器","permalink":"http://example.com/categories/%E4%BC%98%E5%8C%96%E5%99%A8/"}],"tags":[{"name":"优化器","slug":"优化器","permalink":"http://example.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"}]},{"title":"Keras 构建 线性模型和非线性模型","slug":"Keras 构建 线性模型和非线性模型","date":"2021-08-24T09:22:01.000Z","updated":"2021-09-17T12:26:29.678Z","comments":true,"path":"2021/08/24/Keras 构建 线性模型和非线性模型/","link":"","permalink":"http://example.com/2021/08/24/Keras%20%E6%9E%84%E5%BB%BA%20%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"@TOC 预测线性模型使用的数据 是我们随机生成的、首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便） 12345678910import kerasimport numpy as npimport matplotlibimport matplotlib.pyplot as plt# matplotlib.use(&#x27;TkAgg&#x27;)# print(matplotlib.get_backend())# Sequential按顺序构成的模型from keras.models import Sequential# Dense全连接层from keras.layers import Dense 先准备我们需要的数据。随机生成100个随机值x，并随机产生100个噪声值。我们按 $y=0.1x+0.2$ 的公式，得到对应的y标签值。 1234567# 使用numpy 生成100个 随机点x_data = np.random.rand(100)# 测试集，但其实都是随机的 用x_data 也可以x_pre = np.random.rand(100)# 噪声 使得每个点不是 均匀在一条直线上noise = np.random.normal(0,0.01,x_data.shape)y_data = x_data * 0.1 + 0.2 + noise 可以将 100个点的分布图画出。 注意图的显示可能有问题，自行解决一下哦。12plt.scatter(x_data,y_data)plt.show() Q：为什么我们要引入 噪声呢？答：引入噪声可以让我们的数据更加的离散分布 在 我们设计的线性模型上。 使得假设的数据更加的合理。 如下图所示 。 橙色的是我们设定的线性模型，蓝色的是 加入噪声以后的数据分布 使用 keras 中的 Sequential （顺序构成的模型） 构建模型 123456789# 构建一个顺序模型model = Sequential()# 在模型中添加一个全连接层# units 输出的维度# input_dim 输入的维度model.add(Dense(units=1,input_dim=1))# sgd 随机梯度下降法# mse Mean Squared Error 均方误差model.compile(optimizer=&#x27;sgd&#x27;,loss=&#x27;mse&#x27;) 之后我们就按照批次训练。 共训练3001个批次。有两种写法。方法一：用一个循环体，循环3001次； 每500次 打印一次 损失值。1234567# 训练3001个批次for step in range(3001): #每次训练一个批次 cost = model.train_on_batch(x_data,y_data) # 每500个 batch 打印一次 cost值 if step % 500 == 0: print(&quot;cost:&quot;,cost)方法二： 直接使用 model.fit () 函数 1model.fit(x_data,y_data,epochs=3001) 可以查看 参数值 W （权重）和 b（偏置值） 12W,b = model.layers[0].get_weights()print(&#x27;W：&#x27;,W,&#x27;b:&#x27;,b) 预测 测试集的 结果 使用 model.predict () 函数 12# 测试集 输入网络中，得到预测值 y_predy_pred = model.predict(x_pre) 可以再 把预测的 图打出来 12plt.scatter(x_pre,y_pred)plt.show() 我们的训练 使用方法二 结果如图所示： 预测非线性模型使用的数据 也是我们随机生成的 首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便） 注意SGD 需要 tensorflow.keras.optimizers 导入 123456789import numpy as npimport matplotlibimport matplotlib.pyplot as pltmatplotlib.use(&#x27;TkAgg&#x27;)# Sequential按顺序构成的模型from keras.models import Sequential# Dense全连接层from keras.layers import Dense,Activationfrom tensorflow.keras.optimizers import SGD 先准备我们需要的数据。用等差数列生成200个值x，并随机产生200个噪声值。我们按 $y=x^{2}$ 的公式，得到对应的y标签值。 123456# 使用numpy 生成 200 个随机点x_data = np.linspace(-0.5,0.5,200)# 测试集x_pre = np.linspace(-0.5,0.5,200)noise = np.random.normal(0,0.02,x_data.shape)y_data = np.square(x_data) + noise 可以将 200个点的分布图画出。 注意图的显示可能有问题，自行解决一下哦。12plt.scatter(x_data,y_data)plt.show() 使用 keras 中的 Sequential （顺序构成的模型） 构建模型。 与线性模型的区别在，我们需要 增加激活函数，并且增加一个 中间层（含有10个神经元）。 并且增加一点 sgd 的学习率，不然学习度太慢，需要的训练次数就会非常大。 1234567891011121314# 构建一个顺序模型model = Sequential()# 在模型中添加一个全连接层# units 输出的维度 维度就是神经元个数# input_dim 输入的维度# 需要的神经模型为 1-10-1model.add(Dense(units=10,input_dim=1,activation=&#x27;tanh&#x27;))model.add(Dense(units=1,input_dim=10,activation=&#x27;tanh&#x27;))# sgd 随机梯度下降法# mse Mean Squared Error 均方误差# sgd 的学习率太小 训练次数可能非常多# 需要修改一下 sgd的学习率sgd = SGD(lr=0.3)model.compile(optimizer= sgd,loss=&#x27;mse&#x27;) 之后我们就按照批次训练。 共训练3001个批次。有两种写法。方法一：用一个循环体，循环3001次； 每500次 打印一次 损失值。1234567# 训练3001个批次for step in range(3001): #每次训练一个批次 cost = model.train_on_batch(x_data,y_data) # 每500个 batch 打印一次 cost值 if step % 500 == 0: print(&quot;cost:&quot;,cost)方法二： 直接使用 model.fit () 函数 1model.fit(x_data,y_data,epochs=3001) 可以查看 参数值 W （权重）和 b（偏置值） 12W,b = model.layers[0].get_weights()print(&#x27;W：&#x27;,W,&#x27;b:&#x27;,b) 预测 测试集的 结果 使用 model.predict () 函数 12# 测试集 输入网络中，得到预测值 y_predy_pred = model.predict(x_pre) 可以再 把预测的 图打出来 12plt.scatter(x_pre,y_pred)plt.show() 我们的训练 使用方法二 结果如图所示：","categories":[{"name":"Keras","slug":"Keras","permalink":"http://example.com/categories/Keras/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://example.com/tags/Tensorflow/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/tags/Keras/"},{"name":"线性模型","slug":"线性模型","permalink":"http://example.com/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"name":"非线性模型","slug":"非线性模型","permalink":"http://example.com/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"}]},{"title":"各类 激活函数 详解与选择","slug":"各类 激活函数 详解与选择","date":"2021-08-24T09:22:01.000Z","updated":"2021-08-24T09:28:29.357Z","comments":true,"path":"2021/08/24/各类 激活函数 详解与选择/","link":"","permalink":"http://example.com/2021/08/24/%E5%90%84%E7%B1%BB%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/","excerpt":"","text":"@TOC Relutanhsigmiodsoftmax","categories":[{"name":"损失函数与激活函数","slug":"损失函数与激活函数","permalink":"http://example.com/categories/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"}],"tags":[{"name":"激活函数","slug":"激活函数","permalink":"http://example.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"}]},{"title":"各类 损失函数 详解与选择","slug":"各类 损失函数 详解与选择","date":"2021-08-23T09:22:01.000Z","updated":"2021-08-24T09:28:21.357Z","comments":true,"path":"2021/08/23/各类 损失函数 详解与选择/","link":"","permalink":"http://example.com/2021/08/23/%E5%90%84%E7%B1%BB%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/","excerpt":"","text":"@TOC mean_squared_error 均方误差","categories":[{"name":"损失函数与激活函数","slug":"损失函数与激活函数","permalink":"http://example.com/categories/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"}],"tags":[{"name":"激活函数","slug":"激活函数","permalink":"http://example.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"}]},{"title":"Tensorflow 代码学习","slug":"Tensorflow 代码学习","date":"2021-08-22T06:05:01.000Z","updated":"2021-09-17T12:26:38.979Z","comments":true,"path":"2021/08/22/Tensorflow 代码学习/","link":"","permalink":"http://example.com/2021/08/22/Tensorflow%20%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"@TOC 机器学习的整体思路为： 用TensorFlow 预测线性模型我们以这个做最简单的栗子，题目描述如下图所示：这就是一个 最简单的数据模型。 首先我们要引入需要的包，这边使用的是 keras API包12from tensorflow import kerasimport numpy as np先构建模型，构建的为一层的神经网络，输入只有一个变量x1model = keras.Sequential([keras.layers.Dense(units=1,input_shape=[1])])再设置 优化器 和 损失函数1model.compile(optimizer=&#x27;sgd&#x27;,loss=&#x27;mean_squared_error&#x27;)准备训练数据12xs = np.array([-1.0,0.0,1.0,2.0,3.0,4.0],dtype=float)ys = np.array([-3.0,-1.0,1.0,3.0,5.0,7.0],dtype=float)训练模型 迭代500次1model.fit(xs,ys,epochs=500)使用模型，对一个 测试集 x 进行预测 y 并输出1print(model.predict([10.0])) 总体代码如下：1234567891011121314151617from tensorflow import kerasimport numpy as np#构建模型#构建一层的神经网络，并且输入只有1个值model = keras.Sequential([keras.layers.Dense(units=1,input_shape=[1])])model.compile(optimizer=&#x27;sgd&#x27;,loss=&#x27;mean_squared_error&#x27;)#准备训练数据xs = np.array([-1.0,0.0,1.0,2.0,3.0,4.0],dtype=float)ys = np.array([-3.0,-1.0,1.0,3.0,5.0,7.0],dtype=float)#训练模型model.fit(xs,ys,epochs=500)#使用模型，对一个x 进行预测 yprint(model.predict([10.0]))结果如下所示：预测的结果是 18.977886 按照 正确的数学模型 $y = 2x - 1$ 结果应该是 19 。但深度学习不可能完美预测这个值，只能是近似。 需要理解的几个点 Q1 ：请问 model.fit(xs,ys,epochs=500) 的 epochs 500 是什么意思？就是针对同一批数据，利用各类算法（比如梯度下降算法），优化训练的次数，理论上训练次数越多，损失函数越小，准确度越高。 Q2：如何看待这个模型是不是正确的？ 第一，需要看输出的 loss 是不是越来越小 ，accuracy 越来越高。 如果loss不是越来越小，那就说明有问题 第二，你可以看看在测试集 上表现怎么样。 Q3：请问 这个 epochs 越多越好么？ 当然不是，正常来说 模型在测试集上的表现 是不如训练集的。 要选取一个合适的 epochs 值，不然会出现 过拟合的现象。 注意：过拟合 是在测试集上的概念。是训练集上表现不错，但测试集表现不尽人意，叫做过拟合 用TensorFlow 做全神经网络的图像识别分类对 Fashion MNIST 进行图像分类 类别包括0 T-shirt/top(体恤) 1 Trouser(裤子) 2 Pullover(套头衫) 3 Dress(连衣裙) 4 Coat(外套) 5 Sandal(凉鞋) 6 Shirt(衬衫) 7 Sneaker(运动鞋) 8 Bag(袋子) 9 Ankle boot(短靴） 总体代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546import numpy as npimport tensorflow as tffrom tensorflow import kerasfashion_mnist = keras.datasets.fashion_mnist(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data()#具体值 每一个数字都是灰度值print(train_images[0])#可以可视化的 查看其中的图片import matplotlib.pyplot as pltplt.imshow(train_images[0])plt.show()#构建一个全连接的神经网络model = keras.Sequential()#输入层model.add(keras.layers.Flatten(input_shape=(28,28)))#中间层 128个神经元 激活函数使用relumodel.add(keras.layers.Dense(128,activation=tf.nn.relu))#输出层 10个神经元 激活函数使用softmaxmodel.add(keras.layers.Dense(10,activation=tf.nn.softmax))# 中间层 参数共有 100480个#为 784 × 128 = 100352 还要再加上 每个神经元都有的 bias 100352+128=100480# 输出层 参数共有 1290个# 同理 为 128 × 10 + 10 = 1290model.summary()# optimizer 优化器 loss：损失函数# 当标签是除了0，1以外有其他数字的 用sparse_categorical_crossentropy#为 one-hot 只有一个1 如： [0,0,0,1]用 categorical_crossentropytrain_images_scaled = train_images/255model.compile(optimizer=tf.optimizers.Adam(),loss=tf.losses.sparse_categorical_crossentropy,metrics=[&#x27;accuracy&#x27;])model.fit(train_images_scaled,train_labels,epochs=5)test_images_scaled = test_images/255# # 输出 loss 和 accuracy# model.evaluate(test_images_scaled,test_labels)print(np.shape(test_images[0]/255))# 要满足输入的维度, 并从print(model.predict((test_images[0]/255).reshape(1,28,28,1)))print(np.argmax(model.predict((test_images[0]/255).reshape(1,28,28,1))))print(test_labels[0]) 先引入 需要的包 123import numpy as npimport tensorflow as tffrom tensorflow import keras 加载Fashion MNIST数据集12fashion_mnist = keras.datasets.fashion_mnist(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data() 可以查看一下 图片内容是什么 是个 28 × 28 的二维数组 12#具体值 每一个数字都是灰度值print(train_images[0]) 可以可视化的 查看一下 这张图片 1234#可以可视化的 查看其中的图片import matplotlib.pyplot as pltplt.imshow(train_images[0])plt.show() 构造神经元网络模型有两种表达方式方式一 ：12345678#构建一个全连接的神经网络model = keras.Sequential()#输入层model.add(keras.layers.Flatten(input_shape=(28,28)))#中间层 128个神经元 激活函数使用relumodel.add(keras.layers.Dense(128,activation=tf.nn.relu))#输出层 10个神经元 激活函数使用softmaxmodel.add(keras.layers.Dense(10,activation=tf.nn.softmax))方式二 ： 12345model=tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28,28)), tf.keras.layers.Dense(512, activation=tf.nn.relu), tf.keras.layers.Dense(10, activation=tf.nn.softmax)]) 可以 用summary 函数 查看各层的信息 包括参数等; 1model.summary() Q： 中间层参数 100480 怎么来的？ 中间层 参数共有 100480个 ：为 784 × 128 = 100352 还要再加上 每个神经元都有的 bias 100352+128=100480 输出层 参数共有 1290个 ： 同理 为 128 × 10 + 10 = 1290 归一化与训练数据123train_images_scaled = train_images/255 #归一化model.compile(optimizer=tf.optimizers.Adam(),loss=tf.losses.sparse_categorical_crossentropy,metrics=[&#x27;accuracy&#x27;])model.fit(train_images_scaled,train_labels,epochs=5) Q：为什么只对 1875个 进行训练？ epoch 5 的 含义是什么？ 训练没有问题。正在对1875批次（每批次32张图像）而不是1875张图像进行模型训练。 1875 × 32 = 60000张图像 评估模型 与 测试数据评估模型的 loss 和 accuracy 使用 evaluate (测试集全体数据，测试集全体标签) 方法1234test_images_scaled = test_images/255# # 输出 loss 和 accuracymodel.evaluate(test_images_scaled,test_labels)如果要对 测试集图像 进行预测 需要使用 predict (测试集数据) 方法 输出最后的输出内容为 10维向量（因为一共0-9 10个分类 输出层已经设定好了） 然后再用 numpy的 argmax 取得向量中 值最大的那个 就是对应 预测的标签。要注意输入的维度 必须要与 输入层设定的维度 保持一致123456# 要满足输入的维度, 并从print(model.predict((test_images[0]/255).reshape(1,28,28,1)))#输出预测的标签print(np.argmax(model.predict((test_images[0]/255).reshape(1,28,28,1))))#对比一下 真实的标签是什么print(test_labels[0]) 可以设定自动终止训练当损失值 小于 0.4 就终止 批次训练123456789101112131415161718192021222324import numpy as npimport tensorflow as tffrom tensorflow import kerasclass myCallback(tf.keras.callbacks.Callback): def on_epoch_end(self,epoch,logs=&#123;&#125;): if(logs. get(&#x27;loss&#x27;)&lt; 0.4): print(&quot;\\ nLoss is low so cancelling training!&quot;) self.model.stop_training=True print(model.predict((test_images[0] / 255).reshape(1, 28, 28, 1))) print(np.argmax(model.predict((test_images[0] / 255).reshape(1, 28, 28, 1)))) print(test_labels[0])callbacks=myCallback()mnist=tf.keras.datasets.fashion_mnist(training_images, training_labels),(test_images, test_labels)=mnist.load_data()training_images_scaled=training_images/255.0test_images_scaled=test_images/255.0model=tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28,28)), tf.keras.layers.Dense(512, activation=tf.nn.relu), tf.keras.layers.Dense(10, activation=tf.nn.softmax)])model.compile(optimizer=&#x27;adam&#x27;,loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])model.fit(training_images_scaled, training_labels, epochs=5, callbacks=[callbacks])","categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://example.com/categories/Tensorflow/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://example.com/tags/Tensorflow/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/tags/Keras/"}]},{"title":"Numpy、Pandas、Matplotlib  常用代码","slug":"Numpy、Pandas、Matplotlib 常用代码","date":"2021-08-21T02:18:01.000Z","updated":"2021-08-24T09:29:06.390Z","comments":true,"path":"2021/08/21/Numpy、Pandas、Matplotlib 常用代码/","link":"","permalink":"http://example.com/2021/08/21/Numpy%E3%80%81Pandas%E3%80%81Matplotlib%20%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/","excerpt":"","text":"@TOC Numpy 常用代码1234567891011121314151617181920212223242526272829303132333435363738# !/usr/bin/python# -*- coding: UTF-8 -*-import numpy# 生成数组n = numpy.arange(10)print(n)print(&quot;*&quot;*20)# 生成数组，并做2行3列的分隔m = numpy.array([0,1,2,3,4,5]).reshape(2, 3)print(m)print(&quot;*&quot;*20)# 生成数据，分隔成3位数组t = numpy.arange(27).reshape(3, 3, 3)print(t)print(&quot;*&quot;*20)# 加载文本，为int方式tx1 = numpy.loadtxt(&quot;numpy.txt&quot;, delimiter=&quot;,&quot;, dtype=&quot;int&quot;)# 横列替换tx2 = numpy.loadtxt(&quot;numpy.txt&quot;, delimiter=&quot;,&quot;, dtype=&quot;int&quot;, unpack=True)print(tx1)print(tx2)# 1:2横截取，[1,2]为选取tx3 = tx1[1:2,[1,2]]print(tx3)print(&quot;*&quot;*20)# 竖拼接tx4 = numpy.vstack((tx1, tx2))print(tx4)# 横拼接tx5 = numpy.hstack((tx1, tx2))print(tx5)print(&quot;*&quot;*20) 函数简介arrange 函数：用于创建数值范围并返回数组对象1numpy.arrange([1,3,5,7],dtype=numpy.int6或dtype=&#x27;i8&#x27;) linspace 函数： 用于创建等差数组numpy.linspace(start,stop,num,endpoint,retstep,dtype) dtype：默认为 float64num：设置生成的元素个数endpoint：设置是否包含结束值（stop），False为不包含，默认为Trueretstep：设置是否返回步长（即公差），False表示返回，默认为False。当值为 True时，返回值为 二元组，包括数组与步长。 logspace 函数： 用于创建等比数组numpy.logspace(start,stop,num,endpoint,base,dtype) start：开始值，值为$base^{start}$ =》 base为底的 start次幂stop：结束值，值为$base^{stop}$ =》base为底的 stop次幂base：底数dtype：默认数据类型 float64endpoint：True为包含结束值，默认为True numpy 练习题一numpy 的基本用法1.导入numpy库1import numpy as np2.建立一个一维数组 a 初始化为[4,5,6],(1)输出a 的类型（type）(2)输出a的各维度的大小（shape）(3)输出 a的第一个元素（值为4）1234a = np.array([4,5,6])print(a.dtype)print(a.shape)print(a[0])3.建立一个二维数组 b,初始化为 [ [4, 5, 6],[1, 2, 3]] (1)输出各维度的大小（shape） (2)输出 b(0,0)，b(0,1),b(1,1) 这三个元素（对应值分别为4,5,2）12345b = np.array([[4,5,6],[1,2,3]])print(b[0].shape)print(b[1].shape)print(b.shape)print(b[0][0],b[0][1],b[1,1]) 4 (1)建立一个全0矩阵 a, 大小为 3x3; 类型为整型（提示: dtype = int）(2)建立一个全1矩阵b,大小为4x5; (3)建立一个单位矩阵c ,大小为4x4; (4)生成一个随机数矩阵d,大小为 3x2.12345678c = np.zeros([3,3],dtype=int)d = np.ones([4,5],dtype=int)e = np.identity(4,dtype=int)f = np.random.rand(3,2)print(c)print(d)print(e)print(f)5 建立一个数组 a,(值为[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]] ) ,(1)打印a; (2)输出 下标为(2,3),(0,0) 这两个数组元素的值123a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])print(a)print(a[2][3],a[0][0])6.把上一题的 a数组的 0到1行 2到3列，放到b里面去，（此处不需要从新建立a,直接调用即可）(1),输出b;(2) 输出b 的（0,0）这个元素的值1234b = a[0:2,2:4]print(b)print(b[0][0]) #？？？ 用ndarray不会7 把第5题中数组a的最后两行所有元素放到 c中，（提示： a[1:2, :]）(1)输出 c ; (2) 输出 c 中第一行的最后一个元素（提示，使用 -1 表示最后一个元素）1234a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])c = a[1:]print(c)print(c[0][-1])8.建立数组a,初始化a为[[1, 2], [3, 4], [5, 6]]，输出 （0,0）（1,1）（2,0）这三个元素（提示： 使用 print(a[[0, 1, 2], [0, 1, 0]]) ）1234a = np.array([[1, 2], [3, 4], [5, 6]])print(a)#花式索引 第一种print(a[[0,1,2],[0,1,0]])9.建立矩阵a ,初始化为[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]，输出(0,0),(1,2),(2,0),(3,1) (提示使用 b = np.array([0, 2, 0, 1]) print(a[np.arange(4), b]))12345#花式索引 第二种a = np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9], [10, 11, 12]])print(a)b = np.array([0,2,0,1])print(a[np.arange(4),b])10.对9 中输出的那四个元素，每个都加上10，然后重新输出矩阵a.(提示： a[np.arange(4), b] += 10 ）12# 数组广播print(a[np.arange(4),b]+10)numpy 的 array 数学操作 执行 x = np.array([1, 2])，然后输出 x 的数据类型12x = np.array([1, 2])print(x.dtype) 12.执行 x = np.array([1.0, 2.0]) ，然后输出 x 的数据类类型12x = np.array([1.0, 2.0])print(x.dtype) 13.执行 x = np.array([[1, 2], [3, 4]], dtype=np.float64) ，y = np.array([[5, 6], [7, 8]], dtype=np.float64)，然后输出 x+y ,和 np.add(x,y)12345x = np.array([[1, 2], [3, 4]], dtype=np.float64)y = np.array([[5, 6], [7, 8]], dtype=np.float64)print(x+y)print(np.add(x,y))#总结：在numpy中，add和“+”是一样的 14 利用 13题目中的x,y 输出 x-y 和 np.subtract(x,y)12345x = np.array([[1, 2], [3, 4]], dtype=np.float64)y = np.array([[5, 6], [7, 8]], dtype=np.float64)print(x-y)print(np.subtract(x,y))#总结：在numpy中，subtract和“-”是一样的 15 利用13题目中的x，y 输出 x*y ,和 np.multiply(x, y) 还有 np.dot(x,y),比较差异。然后自己换一个不是方阵的试试。123456789x = np.array([[1, 2], [3, 4]], dtype=np.float64)y = np.array([[5, 6], [7, 8]], dtype=np.float64)print(np.multiply(x,y))print(np.dot(x,y))print(x*y)##总结：np.multiply()：数组和矩阵对应位置相乘，输出与相乘数组/矩阵大小一致。# np.dot():执行矩阵乘法运算，若秩为1，则执行对应位置相乘再相加。# *：对array执行对应位置相乘 16 利用13题目中的x,y,输出 x / y .(提示 ： 使用函数 np.divide())12345x = np.array([[1, 2], [3, 4]], dtype=np.float64)y = np.array([[5, 6], [7, 8]], dtype=np.float64)print(np.divide(x,y))print(x/y)## np.divide()与 / 效果相同 17 利用13题目中的x,输出 x的 开方。(提示： 使用函数 np.sqrt() )12x = np.array([[1, 2], [3, 4]], dtype=np.float64)print(np.sqrt(x)) 18.利用13题目中的x,y ,执行 print(x.dot(y)) 和 print(np.dot(x,y))12345x = np.array([[1, 2], [3, 4]], dtype=np.float64)y = np.array([[5, 6], [7, 8]], dtype=np.float64)print(x.dot(y))print(np.dot(x,y))##总结：二维数组矩阵之间dot函数运算得到的乘积是矩阵乘积，一维数组是两个向量的内积 19.利用13题目中的 x,进行求和。提示：输出三种求和(1)print(np.sum(x)): (2)print(np.sum(x，axis =0 )); (3)print(np.sum(x,axis = 1))123456x = np.array([[1, 2], [3, 4]], dtype=np.float64)print(np.sum(x))print(np.sum(x,axis=0))print(np.sum(x,axis=1))##总结：axis为0是压缩行,即将每一列的元素相加,将矩阵压缩为一行## axis为1是压缩列,即将每一行的元素相加,将矩阵压缩为一列，再转置 20.利用13题目中的 x,进行求平均数（提示：输出三种平均数(1)print(np.mean(x)) (2)print(np.mean(x,axis = 0))(3) print(np.mean(x,axis =1))）123456x = np.array([[1, 2], [3, 4]], dtype=np.float64)print(np.mean(x))print(np.mean(x,axis = 0))print(np.mean(x,axis =1))##总结：axis为0是压缩行,即将每一列的元素相加,将矩阵压缩为一行,再取平均值## axis为1是压缩列,即将每一行的元素相加,将矩阵压缩为一列，再转置，再取平均值 21.利用13题目中的x，对x 进行矩阵转置，然后输出转置后的结果，（提示： x.T 表示对 x 的转置）12x = np.array([[1, 2], [3, 4]], dtype=np.float64)print(x.T) 22.利用13题目中的x,求e的指数（提示： 函数 np.exp()）12x = np.array([[1, 2], [3, 4]], dtype=np.float64)print(np.exp(x)) 23.利用13题目中的 x,求值最大的下标（提示(1)print(np.argmax(x)) ,(2) print(np.argmax(x, axis =0))(3)print(np.argmax(x),axis =1))12345678x = np.array([[1, 2,3], [3, 4,5]], dtype=np.float64)print(x)print(np.argmax(x))print(np.argmax(x, axis =0))print(np.argmax(x,axis =1))##总结： numpy.argmax(array, axis) 用于返回一个numpy数组中最大值的索引值。# axis=0则竖着看，当axis=0，是在列中比较，选出最大的 行 索引# axis=1则横着看, 当axis=1，是在行中比较，选出最大的 列 索引 24,画图，y=x*x 其中 x = np.arange(0, 100, 0.1) （提示这里用到 matplotlib.pyplot 库）12345import matplotlib.pyplot as pltx=np.arange(0,100,0.1)y=x*xplt.plot(x,y)plt.show() 25.画图。画正弦函数和余弦函数， x = np.arange(0, 3 * np.pi, 0.1)(提示：这里用到 np.sin() np.cos() 函数和 matplotlib.pyplot 库)1234567x=np.arange(0, 3*np.pi, 0.1)y1=np.sin(x)y2=np.cos(x)plt.plot(x,y1)plt.plot(x,y2)plt.show() Pandas 常用代码12345678910111213141516171819202122232425262728293031323334# !/usr/bin/python# -*- coding: UTF-8 -*-import pandasfrom matplotlib import pyplot# 读取文件df = pandas.read_csv(&quot;BeijingPM20100101_20151231.csv&quot;)# 展示# print(df.head())# print(df.info())# 拼接时间period = pandas.PeriodIndex(year=df[&quot;year&quot;], month=df[&quot;month&quot;], day=df[&quot;day&quot;], hour=df[&quot;hour&quot;], freq=&quot;H&quot;)# 将时间数据赋值df[&quot;dataTime&quot;] = period# 设置索引df.set_index(&quot;dataTime&quot;, inplace=True)# # print(period)# print(df.head())# 通过月份统计df = df.resample(&quot;M&quot;).mean()# (统计)缺失data = df[&quot;PM_US Post&quot;].dropna()# pylot展示x = data.indexy = data.valuespyplot.figure(figsize=(20, 8), dpi=80)pyplot.plot(range(len(x)), y)pyplot.xticks(range(0, len(x), 3), x[::3])pyplot.show() Matplotlib 常用代码12345678910111213141516171819202122232425262728293031323334353637383940414243# !/usr/bin/python# -*- coding: UTF-8 -*-import matplotlibfrom matplotlib import pyplotx = [1, 2, 3, 4, 7, 5, 6, 7, 4, 6, 9, 6, 2, 5, 3, 9, 1, 7]y_1 = [10, 15, 7, 6, 13, 17, 19, 1, 5, 2, 15, 11, 12, 16, 8, 3, 5, 17]y_2 = [17, 5, 3, 8, 16, 12, 11, 15, 2, 5, 1, 19, 17, 13, 6, 7, 15, 10]pyplot.figure(figsize=(20, 12), dpi=50)# 调整字体matplotlib.rc(&quot;font&quot;, family=&quot;MicroSoft YaHei&quot;,weight=&quot;bold&quot;, size=20)# 改变刻度# pyplot.xticks([ i + 1 for i in range(max(x))], [ &quot;time&quot; + str(i + 1) for i in range(max(x))], rotation=45)# 第一个参数x轴 第二个展示的内容 rotation 旋转# 描述pyplot.xlabel(&quot;时间&quot;)pyplot.ylabel(&quot;温度&quot;)pyplot.title(&quot;折线图&quot;)# 折线图pyplot.plot(x, y_1)# pyplot.plot(x, y_2)# 散点图# pyplot.scatter(x, y_1)# pyplot.scatter(x, y_2)# 柱状图# pyplot.bar(x, y_1)# pyplot.bar(x, y_2)# 横版柱状图# pyplot.barh(range(len(x)), y_1, height=0.3)# pyplot.barh(range(len(x)), y_2, height=0.3)# 直方图# pyplot.hist(x, (max(x)-min(x))//1)pyplot.xticks(range(min(x), max(x) + 1, 1))# pyplot.grid()# 保存图片# pyplot.savefig(&quot;link.png&quot;)pyplot.show()","categories":[{"name":"python框架","slug":"python框架","permalink":"http://example.com/categories/python%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"Numpy","slug":"Numpy","permalink":"http://example.com/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://example.com/tags/Pandas/"},{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://example.com/tags/Matplotlib/"}]},{"title":"无监督学习（Unsupervised Learning）之 聚类与降维","slug":"无监督学习（Unsupervised Learning）之 聚类与降维","date":"2021-08-19T14:36:01.000Z","updated":"2021-08-20T10:39:03.082Z","comments":true,"path":"2021/08/19/无监督学习（Unsupervised Learning）之 聚类与降维/","link":"","permalink":"http://example.com/2021/08/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Unsupervised%20Learning%EF%BC%89%E4%B9%8B%20%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/","excerpt":"","text":"@TOC 总结 无监督学习 的要点：1、无监督学习的概念 - 什么叫无监督学习（输入都是无label的数据，没有训练集之说，也就是**只能从一些无label的数据中自己寻找规律**） - 无监督学习的两大任务：“化繁为简”（聚类、降维）、“无中生有” 2、聚类Clustering（K-means、HAC）3、降维Dimension Reduction（PCA） 无监督学习的具体分类？ 化繁为简：找一个函数，将本来复杂的输入，变成比较简单的输出。比如找一个函数，可以把所有的树都变成抽象的树。因此我们拥有一大堆各种不同的图像的数据，但不知它的 output 长什么样子。 无中生有：找一个函数，随机给它一个input（比如一个数字1），然后output一棵树，输入数字2，output另外一棵树，输入3，又是另外一棵树。输入一个随机数，就自动画一张图出来，不同的数画出来的图不一样。这个任务里面，要找的可以画图的函数，只有output没有input。只有一大堆的图像，但是不知道输入什么数字才可以得到这些图像。 化繁为简包括 聚类 Q：什么是 聚类？ 假设做图像的聚类，现在有一大堆的图像，然后把它们分成各类。如上图左边的图像都属于 簇1，右边的图像都属于 簇2，上方的图像都属于 簇3。这就像给图像贴标签，把类似的图像，都用同一个簇表示，就做到化繁为简这件事情。 Q：聚类的注意点是什么？ 是 这些数据到底有多少个簇！ 这和神经网络需要设计几层一样，是需要算法工程师的个人经验的。 这个簇不能太多也不能太少。 比如多到说9张图像9个簇，那聚类就没有意义，直接每个图像一个簇就好了，或者说全部图像都是一个簇，也跟没有做一样。 聚类方法最常用的就是K-means，有一大堆未标注数据 $x^{1}$ 到 $x^{n}$ ，每一个 $x$ 代表一张图像，做成 K 个簇。 K-means聚类算法怎么做？先找簇的中心，假如每一个对象都用一个向量表示，有 K 个簇就需要 $c^{1}$ 到 $c^{K}$ 个中心。可以从训练数据里随机找 K个对象出来作为初始化中心。而后对所有数据，决定属于哪一个簇。假设 $x^{n}$ 和 $c^{i}$ 最接近，那么 $x^{n}$ 就属于 $c^{i}$ ，用 $b_{n}^{i}$ 表示。然后更新簇，所有属于 $c^{i}$ 的数据做平均，就是第 $i$ 个簇新的中心，更新要反复进行。 Q：为什么 是从数据集中挑选 K 个样本做初始化 簇中心？答：之所以从数据集挑选K个样本做初始化簇中心，有一个很重要的原因是，如果是纯粹随机的（不从数据集里挑），那很可能在第一次分配这个簇中心的时候，没有任何一个样本跟这个中心很像，也可以说这个簇没有任何样本，再次更新就会出错。 K-means 用更简单的话来说：其算法思想大致为：先从样本集中随机选取 K 个样本作为簇中心，并计算所有样本与这 K 个“簇中心”的距离，对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中，对于新的簇计算各个簇的新的“簇中心”。循环反复。 Q：总结一下 K-means 算法的 主要流程 簇个数 K 的选择 初始化簇中心（可以从你的train data里面随机找K个x出来，就是你的k个center） while（收敛——聚类结果不再变化） { 各个样本点到“簇中心”的距离 ； 根据新划分的簇，更新“簇中心”（求均值）; } 层次凝聚聚类算法（HAC）怎么做？首先 我们要做一个树结构 （其过程 非常像 哈夫曼树的构造） 假设有5个样本做层次聚类，先要做一个树结构。计算两两样本的相似度，挑出最相似的数据对。 比如第一个和第二个样本最相似，那就合并（比如用平均值代表），5个样本变为4个样本；再计算相似度，配对的是4，5样本，然后把他们合并（平均值），变成3个样本；接着计算相似度，配对的是黄色数据点和剩下的蓝色数据点，再次合并（平均），最后只剩红色和绿色，那么最后平均起来得到root。根据5笔数和之间的相似度，就建立出了一个树结构。 但树结构只是告诉我们说哪些样本比较像，还没有做聚类。 Q：那怎么做聚类呢？或者说我怎么看我分的那几个聚类？答： 看你怎么切，如图上面不同颜色的切线。 比如在上图蓝线初切一刀，意味着把数据分成3簇，1、2为一簇，3单独为一簇，4、5为一簇。 在红色线切一刀，则1、2、3为一簇，4、5为一簇。 在绿色点切一刀，则1、2为一簇，3、 4、 5单独为一簇。 Q：层次聚类 和 K-means的差别？ 在K-means里要自己决定K的值，也就是你要分多少个簇。 在层次聚类里要决定的是在哪里切一刀，如果切比较容易考虑的话，那层次聚类可能更好。 化繁为简包括 降维 Q：什么是降维？答：降维意思是说，原本高维的东西，其实是可以用低维去表示它。就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。换句话说，可以减少数据的维度。就是 z = Wx Q：为什么降维有用？答：假设数据分布如上图左边，在3D空间里分布是螺旋的样子，但是用3D描述数据分布比较浪费的，直觉上也可以感觉可以摊开变成右边2D的样子，只需要2D的空间就可以描述3D的信息。在3D空间里面解比较麻烦，那就在2D里做这个任务。考虑一个实际的简单栗子：每一个input的数字都是28 × 28的矩阵来描述。但是实际上，多数28 × 28矩阵转成一个图像看起来都不像数字，在28 × 28空间里是数字的矩阵是很少的。所以要描述一个数字，或许不需要用到28 × 28维，远比28 × 28维少。所以举一个极端的例子，有一堆3，从像素点看要用28 × 28维来描述每张图像。实际上，只要用一个维度就可以表示，中间的是3，其他的3都是中间的3左转右转10、 20度。所以唯一需要记录的就是中间的3，左转和右转了多少度，即只需要角度的变化，就可以知道28维空间中的变化。 Q：怎么做降维？答：找一个函数，input是一个向量x，output是另外一个向量z（z的维度比x小）。 在降维里最简单的方法是特征选择，把数据的分布拿出来看一下， 如在二维平面上发现数据集中在 $x$ 维度，所以 $y$ 这个维度没什么用，那么就把他拿掉，等于是降维这件事。特征选择不一定有用，有可能case里面任何一个维度都不能拿掉。 另一个常见的方法是PCA，函数是一个很简单的线性函数，input x和output z之间的关系就是一个线性的transform，即 $x$ 乘上一个矩阵 $W$ 得到 $z$ 。现在不知道 $z$ 长什么样子，要根据一大堆的 $x$ 把 $W$ 找出来。 分布式表示（Distributed Representation） Q：光做聚类的话是非常以偏概全的。为什么呢？答：因为在聚类思想中，每个样本都必须属于某一个簇。就好像念力分成6大类，每个人都会被分配到6个大类其中一类。但这样分配太过粗糙，比如某个人的能力既有强化系的特性又有放出系的特性，只分为一类就会丢失很多信息。 **只分为一类就是以偏概全了，应该要用一个向量来表示每个对象，向量的每个维度代表了某一种特质（属性）。这件事情叫做Distributed Representation**。 比如上图所示，这个人每个系都可以有固定的能力占比。 如果对象是一个高维的东西，例如图像，现在用它的特性来表示，就会把它从高维空间变成低维空间，这件事情叫做降维。 Distributed Representation和**降维**是一样的东西，不同的称呼。 ### 主成分分析（PCA） 函数是一个很简单的线性函数，input x和output z之间的关系就是一个线性的transform，即 $x$ 乘上一个矩阵 $W$ 得到 $z$ 。现在不知道 $z$ 长什么样子，要根据一大堆的 $x$ 把 $W$ 找出来。 PCA的实现一般有两种： - 一种是用特征值分解去实现的 - 一种是用奇异值分解去实现的 #### PCA-用特征值分解实现 ![在这里插入图片描述](https://img-blog.csdnimg.cn/3acdbf6ffca44f85b12bb6a0719ae8c9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 刚才讲过PCA要做的事是找 $W$ ，假设一个比较简单的case，考虑一个维度的case。假设要把我们的数据投射到一维空间上，即 $z$ 只是一维的向量。 $w^{1}$ 是W的第一行，和 $x$ （列向量）做内积得到一个标量 $z_{1}$ 。 > **Q：$w^{1}$应该长什么样子？** > 首先假设 $w^{1}$ 的长度是1，即 $||w^{1}||_{2}=1$ 。如果$||w^{1}||_{2}=1$，$w^{1}$ 是高维空间中的一个向量，那么 $z_{1}$ 就是就是 $x$ 在 $w^{1}$ 上的投影长度。现在要求出每一个 $x$ 在 $w^{1}$ 上的投影，那 $w^{1}$ 应该长什么样子？ 举个例子，假设上图右上方是 $x$ 的分布，$x$ 都是二维的，每个点代表一只宝可梦，横坐标是攻击力，纵坐标是防御力。现在要把二维投影到一维，应该要选什么样的 $w^{1}$ ? 可以选 $w^{1}$ 为上图右上方右斜方向，也可以选左斜方向，**选不同的方向，最后得到的投影的结果会不一样**。 那总要给我们一个目标，我们才知道要选什么样的 $w^{1}$ ，现在目标是经过投影后得到的 $z_{1}$ 的分布越大越好。我们不希望投影后所有的点都挤在一起，把本来数据点之间的奇异度消去。我们希望投影后，数据点之间的区别仍然看得出来，那么我们可以找投影后方差越大的那个 $w^{1}$ 。 看上面的例子，如果是右斜方向，那么方差较大，左斜方向方差则较小，所以更可能选择右斜方向作为 $w^{1}$ 。 从上面的例子里看， $w^{1}$ 代表了宝可梦的强度，宝可梦可能有一个隐藏的向量代表它的强度，这个隐藏的向量同时影响了防御力和攻击力，所以防御力和攻击力会同时上升。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/dd45a73b7b0640d1bbb675b9132e5044.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 开始计算： 1、把方差式子展开，转化成协方差（具体转化过程不描述了） 2、结论：我们要找的 $w^{1}$ 就是协方差矩阵 $S$ 的最大特征值所对应的特征向量， $w^{2}$ 就是协方差矩阵 $S$ 的第二大特征值所对应的特征向量，以此类推 . PCA - 用奇异值分解（SVD）特征值分解是一个提取矩阵特征很不错的方法，但是特征值分解只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵。奇异值分解是一个能适用于任意的矩阵的一种分解的方法。 假设现在考虑手写数字识别，我们知道手写数字其实是由一些基本成分组成的，这些基本成分可能是笔画。例如斜的直线，横的直线，比较长的直线，小圈、大圈等等，这些基本成分加起来以后得到一个数字。 基本成分我们写作 $u^{1},u^{2},u^{3}…$，这些基本的成分其实就是一个一个的向量。考虑 MNIST数据集 ，一张图像是28 × 28像素，就是28 × 28维的向量。基本成分其实也是28 × 28维的向量，把这些基本成分向量加起来，得到的向量就代表了一个数字。 如果写成公式的话，就如上图最下方所示的公式。$x$ 代表某一张图像的像素，用向量表示。$x$ 会等于 $u^{1}$ 这个成分乘上 $c{1}$ ，加上 $u^{2}$ 这个成分乘上 $c{2}$，一直加到 $u^{K}$ 这个成分乘上$c_{K}$，再加上 $\\bar{x}$（$\\bar{x}$ 是所有图像的平均）。所以每一张图像，就是一堆成分的线性组合加上所有图像的平均所组成的。 例如数字7是 $u^{1},u^{3},u^{5}$ 加起来的结果，那么对数字7来说，公式里的 $c{1}=1 ,c{2}=0, c{3}=1…$，所以可以用 $c{1},c{2},c{3}…,c{K}$ 来表示一张图像，如果成分远比像素维度小的话，那么用$\\begin{bmatrix}c{1}\\c{2}\\…\\c{K}\\\\end{bmatrix}$表示一张图片是会比较有效的比如7可以由向量 $\\begin{bmatrix}1\\0\\1\\0\\1\\…\\\\end{bmatrix}$ 描述。 我们把公式里的 $\\bar{x}$ 移到左边，$x$ 减 $\\bar{x}$ 等于一堆成分的线性组合，写作 $\\hat{x}$ 。 Q：如果我们不知道K个u（成分）是什么，那怎么找出这K个向量？ 找K个u，让$x−\\bar{x}$ 和 $\\hat{x}$ 越接近越好，$||(x-\\bar{x})-\\hat{x}||{2}$ 称为重构误差，代表没办法用成分描述的部分。接下来，最小化 $||(x-\\bar{x})-\\hat{x}||{2}$，损失函数如上图 $L$。 回忆下PCA，$w{1},w{2},w{3}…w{K}$ 是 $x$ 协方差矩阵的特征向量，事实上 $L$ 的解就是PCA的 $w{1},w{2},w{3}…w{K}$。 PCA实例手写数字识别以把每一张数字图像拆成成分的线性组合，每一个成分也是一张图像（28 × 28 维的向量），所以可以把成分画在图上变成一张图像。 通过PCA画出前30个成分如上图所示，白色的地方代表有笔画。用这些成分做线性组合，就可以得到0-9的数字，所以这些成分叫做Eigen-digit。 Eigen（本征）是说，这些成分都是协方差矩阵的特征向量。 ## 人脸识别 ![在这里插入图片描述](https://img-blog.csdnimg.cn/9e97b877a5114592ac90e2b7f5f3c0f6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 上图右上方有一大堆人脸，找它们前30个主成分。找出来就如上图最下方所示，每张图像都是哀怨的脸，叫做Eigen-face。把这些脸做线性组合，就可以得到所有的脸。 > **Q：但这边有没有觉得有问题，因为主成分找出来的是成分，但是现在找出来的几乎都是完整的脸，也不像是成分啊？像前面的数字识别，成分看起来也像是玛雅文字，而不是笔画，看起来也不是成分啊？** ![在这里插入图片描述](https://img-blog.csdnimg.cn/1215e23adb994fd6b8380a73b1e64527.png#pic_center)> **答**：仔细想想PCA的特性，$α_{1},α_{2}$ 这种权重可以是任何值，可以是正的，也可以是负的。所以当我们用这些主成分组成一张图像的时候，可以把这些成分相加，也可以把这些成分相减，这就会导致你找出的东西不见得是一个图的基本的结构。> > 比如我画一个9，那可以先画一个8，然后把下面的圆圈减掉，再把一杠加上去。我们不一定是把成分加起来，也可以相减，所以说就可以先画一个很复杂的图，然后再把多余的东西减掉。这些成分不见得就是类似笔画的这种东西。 > > 如果要得到类似笔画的东西，就要用另一个技术*NMF（非负矩阵分解）*。PCA可以看成是对矩阵X做SVD，SVD就是一种矩阵分解的技术。**如果使用NMF，就会强迫所有成分的权重都是正的，正的好处就是一张图像必须由成分叠加得到，不能说先画一个复杂的东西再去掉一部分，再来就是所有成分的每个维度都必须是正的。** 所以在同样的任务上，例如手写数字的测试上，使用NMF时，找出来的主成分会如下图所示。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/ac0fc298e03847819527bc49a3198b45.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 你会发现，白色图案类似于笔画，找出来的主成分就成了笔画了。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/36c61755d6ce40318f4df22c47721ab5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 看脸的话，会发现如上图所示。比较像脸的一部分，比如人中、眉毛、嘴唇、下巴。 ## 宝可梦 ![在这里插入图片描述](https://img-blog.csdnimg.cn/c19d6fca60984ad6a435fd0750f485fa.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 有800种宝可梦，每种宝可梦可以用6个特征来表示。所以每个宝可梦就是6维的数据点，6维向量。 现在用PCA来分析，PCA里常有的问题是到底需要几个成分，即到底要把数据降到几维。这个一般取决于你的目的是什么，比如你想做可视化，分析宝可梦特性之间的关系，6维没办法可视化的，那就投影到二维。要用几个主成分就好像是神经网络需要几层，每层几个神经元一样。 一个常见决定使用几个主成分的方法是，去计算每个主成分（特征向量）对应的特征值，这个特征值代表在该主成分上投影数据的方差。 现在的例子里宝可梦是6维的，那就有6 × 6维的协方差矩阵，所以有6个特征值，如上图计算每个特征值比例，结果是0.45，0.18，0.13，0.12，0.07，0.04。那第5、6个主成分的作用比较小，意味着投影数据的方差很小，宝可梦的特性在这两个主成分上信息很少。那么分析宝可梦特性只需要前4个主成分。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/8fcb23f86b76476e9b66585379b04cbb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) PCA后选择4个主成分，每个主成分是一个6维向量（因为原来每个特征都要投影，那就有6种投影数据）。 每个宝可梦可以想成是4主成分向量做线性组合的结果，且每只宝可梦组合的权重不同。 看第一个主成分PC1，数值都是正的，如果给它的权重大，意味着宝可梦6维都是强的，给它的权重小，意味着宝可梦6维都是弱的，所以第一个主成分，代表了这只宝可梦的强度。 看第二个主成分PC2，Def防御力是正值，速度是负值，那么增加权重的时候，会增加防御力并减小速度。 把第一个和第二个主成分画出来如上图最下方，图上有800个点，每个点代表一只宝可梦。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/995d8f5d622b4b3691e432d0e2554a0e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 第三个主成分PC3，特殊防御力是正的，攻击力和HP都是负的，也就是说这是用攻击力和HP来换取特殊防御力的宝可梦。 第四个主成分PC4，HP是正的，攻击力和防御力是负的，这是用攻击力和防御力换取生命值的宝可梦。 把第三、第四主成分画出来如上图最下方，维度是去相关的。 ## 矩阵分解-推荐系统 有时候，你会有两种东西，两种对象，它们之间受到某种共通的潜在因素操控。 假设现在做一个调查，调查每个人手上买的公仔的数目，有5个宅男同学A,B,C,D,E，横轴的公仔人物是凉宫春日、御坂美琴、小野寺、小唯，调查结果如下图。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/7557d3f48de34c97be70ec2ca4212e74.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 看这个矩阵可以发现，买凉宫春日的人，比较有可能有御坂美琴；买小野寺的人，也比较有可能买小唯。这说明人和公仔有一些共同的特性，有共同的因素在操控这些事情发生。 动漫宅获取可以分成两种，一种是萌傲娇的，一种萌天然呆的。每个人都是萌傲娇和萌天然呆平面上的一个点，可以用一个向量表示，那么看上图，A是偏萌傲娇。每一个公仔角色，可能有傲娇属性或者天然呆属性，所以每一个角色，也是平面上一个点，可以用一个向量描述。 如果某个人的属性和角色的属性匹配的话，他们背后的向量就很像（比如做内积的时候值很大），那么A就会买很多的凉宫春日。他们匹配的程度取决于潜在因素是不是匹配的。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/d5f8553af83f4034bbcff5b07c95251a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 所以ABC的属性如上图最左边所示，A、B是萌傲娇的，B稍微没有那么傲娇，C是萌天然呆。每个动漫角色后面也有傲娇、天然呆这两种属性，如果人物属性和角色属性匹配的话，人买角色的可能性就很大。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/006698ec686f439ea7e5161e55e7213f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 上图中右下方矩阵公式中，右边两个矩阵的N应该是M，代表M个人。 我们知道的只有人买的角色的数目，然后凭着这种关系去推论每个人和每个动漫人物背后的潜在因素。每个人背后都有一个向量，代表萌傲娇或者萌天然呆的程度。每个角色后面也有一个序列，代表是傲娇或天然呆的属性。 我们可以把购买的公仔数量合起来看做是一个矩阵X ，行数是人的数量，列数是公仔角色的数量。 现在有一个假设，矩阵X里的每个元素都来自于两个向量的内积。为什么A会有5个凉宫春日的公仔，是因为 $r^{A}·r^{1}$ 的内积很大，约等于5。这件事情用数学公式表达的话，可以把 $r^{A}$ 到 $r^{M}$ 按列排起来，把 $r^{1}$ 到 $r^{4}$ 按行排起来，K是潜在因素的个数，一般没办法知道，需要自己测试出来。 Q：矩阵X的每个维度是什么？我们要做的事情就是找一组rA到rE，找一组r1到r4 ，让两个矩阵相乘后和矩阵X越接近越好，就是最小化重构误差。这个就可以用SVD来解，把Σ并到左边或右边变成两个矩阵就可以了。有时候有些信息是缺失的，比如上图所示的，你不知道A、B、C手上有没有小野寺，可能在那个地区没有发行，所以不知道发行的话到底会不会买。那用SVD就很怪，也可以把缺失值用0代替，但也很奇怪。 Q：那有缺失值怎么办呢？可以用梯度下降的方法来做，写一个损失函数，让$r^{i}$（每个人背后的潜在因素）和$r^{j}$（角色背后的潜在因素）的内积和角色购买数量越接近越好。现在重点是，在summation over元素的时候，可以避开缺失的数据，如果值是缺失的，就不计算。有了损失函数后，就可以使用梯度下降了。根据刚才的方法实际计算一下，假设潜在因素的数量是2。那么A到E都是二维的向量，每个角色也是二维的向量。数值代表了属性的程度，把大的用红色框框圈出来，会发现A、B萌同一组属性，C、D、E萌同一种属性，1,2有同样的属性，3,4有同样的属性。没有办法知道每个属性代表什么，要先找出这些潜在因素，再去分析它的结果。有了这些潜在因素数据，就可以用来预测缺失值。已经知道了$r^{A}$和$r^{3}$，那只要$r^{A}$和$r^{3}$做内积就可以了。 之前的model可以做得更精致一点，刚才说A背后的潜在因素乘上 春日 背后的潜在因素，得到的结果就是矩阵里的数值。但是事实上，可能还会有其他因素操控这些数值。那么更精确的写法就可以写成。 r^{A}⋅r^{1}+b_{A}+b_{1}≈5$b{A}$是跟 $A$ 有关的标量，代表了 $A$ 有多喜欢买公仔，有的人就是喜欢买公仔，也不是喜欢某个角色。$b{1}$是跟 春日 有关的标量，代表了角色有多想让人购买，这个事情是跟属性无关的，本来人就会买这个角色。 然后修改损失函数如上图所示，使用梯度下降求解即可。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"无监督学习","slug":"无监督学习","permalink":"http://example.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"name":"聚类","slug":"聚类","permalink":"http://example.com/tags/%E8%81%9A%E7%B1%BB/"},{"name":"降维","slug":"降维","permalink":"http://example.com/tags/%E9%99%8D%E7%BB%B4/"},{"name":"PCA","slug":"PCA","permalink":"http://example.com/tags/PCA/"},{"name":"K-means","slug":"K-means","permalink":"http://example.com/tags/K-means/"},{"name":"HAC","slug":"HAC","permalink":"http://example.com/tags/HAC/"}]},{"title":"半监督学习 Semi-Supervised","slug":"半监督学习 Semi-Supervised","date":"2021-08-17T14:36:01.000Z","updated":"2021-08-19T07:05:04.586Z","comments":true,"path":"2021/08/17/半监督学习 Semi-Supervised/","link":"","permalink":"http://example.com/2021/08/17/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20Semi-Supervised/","excerpt":"","text":"@TOC 总结 半监督学习 的要点：Q1：什么是Semi-Supervised？Q2：Semi-Surpervised在生成模型中如何实现的（EM算法）？Q3：Semi-Surpervised基于Low-density Separation（低密度分离）假设是如何实现的？Q4：Semi-Surpervised基于Smoothness Assumption（平滑）假设是如何实现的？ 什么是Semi-Supervised？大家知道在监督学习里，有一大堆的训练数据（由input和output对组成）。例如上图所示 $x^{j}$是一张图片，$y^{r}$ 是类别的 label。半监督学习是说，在label数据上面，有另外一组unlabeled的数据，写成$x^{u}$ (只有 input 没有 output )，有U笔 unlabeled 的数据。通常做半监督学习的时候，我们常见的情景是 unlabeled 的数量远大于labeled 的数量（U&gt;&gt;R)。 举个栗子：现在我们要做一个猫狗分类 如果只考虑 labeled data，我们分类的分界线会画在中间； 如果把 unlabeled data 也考虑进去，我们可能会根据 unlabeled data 的分布，分界线画成图中的斜线； semi-supervised earning使用 unlabel 的方式往往伴随着一些假设，学习有没有用，取决于你这个假设合不合理。（比如灰色的点也可能是个狗不过背景跟猫照片比较像） 半监督学习可以分成两种： 一种叫做转换学习，unlabeled 数据就是 testing set ，使用的是testing set的特征。 另一种是归纳学习，不考虑testing set，学习model的时候不使用testing set。 Q：用 testing set 作为 unlabeled 数据，不是相当于用到了未来数据吗？答：用了 label 数据才算是用了未来数据，用 testing set 的特征不算是使用未来数据。Q：什么时候使用转换学习或者归纳学习？答：看 testing set是不是给你了。在一些比赛里，testing set 它是给你的，那么就可以使用转换学习。但在真正的应用中，一般是没有 testing set 的，这时候就只能做归纳学习。Q：为什么使用半监督学习？答：缺少 lable 的数据，比如图片，收集图片很容易，但是标注label很困难。半监督学习利用未标注数据做一些事。Q：用沙雕的简单日常语言，讲一讲什么是 半监督学习答：对人类来说，可能也是一直在做半监督学习，比如小孩子会从父母那边做一些监督学习，看到一条狗，问父亲是什么，父亲说是狗。之后小孩子会看到其他东西，有狗有猫，没有人会告诉他这些动物是什么，需要自己学出来。 Semi-Surpervised在生成模型中如何实现的（EM算法） Q：在生成模型中为什么使用半监督学习？答：在监督学习中，有一堆用来训练的样本（就是label），你知道它们分别属于类别1，还是类别2。会去估算类别1和类别2的先验概率P(C1),P(C2) ，然后计算类条件概率 P(x|C1),P(x|C2) 。假设 P(x|Ci) 服从一个高斯分布。 假设类别1的数据是从均值为 μ1，协方差为 Σ 的分布中取出来的，而类别2的数据是从均值为 μ2 ，协方差也为 Σ 的分布中取出来的（之前讲过共享协方差，效果会好一点）。然后可以计算后验概率 P(C1|x) ，决定一个决策边界在哪里。如果今天有一些未标注数据，如上图绿点，那仍然假设均值和方差是μ1,μ2,Σ显然不合理。如上图左下所示，Σ应该比较接近圆圈（蓝色圆圈），也许在类1采样的时候有问题，所以采样到奇怪的分布（蓝色椭圆）。如上图右下，类2的μ2不应该在橙色椭圆内，而应该在更下面。这样会使先验概率受到影响，本来两个分布，正例数据是一样多，但是加入未标注数据之后，你可能会觉得类2的正例数据更多（先验概率就更大）。总之加入未标注数据后，会影响对均值和协方差的估测，继而影响类条件概率，最后影响了你的决策边界。 通俗的讲，回顾有监督学习中的生成模型，由于data都是有label的，P(Ci) 是已知的，P(x|Ci) 是通过我们基于高斯分布的假设用最大似然估计出来的；现在半监督学习中的生成模型，data的一部分是unlabel的，P(Ci) 是不确定的（隐变量），P(x|Ci)的假设模型也不能套用原来的 u 等参数，这时候需要用EM算法(Expectation-Maximization algorithm，又译为期望最大化算法) EM算法 具体怎么做EM算法适用于带有无法观测的隐变量的概率模型估计初始化一组参数，如果是二分类任务，就是初始化类1和类2的先验概率、均值和协方差，可以随机初始化，用已经有标注的数据估测，统称为 θ 第一步（E步），用labeled data算出来的高斯模型参数 θ 代入公式去求出每一笔未标注数据（unlabeled data）的后验概率（属于类1 的概率）的 P(C1|Xu)； 第二步（M步），用极大似然估计更新 P(Ci) 以及高斯模型参数 θ ，求出 P(x|Ci)，进一步求出新的后验概率 P(Ci|Xu) ，重复这两步直到收敛（似然概率最大） 至于为什么更新参数是要加入P(Ci|Xu) 这一项，是因为EM算法的思想是把不确定的data用一个概率来表示label，而每一笔不确定的data都有可能来自 类C1 和 类C2，看右下图： Q：EM 算法背后的理论是什么？答：原来只有标注数据的时候，目标是最大化一个似然函数，那么给定θ，每一笔训练数据的似然函数值是可以计算的，然后把所有的似然函数值相加，就是总的似然函数值，然后找 θ 最大化。θ 有显式解，求最大值点（导数为0）。现在加入未标注数据后，我们不知道未标注数据来自哪一个类别，那么未标注数据出现的概率就是和 C1的联合概率+和C2的联合概率（相当于是$\\sum{C}P(x^{u},C^{i})$ 。接下来目标就是最大化 $P{\\theta }(x^{u})$ ，但是 $P_{\\theta }(x^{u})$ 的式子是非凸的，所以使用EM算法求解。 Semi-Surpervised基于Low-density Separation（低密度分离）低密度分离的假设是，不确定的data的label要不是1，要不是0（“非黑即白”）。低密度的意思是，两个Class的分界处是低密度的（分得比较开的） Q：这个世界是非黑即白的，什么是非黑即白？答：假设现在有一大堆的data，有标注数据，有非标注数据，在两个类别之间会有一个明显的鸿沟。给一些标注数据，可以把边界分在上图右边的线，也可以把边界分在上图左边的线。但是考虑非标注数据，那么左边的边界会好一点，在边界处，两个类别的密度是低的（不会出现data） Self-training + Entropy-based Regularization(基于熵的正则化)self-training低密度分离最代表性、最简单的方法是self-training，非常直觉。 我们有一些标注数据，和一些未标注数据。接下来： 从标注数据训练一个model f* (用DNN，deep、shallow还是其他机器学习的方法都可以) 根据f* 标注未标注数据，丢入$x^{u}$ ，得到$y^{u}$ ，${(x^{u} ,y^{u} )}^{R+U}_{u=l}$ 叫做伪标签数据（称为Pseudo-label伪标签） 接下来，从伪标签数据集移除一些数据加到标注数据集（移除哪些数据需要自己决定，设计一些启发式的规则，或者给权重，有些数据的标签比较确定，那就给大的权重） 有了更多的标注数据之后，回头再去训练model f* Q：self-training在回归上有用吗？回归问题用self-training不影响f∗，所以回归问题不能用self-training方法。self-training 很像是刚才生成模型里面用的EM算法，唯一的差别是在做 self-training 的时候，用的是硬标签，生成模型里用的是软标签（概率）。在做 self-training 的时候，会强制分配一个数据属于某一个类别，在生成模型里，使用的是后验概率，部分属于类1，部分属于类2。 Entropy-based Regularization(基于熵的正则化) — self-training的进阶版熵：一个事件的不确定程度 Entropy-based Regularization（基于熵的正则化）是self-training的进阶版，self-training里用概率划分类别，可能觉得比较武断，那就可以用Entropy-based的这个方法。 Entropy-based是说，如果使用神经网络，output是一个分布，我们不去限制output具体属于哪一个类别，而是假设分布很集中（非黑即白的世界）。如上图，假设做5个类别分类的 model： 第一个 类别1的概率为1，其他类别概率为0，是good 第二个 类别5的概率为1，其他类别概率为0，是good 第三个 所有类别的概率很平均，是bad，不符合低密度分离的假设（非黑即白） Q：怎么用数值的方法评估分布是集中还是不集中？ 使用熵，分布的熵告诉你集中还是不集中。可以用 每个类别的概率 乘以 log（每个类别的概率），再对类别个数求和取负数。 这个式子来评估。 这个其实就是理解成损失函数，因为这边讲究的是非黑即白，所以其实是一致的。损失函数 也可以用分布的距离来描述。我们希望model的output在标注集上正确，在未标注集上的熵越小越好。 第一个分布，熵为0，分布集中 第二个分布，熵也为0，分布集中 第三个分布，熵为 ln(5) ，分布比较散 根据这个目标，重新设计损失函数。原来只是希望model在标注集上的output和label距离越近越好，用交叉熵来评估它们之间的距离。 现在在原来的基础上，加上未标注集的output分布的熵。 然后在未标注集部分乘上一个权重，来表明偏向标注部分还是未标注部分。 上图右下的损失函数可以算微分，那就使用梯度下降最小化这个损失函数，迭代求解参数。加入未标注部分，作用就类似于正则化（在原来损失函数后加一个L1正则或者L2正则），这里则加入一个未标注集熵来防止过拟合，所以称之为基于熵的正则化。 Semi-supervised SVM（半监督SVM) Q：SVM 支持向量机（Support Vector Machine）是什么？SVM是找边界，给你两个类别的数据，SVM找一个边界，这个边界一方面要有最大的间隔（让两个class分的越开越好），一方面要有最小的分类错误。如上图，假设现在有一些未标注数据，半监督SVM会穷举所有可能的label。上图中，有四笔未标注数据，每笔数据既可以属于 class1，也可以属于 class2，可能的情况如上图右边所示（还有很多种其他的可能）。然后对每个可能的结果，都去做一个SVM，边界如上图红色线。然后再去找让间隔最大，错误最小的那一种结果。在例子里可能是黑色框这种结果。 Semi-Surpervised基于Smoothness Assumption（平滑性）假设是如何实现的平滑性假设与高密度区域假设：x的分布是不平均的，在某些地方很集中，在某些地方又很分散。如果 x1 和 x2 在一个高密度的区域很相似的话，两者的标签也会很像。 Q：什么叫在高密度区域下呢？ 意思是说可以用高密度的路径做连接举个例子，假设数据的分布如上图右边所示，像一个血轮眼。现在有3笔数据：x1,x2,x3，x1和x2中间是一个高密度区域（x1和x2由一个高密度区域连接），有很多数据(中间想成平原地带，地势平坦，人烟很多)，而x2和x3之间数据稀少(中间想成一座山，人烟稀少)，那么走平原会比走山容易，x2走到x1更容易（更相似）。 Q：举一个现实中，相似图形的栗子？为什么会有高密度区域假设？因为在真实情况下，这个假设成立的可能性很高。我们考虑手写数字识别的例子，有两个2一个3，如果计算像素点相似度的话，可能上图右边的2和3更像。但是从所有数据中看，左边的2到右边的2中间会有很多连续的形态。所以根据平滑度假设，左边的2和右边的2更像，因为右边的2和3之间没有过渡的形态。看人脸识别也是一样的，比如左脸像和右脸像差很多，两个人的左脸像计算像素点相似度的话，可能比同一个人的两张侧脸像更高。但是如果收集到足够多的未标注数据，会找到两个侧脸像的很多过渡形态，根据高密度区域假设，这两张侧脸像就是同一个人。 高密度区域假设，在文件上非常有用，假如现在要区分天文学和旅游的文章。 天文学的文章会出现asteroid、bright，而旅游的文章会出现yellowstone、zion。如果未标注文章和标注文章的词语有重叠，那可以很容易分类。但真实情况情况是，未标注文章和标注文章可能没有任何词语重叠，因为世界上的词语太多了，一篇文章词汇不会很多，每篇文章的词语是非常稀疏的。但是收集到够多的数据的话，就可以说上图d1、d5像，d5、d6像，传播下去就可以说d1、d3是一类，d2、d4是一类。 方法一：聚类，而后标注（图像上不太行） Q：如何实践平滑度假设？最简单的方法是聚类、然后标记。假如数据分布如上图，橙色是class 1，绿色是class 2，蓝色是未标注数据。接下来做聚类，可能把所有数据分成3个簇。在簇1里，class 1的label最多，那簇1里所有数据标记为class 1，同样的簇2和簇3都标记为class 2。把标记后的数据拿去learn就结束了。 这个方式不一定有用，因为要求簇正确，这个方法有效的假设是同一个class的东西聚集在一起。但是在图像里要把同一个class的东西聚集在一起没有那么容易。之前深度学习讲过，不同class的图像可能会很像，同一个class可能会不像，只用像素点做聚类，结果八成是不好的。没办法把同一个class的数据聚集在一起，那未标注数据就没有用。 所以要有用，就要有一个好的方法来描述一张图像，比如用 Deep Autoencoder 抽特征，然后再做聚类。 方法二：基于图的方法另一个方法是引入图结构，来表达通过高密度路径进行连接这件事情。 把现在所有的数据点都建成一个图，每个数据点就是图上的一个点，想办法计算它们之间的奇点，想办法把它们之间的边建出来。 所谓的高密度路径的意思是说，如果有两个点，在图上是相连的，那它们就是同一个class，如果没有相连，就算距离很近，也走不到。 Q：生活中，如何构建图？有些时候，图的表示可以很自然的得到。 举例说网页的分类，你有记录网页和网页之间的超链接，那超链接就很自然的告诉你网页是怎么连接的。 又举例论文的分类，论文和论文之间有引用的关系，这种引用的关系也是另外一种图的边，可以很自然地把这种图画出来。 怎么自己想办法构建图？*其实的图的好坏对结果的影响是非常严重的，但是自己用什么方法做还是很启发的，用自己觉得合适的方式做就可以了。 通常的做法是： 先定义两个对象之间的相似度，比如图像可以是基于像素点的相似度（可能效果不好），也可以是基于自动编码器抽取出来的特征计算相似度（效果可能好一点） 定义完相似度后，就可以构建图了（添加边），图有很多种： K近邻的图，现在有一大堆数据，可以计算数据与数据之间的相似度，然后设置k例如3，就是3个最相似的点相连 e-Neighborhood的图，只有相似度超过某个阈值的点才会相连 所谓的边也不是只有相连和不相连这两种选择，可以给边一些权重，让边跟两个数据点的相似度成正比。相似度可以用Gaussian Radial Basis Function来定义 怎么计算这个相似度可以先算xi,xj的欧式距离，乘以一个参数取负号，再取e为底的指数函数。取exp很有必要，在经验上最后效果比较好。因为取exp，下降速度很快，只有当xi,xj非常靠近时，奇点才会大，距离远一点奇点就会下降很快变得很小。这样才能制造如上图右下方所示的，两个距离近的橙色点有连接，绿色点和橙色点虽然距离也近，但是使用了exp导致只有很近很近的点才有连接，即使远一点点就不会有连接了，有这样的机制才能避免跨海沟的连接(橙色点和绿色点连接)。 基于图的方法是，如果现在在图上面有一些标注数据，比如上图左上方，已经知道了蓝色圈的数据属于class 1，那么跟他们有相连的数据点属于class 1的概率也会上升。每一笔数据会去影响它的邻居。 光会影响邻居还不够，因为有连接说明本来就很像，那很像的input ，output本来也就很像。这种方法真正的精髓是，class是会传递的，虽然一个点没有与标注数据直接相连，但是有连接路径，那么class 1就会随着边传递。 例如上图右上方，所有数据点构建成一个图（理想的例子），然后有一个蓝色点属于class 1，一个红色点属于class 2。经过基于图的方法，蓝色点会传递，红色点也会传递，如上图右下方所示。 要让基于图的这种半监督学习方法有用的话，一个重要的原则是你的数据要多，如果数据不够多，例如下图所示，中间有断开，那信息就传递不过去。 考试一般的考题，定量的计算图定量的使用方式是在这个图的结构上面定一个东西，叫做label的平滑度，来表明这个label有多符合平滑度假设。 怎么定平滑度？ 看上图两个例子，这两个例子都有4个数据点，数据点之间连接的数字代表了边的权重。现在给两个例子的数据不同的label，左边例子的label是1,1,1,0，右边例子的label是0,1,1,0，那谁更平滑呢？ 直观感觉就是左边例子更平滑，但我们需要定量描述。常见的方法是，考虑两两相连的点（不管有label还是没有label），在所有的配对上，计算label差值的平方，然后乘上权重 ，最后求和。 所以左边这个例子的S就是0.5，右边例子的S是3，S越小越平滑。 用矩阵来表达 S可以稍微整理下，写成向量形式如上图。 把y串成一个向量，y包括标注数据和未标注数据，所以有 R+U 维。 L是(R+U)×(R+U)的矩阵，叫做图拉普拉斯，L的定义是 D−W ，W是两两数据点之间的权重，D是W每行值之和（放在对角线）。 再加一个正则化项现在可以用 y转置Ly 来评估现在得到的label有多平滑，式子里面的y是label的值，取决于神经网络的参数。那么如果要把平滑度考虑到神经网络里时，就是在原来的损失函数里加上λS（λ是某一个想要调的参数）。λS像一个正则化项，在调整参数时，不只是让标注数据的output跟真正的label越近越好，同时还要让output的label在标注数据和未标注数据上符合平滑度假设。平滑度假设由S衡量。 不一定要在output上计算平滑度，在深度神经网络里，可以把平滑度计算放在网络的任何地方。你可以假设你的output是平滑度，也可以把某个隐藏层乘上一些别的transform，它也要平滑，也可以要求每个隐藏层的output都是平滑的。 转换的想法 Better Representation我们观察到的世界其实是比较复杂的，在背后有一些比较简单的向量，比较简单的东西在操控这个复杂的世界。那只要看透假象，直指核心，就可以让学习变得比较容易。 例如上图右方剪胡子，胡子的变化是很复杂的，但是胡子受头操控，头的变化是有限的。所以胡子是观测，而头就是Better Representation。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"半监督学习","slug":"半监督学习","permalink":"http://example.com/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}]},{"title":"卷积神经网络CNN（Convolutional Neural Network）","slug":"卷积神经网络CNN（Convolutional Neural Network）","date":"2021-08-12T14:36:01.000Z","updated":"2021-08-19T07:03:47.166Z","comments":true,"path":"2021/08/12/卷积神经网络CNN（Convolutional Neural Network）/","link":"","permalink":"http://example.com/2021/08/12/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%EF%BC%88Convolutional%20Neural%20Network%EF%BC%89/","excerpt":"","text":"@TOC 总结CNN的要点：Q1：什么是CNN？为什么要用CNN？为什么图像处理一般都采用CNN？Q2：实现CNN的步骤？ input——&gt;convolution——&gt;max pooling——&gt;…——&gt;flatten——&gt;fully connected network——&gt;output Q3：如何用 keras 搭建一个CNN?Q4：CNN的应用?什么时候适用CNN效果最好？满足三个图像三个特性的时候。但要考虑第三点取子样是否合理。 CNN的概述什么是CNN？CNN也叫convnet，中文名称为卷积神经网络，是计算机视觉领域常用的一种深度学习模型。 为什么要用CNN？其可以简化DNN模型，可以减少不必要的神经元节点。特别是用在图像处理上。 为什么图像处理一般都采用CNN？**CNN的参数比全连接神经网络少得多**，为什么CNN只用较少的参数就可以用于处理图像呢？ 这是因为图像具有以下三点特征：1、一些模式比整张图片小得多，例如“鸟喙”就比整张图片小得多；2、同样的模式可能出现在图像的不同区域，例如“鸟喙”可能出现在图片的左上方也可能出现在图像的中间；3、对图像的降采样不会改变图像中的物体。CNN的卷积层的设计对应着前两点，池化层的设计对应着第三点。 如下图所示： 实现CNN的步骤input——&gt;convolution——&gt;max pooling——&gt;…——&gt;flatten（压平）——&gt;fully connected network——&gt;output 卷积层（减少训练参数） 对应Property1：每一个Filter（待训练的参数，即卷积核）代表一个局部特征探测器（他是一个可以特点图形的表示器），假设现在两个特征探测器（Filter1 和 Filter2） 卷积核1的结果 值最大的那个点所在的图片部分，就是我们要找的内容 卷积核2的结果。与卷积核1的结构共同组成了 feature map 对应Property2：用 Filter1 就能探测出在不同位置的同一个flatten，而不需要用不同的Filter 如果图片是彩色的，也就是说它是三通道的，还没卷积之前，可以说有3个通道（RGB），意味着每一个像素由3个数值表示。如下图所示：卷积核是立方体3×3×3，图片为9×9×3。图片中同样选择卷积核大小，与卷积核累和，但要注意我们并不是把RGB三层，分开算，应该算合为一体的。 与全连接方式的对比全连接层，图像处理的神经网络图如下所示，每个输入与各神经元都有链接（有固定的权值）下面是CNN神经网络图，其主要的目的是减少参数（减少权值数量）。并且可以存在一定的共享权值，下面颜色相同的权值边值应该相同，这样调参可以更快。 池化层（Maxpooling）目的是减少每一个特征的维度，也就是减少后面flatten的输入特征数量。Maxpooling这边我们取每个框内的最大点。在整理一下，变成一个新的feature map 合计上两步得到：做一次卷积+池化后，把原来的 6 × 6 图像变成了 2 × 2 图像，2 × 2图像的深度（每个像素点用多少个值表示）取决于有多少个过滤器，如果有 50 个过滤器，2 × 2 图像就有 50 维，上图是只用两个过滤器，那就是 2 维。所以上图右边，就是一个新的比较小的图像，每个过滤器代表了一个channel(通道)。 重复上两步操作可多次重复上述两个步骤，将输出的结果变得最小化。 Q: 假设我们第一次卷积的时候，我们有25个卷积核，那输出的结果 feature map中应该有25个矩阵。那请问，第二次卷积的时候，输出的feature map 应该是 25×25=625 个矩阵么？答：不对，做完第一次卷积得到25个矩阵，做完第二次后还是25个矩阵。例如输入是三个通道 (RGB) 的 6 × 6 矩阵数据（即一个立方体，6 × 6 × 3），有两个过滤器（也是立方体，三个通道，3 × 3 × 3），则输出为4 × 4 × 2。（4 = 6-2 ； 2是过滤器个数 过滤器决定通道数） Flatten（压平）flatten(压平)的意思是，把特征图拉直，然后丢到一个全连接神经网络里。 CNN in Keras 卷积前，一个pixel用多少个数值表示，取决于通道数 卷积后，一个pixel用多少个数值表示，取决于Filter个数，而通道数决定Filter的高 上一个卷积层有多少个Filter，下一层卷积input就有多少个通道 123456789model = Sequential()model.add(Conv2D(25,3,3),input_shape=(1,28,28))model.add(MaxPooling2D((2,2)))model.add(Conv2D(50,3,3))model.add(MaxPooling2D((2,2)))model.add(Dense(output_dim=100))model.add(Activation(&#x27;relu&#x27;))model.add(Dense(output_dim=10))model.add(Activation(&#x27;softmax&#x27;)) Q：为什么上图第二次 每个卷积核的 参数是 225？答：因为是 3×3×25 那为什么是25而不是50呢？ CNN在学什么？CNN卷积和池化部分在做什么？ 分析第一个层的卷积核是比较容易的，里面每个卷积核就是一个3 3 的矩阵，对应3 3 范围内的9个像素点，只要看到矩阵的值，就知道在检测什么（有明显的特征）。 第二层的卷积核没法知道在做什么，虽然也是 3 × 3 的矩阵，总共 50 个。但是这些卷积核的输入不是像素点，而是在上一层做完卷积和池化后的输出。就算知道第二层卷积核的矩阵值，也不知道在检测什么。另外第二层卷积核考虑的不是原图 3 × 3 的像素点，而是比原图 3 × 3 像素点更大的范围，因为在第一层的池化后，压缩了原图 3 × 3 的区域，第二层卷积核是在压缩后的图像里再选取 3 × 3 像素点，相当于扩大了原图检测的范围。 Q：那怎么分析第二层卷积核在做什么？ 第二层的50个卷积核，每个卷积核的输出是一个 11 × 11 的矩阵。把第k个卷积核输出拿出来如上图左下，矩阵元素表示为 $a_{ij}^{k}$ (第k个卷积核，第i个行，第j个列)。接下来 定义一个“Degree of the activation of the k-th filter”（第k个卷积核的激活程度），值代表第k个卷积核的被激活程度（input和第k个卷积核侦测的东西有多匹配）。 第k个卷积核被激活程度表示为：$a^{k}=\\sum{i=1}^{11}\\sum{j=1}^{11}a_{ij}^{k}$ ，11 × 11 矩阵所有元素值之和。 Q：找一张图像，可以让第k个卷积核被激活程度最大，如果做到这件事情？称 input 的图像为 x，目标是找一个让 $a^{k}$ 最大的 x，如何找到这个 x？ 使用梯度上升，因为我们的目标是最大化 $a^{k}$ 。现在是把 x 当做我们要找的参数，对 x 用梯度上升。原来CNN的 input 是固定的，model 的参数使用梯度下降求解。现在反过来，model 的参数是固定的，使用个梯度上升更新 x，让被激活程度最大。 上图左下，是随便取12个卷积核后对 x 做梯度上升后的结果，每个卷积核都找到一张图像，这张图像让这个卷积核的被激活程度最高。如果有50个卷积核，理论上可以找50张图像。 Q：这12张图像有一个共同的特征：是某种纹路在图上不断反复。为什么会这样？看第三张图像，都是小小的斜条纹，这意味着第三个卷积核是在检测是否有斜的条纹。因为卷积核考虑的范围是很小的，所以原图像上任何地方出现一个小小的斜纹的话，这个卷积核（过滤器）就会被激活，输出值就会很大。如果原图像所有范围都是这种小小的条纹，那这个卷积核的被激活程度就最大。 你会发现每个过滤器都是在检测某一种图案（某一种线条），例如上图左下第3个过滤器是检测斜条纹，第4个是检测短、直的线条，第6个是检测斜成一定程度的线条等等。 每个卷积核（过滤器）都在检测不同角度的线条。 全连接的隐藏层都在干什么？做完卷积和池化后，会做flatten(压平)，把压平后的结果丢到神经网络里去。 Q：在这个神经网络的隐藏层里，每个神经元都在干什么？答：如法炮制之前的做法，定义第 j 个神经元的输出是 $a{j}$ ，然后找一张图像 x，使 $a{j}$ 最大。找到的图像如上图左下所示，9张图像，是对应神经元的输出最大。你会发现跟刚才卷积核（过滤器）观察的图案很不一样，卷积核观察的是类似纹路的东西，因为卷积核只考虑了原图像的一部分区域。输出通过压平后，现在每个神经元是去看整张图像，能使神经元激活程度最高的图像不再是纹路这种小图案，而是一个完整的图形，虽然看起来完全不像是数字，但神经元被激活后也的确在侦测一个完整的数字。 考虑最后的输出？ 如果最后的输出是10维的，每一维对应一个数字。把某一维拿出来，找一张图像使那个维度的输出最大。例如现在要找一张图像，使输出层上对应数字1的神经元的输出最大，理论上这张图像看起来就是数字1但是实际的图像如上图左边所示，每张图像分别代表0,1,2,3,4,5,6,7,8 Q：那为什么是这种是乱七八糟的雪花状呢，而不是我们能看清的数字呢？答：因为神经网络的视角，他就是和人不一样的。他就认为这些雪花图像是不一样的，对于0-8数字。与我们人的思维不同。Q：能不能让这些图像看起来更像数字？我们知道，一张图像是不是一个数字，有一些基本的假设。比如上图左边，人类看起来显示不是数字。那么我们对x做一些正则项约束，告诉机器，虽然有些 x（图像） 可以让 y 很大，但是这些 x 的确不是数字。Q：那加些什么约束呢？比如最简单的想法，图像上的白点是有墨水（笔画）的地方，对一个数字来说，有白点的部分是有限的，数字的笔画只占图的一小部分，所以我们要对 x 做一些限制。假设 $x{ij}$ 是图像像素点的值，每张图像有 28 × 28 个像素点。把所有像素点的值取绝对值并求和（相当于L1正则），我们希望找一个 x ，让 $y{i}$ 越大的同时，也让像素点绝对值之和越小。那我们找出来的图像大部分的地方就不是白色的。最后得到的结果如上图右边所示，和左边的图看起来，已经可以隐约看出来是个数字了。 CNN的应用Deep Dream你给机器一张图像，机器会在这张图像里面，加上它学习到的东西。 比如把上图丢到CNN里面去，然后把某个卷积核或者某个全连接隐藏层拿出来（一个向量），假设是 $\\begin{bmatrix}3.9\\-1.5\\2.3\\:\\\\end{bmatrix}$ 然后把3.9、2.3调大（本来是正的值调大），-1.5调小（负的值调小），正的更正，负的更负。找一个图像使卷积核或者隐藏层（拿出来的）的输出是调整后的向量。这么做的意思是让CNN夸大化它看到的东西。找到的图像会变成上图所示，出现很多奇怪的东西。右边看起来是一头熊，原来是一颗石头。对机器来说，本来就觉得石头像一头熊，强化它的认知后，就学习出来更像一头熊的图案。这个就是Deep Dream。 Deep Styleinput 一张图像，然后让机器去修改这张图像，让它有另一张图的风格，比如让上图看起来是呐喊。 Q：卷积核和过滤器的区别 卷积核就是由长和宽来指定的，是一个二维的概念。 过滤器是是由长、宽和深度指定的，是一个三维的概念。 过滤器可以看做是卷积核的集合。 过滤器比卷积核高一个维度——深度。 —————————————————————————————————— Q：怎么做到图像风格转变呢？把原来的图像丢给CNN，得到CNN过滤器的输出，代表一张图像里有什么样的内容。 然后把呐喊这张图也丢到CNN里，也得到过滤器的输出，但这时候考虑的不是过滤器输出的绝对值，而是考虑过滤器和过滤器输出之间的关系，这个关系代表了一张图像的风格。接下来用同一个CNN找一张图像，这张图像的内容像原图像的内容（过滤器的输出类似），同时这张图像的风格像呐喊的风格（过滤器输出之间的关系类似）。找一张图片同时最大化内容和风格（使用梯度上升更新参数），得到的结果就像两张图片结合一样。 CNN应用在围棋上要让机器下围棋，不一定要用CNN，一般的神经网络也可以做这件事情。只要学习一个网络，也就是找一个函数，输入是棋盘，输出是棋盘上的位置，根据棋盘的盘势，判断下一步落子的位置。输入是19 ×19 向量，向量每一维是棋盘上的一个位置（是黑子则值为1，是白子则值为-1，反之则为0），丢到一个全连接的神经网络，输出也是19 ×19 的向量（每一维对应棋盘一个位置），那这样机器就可以学会下围棋了。 为什么CNN可以用在下围棋上？但实际采用CNN会得到更好的效果！为什么呢？之前举的例子都是把CNN用在图像上面，input 是一个矩阵。用到下棋上，只要把 19 ×19 的向量表示为 19 ×19 的矩阵。对CNN来说，就是把棋盘和棋子当成一个图像，然后输出下一步落子的位置。 收集很多棋谱，告诉CNN，看到落子在5之五，输出天元的位置为1，其他位置为0看到5之五和天元都有棋子，输出就是5之五的位置为1，其他位置为0这个是监督的部分，AlphaGo还有强化学习的部分 总结一下什么时候用CNN?为什么围棋适用？图像要有该有的特性，开头讲过的根据三个特性设计出了CNN的网络结构，在处理图像的时候特别有效。 Q：为什么围棋很适用CNN？答：因为围棋有一些特性和图像处理是很相似的。 围棋是有图像的第一个和第二个特性 在一张图像上面，有一些图案是比整张图像小的，比如鸟嘴。在围棋也有同样的现象，比如看到一些棋子摆放的图案，就要做一些相应的事情（比如上图黑子叫吃的时候，白子要落在下方保证不被吃）。不需要看整个棋盘，只需要看一个小小的范围，就可以侦测白子是不是属于被叫吃的状态。AlphaGo里第一层的过滤器就是用的 5 × 5 过滤器，显然设计这个过滤器的人觉得围棋上最基本的图案在 5 × 5 范围内就可以被侦测出来。 图像还有个特性是相同的图案会出现在不同的区域，在围棋上也有同样的特征。例如叫吃的图案，可以出现在棋盘左上角，也可以出现在棋盘右下角，图案代表了同样的意义（叫吃），所以可以用同一个检测器来处理这些在不同位置的图案。 Q：困惑的是图像的第三个特性，对原图像做子采样不会影响人看到的这张图像的样子，基于第三个特性有了池化层，但Alpha并没有采用池化层（就是做子采样）？因为不能做子采样。比如丢弃棋盘的奇数行和偶数列，想想也应该是不可以的。 也许AlphaGo里的CNN架构有特殊的地方。AlphaGo论文附录里描述了它的网络结构，input是一个19 ×19 ×48 的图像，19×19 是棋盘可以理解，但48是怎么来的？对AlphaGo来说，把每一个位置都用48个值来描述（卷积后有48个通道）。本来我们只要描述一个位置是不是白子、黑子就可以了，而AlphaGo加上了领域知识（看这个位置是不是出于叫吃的状态等等）。 AlphaGo有做zero padding(零填充)，在原来19 ×19 的图像外围补上 0 值变成 23 × 23 的图像，第一层用的是 5 × 5 过滤器，总共 k 个过滤器（paper里用的是192个过滤器），步长stride=1，有用到 ReLu 作为激活函数，有2到12层的过滤器层，最后变成 21 × 21 的图像，接下来再使用 3 × 3 的过滤器，步长 stride=1。最后发现AlphaGo没有使用池化，针对围棋特性设计CNN结构的时候，是不需要池化这个结构的。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://example.com/tags/CNN/"},{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"卷积核","slug":"卷积核","permalink":"http://example.com/tags/%E5%8D%B7%E7%A7%AF%E6%A0%B8/"},{"name":"过滤器","slug":"过滤器","permalink":"http://example.com/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"name":"Maxpooling","slug":"Maxpooling","permalink":"http://example.com/tags/Maxpooling/"}]},{"title":"为什么要Deep？深而不是宽","slug":"为什么要Deep？深而不是宽","date":"2021-08-10T14:36:01.000Z","updated":"2021-08-10T17:02:00.593Z","comments":true,"path":"2021/08/10/为什么要Deep？深而不是宽/","link":"","permalink":"http://example.com/2021/08/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81Deep%EF%BC%9F%E6%B7%B1%E8%80%8C%E4%B8%8D%E6%98%AF%E5%AE%BD/","excerpt":"","text":"@TOC Q：为什么要用要用深度？而不是广度？答：1.因为深度可以用少量的数据，就完成对数据的分类。2.深度，每个层次都是基于上个层次得到的（其实就是学习的过程），我们可以将神经元的数量减少。如果层次很少的话，会导致神经元可能非常多。可以类比逻辑电路。 模组化在比较浅层网络与深层网络时，要让“矮胖”的网络和“高瘦”的网络的参数数目相等，这样比较才公平。但即便是在深层网络参数较少的情况下，深层网络也会比浅层网络表现好。这是因为 深层”其实相当于“模组化”，第一个隐层是最基本的分类器，第二个隐层是用第一个隐层建造的分类器，以此类推。 举个栗子，为什么说深度好！左边第一幅图可以看到，我们需要分四个类，包括长发女，长发男，短发女，短发男。一共四类，其中长发男的数据样本很少，那区分这个类的能力就非常的弱。这个时候，我们就可以先分为两个神经元，一个区分男女，一个区分长发短发，这样中间加一层，可以使得数据样本少的类 鉴定的效果更好。对于分类一个图像来说，深度使得模块化。 类比逻辑电路浅层网络确实可以表示任意函数，但是使用深层结构更有效率。好比逻辑门电路，用两层逻辑门就可以实现任何布尔函数，但是用多层结构更简单、需要的逻辑门更少。 神经网络也是如此，单隐层网络可以表示任何连续函数，但是多层结构表示起来更简单、需要的神经元更少，所以比较不容易overfitting，或只需较少的data。而且，深层结构可以比较有效率地使用data。 类比图形1层hidden layer与3层hidden layer（相同数目的参数），3层的效果更好。但理论上，3层可达到的效果，1层也能达到：要在1层learn的时候，target从真实label改为3层的output，这样1层的结果会接近3层的结果。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"}]},{"title":"深度学习 Deep Learning 模型优化","slug":"深度学习 Deep Learning 模型优化","date":"2021-08-09T14:36:01.000Z","updated":"2021-08-09T14:37:14.552Z","comments":true,"path":"2021/08/09/深度学习 Deep Learning 模型优化/","link":"","permalink":"http://example.com/2021/08/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/","excerpt":"","text":"@TOC 深度学习 怎么评价效果与改进？ 先检查 训练阶段 是否有比较好的结果training优化方法： 换激活函数（Sigmoid、ReLU、Maxout、Tanh、Softmax） 优化器——优化梯度下降和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam） training没问题了，再检查testing是否有比较好的结果 testing（过拟合）优化： 参数在过拟合之前就停止更新 正则化Regularization dropout 如何优化模型谈什么才是overfitting？首先我们在明确一下，深度学习的流程，其与机器学习基本一致。如下图所示：判断 过拟合 需要看两个数据集上的结果（training set → good， testing set → bad）。在试图解决overfitting之后仍要再看一下training set上的结果！ 误区：不能看见所有不好的 performance 都归因到 overfitting。如只看下右图，不能断言56-layer有 overfitting，要看模型在training set上的表现。根据下左图，可以发现原因是训练的时候没有训练好，即这个层次设计可能本身就是有问题的，不然为啥20层的挺好，56层没道理差啊（这不能叫 underfitting，underfitting：参数不够多，模型能力不足）。 对症下药：训练error，测试error分别用什么方法在读到深度学习的方法时，要思考该方法是解决什么问题。是解决training set上的performance不好，还是解决testing set上的performance不好。比如，Dropout是为了解决testing set上结果不好的问题，如果是training set上结果不好而用Dropout，不会有好的结果。下图是 分别解决各问题，可以采用的方法： 模型Train阶段Error 具体解决如下图是，MNIST手写数字识别，激活函数用sigmoid，training data上的accuracy与层数的关系曲线：层数&gt;7时，performance下降，原因不是 overfitting! 因为train的时候就没train好。那模型压根没有训练好，可以采用的方法上面也给出了包括：换激活函数（Sigmoid、ReLU、Maxout、Tanh、Softmax）。激活函数可能会带来什么样的问题？以sigmoid为例说：会出现梯度消失。 方法一：换激活函数的问题 —— 梯度消失什么是梯度消失 有梯度时，参数才会往梯度最小的地方改变；没有梯度了，参数就停止更新了。前面层的学习速率明显低于后面层（后向传播），这就是梯度消失。 为什么会有梯度消失？ 角度一： 用sigmoid会出现梯度消失的问题（参数的变化经过sigmoid会逐层衰减，对最后的loss影响很小）如上图所示，我刚开始增加的△w，由于 sigmoid函数 ，自变量越大，改变量越小的缘故。数的变化经过sigmoid会逐层衰减，对最后的loss影响很小。也就是对C的偏导会越来越小，导致梯度接近于0，而消失。 角度二：假设现在存在一个网络结构：其整个数学模型可以表示为：若要对于 w1求梯度，根据链式求导法则，得到的解为：这明显数学模型的值，随着隐层的增加，越来越小。不过这个前提是 原先的w（权重）并不是很大，原本相乘就是小于1的。如果w太大的话，第一项相乘就大于1，这就会导致最后的梯度爆炸。 如何解决梯度消失问题 —— 换ReLU激活函数ReLU 与 MaxOut梯度消失是因为 sigmoid 引起的，要解决当然要换一个激活函数。采用的方法是换 ReLU激活函数（原型 input0,输出为原值；可变型）ReLU输出0或x，输出0的ReLU神经元相当于不存在，网络变得瘦长，但是整个网络仍然是非线性的，只有当input改变十分微小的时候才是线性的，因为input不同，输出为0的ReLU神经元也不同。 Q : 为什么ReLU是非线性？明明两端都是线性直线啊？因为ReLU在0点没有导数定义，线性讲究的是在整个定义域内。而在0点处，不可微，所以整体是非线性的。 ReLU是Maxout的特例，Maxout可以学出激活函数。 MaxOut 的训练是怎样的？好在哪？下面是一个神经网络的栗子，我们将激活函数换成 MaxOut激活函数。上面的图 可以删除不需要的边和点，变为如下的神经网络： Q ：如果和上图所示，那我去掉的那些边不是啥用没有？也就是这个权值压根和神经网络没关系呢，不是吗？答 ：当然有关系，因为我们的任务是调参，每个权值和偏值得变化，都会导致 MaxOut激活函数 选择的不同。比如说改变一下 w，他可能会选择 z2呢。 方法二：优化器 —— 优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）寻找最佳参数的方法，调节学习率的方法有（SGD、Adagrad、RMSProp、Momentum、Adam）SGD、Adagrad 可以看之前关于 梯度算法进阶的部分 Post not found: 梯度下降算法 进阶这边主要讲解一下 RMSProp 和 Momentum 关于 RMSProp为什么要使用RMSProp 在 Adagrad 中，学习率是跟损失函数对 w 的二次微分有关。那么对于图中蓝绿相交的一点来说，因为 w1 所在的曲率相对于 w2 要小，所以 w1 的学习率会比 w2 大。现在单考虑 w1（只看横向），那么二次微分是固定的（碗状），也就是说 w1是根据固定的规则去自动调整 η 的。但是现实中同一方向的二次微分是不固定的，因此对于同一方向去 w1，需要不同的规则去调 η。 对于一个参数来说，*Adagrad* 是用固定的规则去调 *η*，*RMSProp* 是用变化的规则去调 *η*![在这里插入图片描述](https://img-blog.csdnimg.cn/cc6becd55c104daaa6f8eddadc55eb9d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) ##### 如何实现RMSProp 在原来分母这一项中，在过去梯度平方和前面加上权值 *a*，现有的梯度平方加上 *1-a*。 其在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得**陡峭的方向变得平缓**，从而加快训练速度） ![在这里插入图片描述](https://img-blog.csdnimg.cn/1377f4da7b564c9a89ede8f17577f241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) #### 关于 Momentum ##### Momentum（动量）是用来解决什么问题的？ **这个是用来解决局部最优解的问题的** 说白了就是要延续他的惯性 如何实现Momentum考虑他此时的方向，并保留他的惯性。来调整下一步，最大可能的避免局部最优解。 关于 Adam他其实是一种 Momentum + RMSProp 结合的方法。其具体算法可以看如下图所示： 模型Train阶段OK Test阶段Error，即过拟合方法一 ：参数在过拟合之前就停止更新（Early Stopping）这里的testing set指的是有label的testing set（即validation set ）。 如果learning rate设得对的话，training set的loss会逐渐降低，而testing set与training set可能分布不同，所以testing set的loss可能先降后升，这时就不要一直train下去，而是要在testing loss最小的地方停止train。这里的testing set 实际指的是**validation set**。 ### 方法二 ：正则化Regularization > **Q ： 首先理解什么是范数，L1（范数为1）和L2（范数为2）是什么？** > 范数：向量在不同空间中“长度”的计算公式 > L1：绝对值之和 > L2：平方和 #### L2正则化（权值衰减） ![在这里插入图片描述](https://img-blog.csdnimg.cn/8a8facf4f6bf42ebb56f4f657102b5ed.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) > **Q ： 为什么通常 不考虑 bias？** > L2正则化让function更平滑，而bias与函数平滑程度没有关系。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/35cbdf14ab7049bd9967fcabc1223ae2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 参数更新： 因为η、λ、n都是正的，**所以 *1−ηλ*小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来**。**当然考虑到后面的导数项，w最终的值可能增大也可能减小**。 正则化在NN中虽有用但不明显。 NN参数初始值一般接近0，update参数即是要参数原理0。L2正则化（让参数接近0）的作用可能与early stopping类似。 > **Q : 为什么参数w衰减能防止过拟合?** > 答 ：模型过于复杂会导致过拟合。那么越小的w（可以想象成0理解），表示网络复杂度低，越简单的网络结构，就越不会过拟合。比如模型 *y=w1×w1 + w2×w2* 的平方 中把 *w2=0* 代入，模型就会简化，就不会引起过拟合。![在这里插入图片描述](https://img-blog.csdnimg.cn/c8fc90454614466ebee4a2d5a549a25e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) > **Q : 对正则化解决过拟合问题的一些本质理解？** > 正则化防止过拟合的本质：减少“没用参数”的权值（防止过拟合），同时也减少“有用参数”的权值（会增加bias误差） > **Q : 什么是有用参数，什么是没用参数？如上图中，怎么就把 *x2* 删去，不把 *x1* 删去呢？** > 我们姑且假设 *w1* 是有用参数， *w2* 是无用参数，由公式知参数更新值跟权值、梯度值两个因素有关，实际上，无论是 *x1* 还是 *x2* ，权值都会衰减，每update一次参数，权值 *w* 就会衰减一次，但如果是下图的情况，*损失函数Loss* 的减少跟 *w2* 没关系的，所以对其偏导为0，那么 *w2* 的参数更新只跟权值有关了，随着更新次数叠加，权值就会逐渐衰减接近0；对于***有用参数 w1*** ，虽然它权值衰减，但是它其作用的是后面的偏导值，所以它还是不会变成0的。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/50e060cb7bc24fbe896ef111e287092b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) #### L1正则化 L1是在Loss函数加上绝对值之和，求偏导后比原始的更新规则多出了η ×λ ×sgn(w)这一项。**当w为正时，更新后的w变小。当w为负时，更新后的w变大（一加一减）——因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合** ![在这里插入图片描述](https://img-blog.csdnimg.cn/113c9ac61f5946c783a434a07f8d52e9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) #### L1和L2相比 L1 update的速度是*|ηλ||ηλ|* （划横线那项每次改变量是一定的）。而 L2 update 的速度与w的t有关（如果wt 很大，那么改变量也很大）。 用L1做training，结果比较sparse(稀疏的)，参数中有很多接近0的值，也有很多很大的值。 L2 learn的结果：参数值平均来讲比较小。 ### Dropout **Dropout也是为了简化神经网络结构的目的**，但L1、L2正则化是通过修改代价函数来实现的，而Dropout则是通过修改神经网络本身来实现的。 #### dropout是如何实现的 在training的时候，每次update参数之前，对每一个Neuron（包括input_layer）做sampling，决定这个Neuron按一定几率p丢掉，跟它相连的weight也被丢掉，结果得到一个细长的Network。（每一次update一个mini-batch之前，拿来traing的Network structure是不一样的）。 换句话说input layer中每个element也算是一个neuron. 每次更新参数之前都要resample. **用dropout，在training上的结果会变差，但在testing上的结果会变好。** ![在这里插入图片描述](https://img-blog.csdnimg.cn/196bfc65730b4cb6b5877bcbc0a40729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 在**testing的时候不做dropout，所有neuron都要用**。 如果training时的删除神经元的概率为p%，则在testing时，所有的weight都要乘以（1-p）% ![在这里插入图片描述](https://img-blog.csdnimg.cn/2c2491b39cef4452b88c9c465a8a1cbe.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) #### Dropout的原理 ![在这里插入图片描述](https://img-blog.csdnimg.cn/27c81c02d95847c2a5776da769696c54.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) > **Q : 为什么当在做 *testing* 的时候，weights需要都乘以 *(1-p)%* (p是Dropout的概率)？** > > 答：如下图所示，左侧是在 *Traing* 阶段，*w2* 与 *w4* 由于丢失几率和丢掉(几率是这里是0.5)，假设神经元此时都是1，这时输出的应该是 *w1+w3*；右侧是在 *Testing* 阶段，在这个阶段要保证所有的神经元都不做 *Dropout* 。所以变成了w1+w2+w3+w4，约等于变成了左侧的两倍；所有取所有weight都要乘以（1-0.5）。 实际上，dropout是利用ensemble思想，把一个复杂神经网络的训练转化为，训练很多个简单的神经网络，然后再把多个简单神经网络训练出来的参数做平均。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/96b24fb760ee4d65b18862bc7bcf4ee0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) 一个复杂model, bias准，但variance大，把多个复杂model ensemble起来，variance变小。 每个dropout后的结构由一个batch来train，但是权重是共享的，每个权重是由多个batch 来train的。 ![在这里插入图片描述](https://img-blog.csdnimg.cn/32d269be253c4f1790664d3b5c88caca.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center) **在testing的时候，把多个结构的结果取平均，与把所有参数乘以(1 - p%)，效果是近似的。** **Dropout用在ReLU、Maxout上效果较好。** Q : 为什么 Dropout 在testing的时候，把多个结构的结果取平均，与把所有参数乘以(1 - p%)，效果是近似的？ 而不是相同答：因为神经元之间的层次，使用的激活函数不一定都是线性的，只有线性的情况下才会是相同的。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"模型优化","slug":"模型优化","permalink":"http://example.com/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"}]},{"title":"深度学习 Deep Learning 基础","slug":"深度学习 Deep Learning 基础","date":"2021-08-07T09:03:01.000Z","updated":"2021-08-07T09:03:41.534Z","comments":true,"path":"2021/08/07/深度学习 Deep Learning 基础/","link":"","permalink":"http://example.com/2021/08/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E5%9F%BA%E7%A1%80/","excerpt":"","text":"@TOC 深度学习需要明白的几个问题？思路： 什么是深度学习？为什么需要深度学习？深度学习和机器学习的关系？ 深度学习的步骤 确定神经网络模型的损失函数，如何优化模型，即调参问题 如何使用Back Propagation（反向传播） 方法 update DNN（深度神经网络）参数 深度学习的概念什么是深度学习？深度学习（Deep Learning，DL）是指多层的人工神经网络和训练它的方法。一层神经网络会把大量矩阵数字作为输入，通过非线性激活方法取权重，再产生另一个数据集合作为输出。这就像生物神经大脑的工作机理一样，通过合适的矩阵数量，多层组织链接一起，形成神经网络“大脑”进行精准复杂的处理，就像人们识别物体标注图片一样。 深度学习的model是一个深度神经网络结构（neural structure） 深度学习的“深度”是指神经网络的隐层（hidden layer）数量足够多 深度学习是自动提取特征（Feature extractor），不需要像逻辑回归那样特征转换（Feature engineering） 为什么需要深度学习？ 深度学习和机器学习的关系？ 传统机器学习的模型结构较简单，很依赖算法工程师做特征工程甚至子模型来提升模型效果。就像我们之前上节那个栗子一样，做多分类的问题，四个角对角是一个类的情况，没办法进行分类，所以只能使用特征工程来进行特征的变换。 深度学习由于其层次化的结构，理论上可以拟合任意函数，整个复杂结构即可以用来对特征进行自动组合（如图像），也可以用来构建复杂的模型（如nlp领域里的LSTM，能够考虑上下文）。 深度学习的步骤其实和机器学习一样分为三步。 Step 1：定义一个神经网络结构（neural structure）神经网络的创建包括3部分： 神经网络有多少隐层（layer） 每一层有多少神经元（neuron） 每个神经元之间如何连接 常见的出名神经网络 神经元怎么定义？每个神经元都有一个 bias 和一个 function ，每条输入的边都有一个 weight 一个神经网络的栗子下面是一个 3个隐层（layer）、六个神经元（neuron）（每个球都是一个神经元）、全连接前馈网络（Fully Connection Feedforward Network） “前馈”是指整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示其实我们常用的网络，都是前馈神经网络，从输入到输出是一个有向图，中间不会有环或者反向传播。当然，我们在训练前馈神经网络的时候，会用到反向传播进行参数调整。但仍不影响整个网络的有向和前馈性质。 神经网络如何工作？其实 这就是矩阵运算（可以使用GPU加速计算）总体可以归纳为： 栗子：识别手写数字图像从图像中识别是数字几？ 制作神经网络模型 的FAQ（最容易被问得问题） Q1： 神经网络需要几个隐层（layers）？每个层需要多少个神经元呢？ 反复试验+ 直觉 （说白了就是要慢慢试= = 看经验呗） Q2：神经网络可以自动确定结构吗？ 理论上其实是可以的，不过这些方法我们还没学到。Evoluntionary Artifical Neural Networks Q3：我们可以设计层次之间的结构么？ 意思就是说例如Layer1 链接 Layer3 这样跳着的 等等。当然可以 CNN就是不按顺序来的，具体看下一节。 Step 2：确定神经网络模型的损失函数还是和逻辑回归一样，用交叉熵损失函数，调参使得交叉熵损失函数最小，如下图所示：当然上面只是一个点的，把每个样本结合起来看。 不过这个地方有几个疑问？ Q1：神经网络里面有很多层，所以有很多 w 和 b 调整。那是全体都一次性调整么？ 答： 现在不知道啊！ Q2：上面我是把所有的 交叉熵 都加在一起 然后对 整个大的损失函数进行调参是这样嘛？ 那我的训练样本应该是 各个数字都有是吗？ 答： 现在不知道啊！ Step 3：如何找到一个最好的函数（最佳参数），即调参用的还是 梯度下降法 随机选取一组参数 输入所有的训练样本 然后样本数据不变，参数不断变，用梯度下降法更新参数 Backpropagation（反向传播） 来优化调参速度神经网络可能有很多个隐层，因此可能有百万数量级的参数，为了在梯度下降时有效地快速计算梯度，使用反向传播。下图是 使用的损失函数为：交叉熵损失函数 ， 梯度下降法就是对每个参数求偏导，然后根据偏导大小进行左右移动，找到偏导为0的点，就是最佳参数值。 计算对参数偏导的表达式 交叉熵损失函数，为 正确值乘以 ln的带入x样本的线性模型 +（1-正确值）乘以ln（1-带入x样本线性模型） 只考虑刚输入阶段的神经元，可以得到用 sigmoid激活函数之前的量 z，而 z 是由线性模型（按权重分配向量 + 偏值 b 得到）。如下图所示，可以看到对参数 w 的偏导可以按照链式法则为 ∂C/∂w=∂z/∂w × ∂C/∂z 前项为前项传递，后项为后向传递 前项传递非常好看出来，就是对应的输入值，比如 ∂C/∂w1=∂z/∂w1 × ∂C/∂z 。其中∂z/∂w1 看图上所示，不就是对 w1 进行偏导么，那就是 x1。 后项过程比较复杂，根据链式法则，∂C/∂z=∂a/∂z × ∂C/∂a，其中∂a/∂z = σ&#39;(z) 如下图所示 归纳一下得到如下：这个时候我们从另一观点看待上面的式子：有另外一个神经元（下图中的三角形，表示乘法/放大器），input是∂C/∂z‘与∂C/∂z′&#39; ，权重分别是 w3,w4，求和经过神经元（乘以σ′(z)），得到 ∂C/∂z。（相当于反向传播，先线性加权再乘以一个σ′(z) 和正向非常类似） 后项传递 的两种情况如上图所示是我们最后得到的后项传递的式子，其中我们还有两个项不知道。那我们现在需要计算这两个部分，但分为两种情况 case 1：下一层是最终输出层第一种情况，z′,z′′ 所接的neuron是output layer的neuron。这个就比较简单，直接根据最后一层的输出反向写出即可。 case 2：下一层不是最终输出层第二种情况，z′,z′′ 所接的neuron不是output layer的neuron。就上面那个图，我们可以推得类似后项传递的式子就这样反复迭代(递归)，直到遇到case1的情况，就可以算出整个后项传递。然后结合之前的前项传递，就是我们要得到的对这个参数的偏微分值。 总结梯度下降时需要计算损失函数对每个参数偏微分∂C/∂w，w 代表相邻隐层之间的一条连线（即权值），每个 w 只有一个所指的神经元。 链式法则将计算 ∂C/∂w 拆成前向过程与后向过程。 前向过程计算的是∂z/∂w ，这里 z 是 w 所指neuron的input，计算结果是与 w 相连的值。后向过程计算的是∂C/∂z，这里 z 仍是 w 所指neuron的input，计算结果通过从后至前递归得到。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"反向传播","slug":"反向传播","permalink":"http://example.com/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"}]},{"title":"逻辑回归 Logistic Regression","slug":"逻辑回归 Logistic Regression","date":"2021-08-05T15:08:06.000Z","updated":"2021-08-05T15:08:47.033Z","comments":true,"path":"2021/08/05/逻辑回归 Logistic Regression/","link":"","permalink":"http://example.com/2021/08/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%20Logistic%20Regression/","excerpt":"","text":"@TOC 逻辑回归需要明白的几个问题？ 1、逻辑回归(Logistics Regression) 与 线性回归(Linear Regression)的区别在哪2、生成模型(Generative Model) 与 判别模型(Discriminative Model）的区别在哪 生成模型就是要学习 x 和 y 的联合概率分布 P(x,y)，然后根据贝叶斯公式来求得条件概率 P(y|x)，预测条件概率最大的 y。 判别模型就是直接学习条件概率分布 P(y|x)。 3、逻辑回归(Logistics Regression) 与 深度学习（ VS Deep Learning）逻辑回归具有缺陷，需要做特征工程来转变特征，但是这个人为步骤非常麻烦，所以引入了深度学习。 逻辑回归与线性回归什么是逻辑回归？ 逻辑回归是解决分类问题的一种算法 它与 线性模型 形式上有点像（本质上是在线性模型外面“裹”一个sigmoid激活函数，来表示概率的函数） 它是一种判别模型，与前面说的生成模型不同 它是深度学习的基础 对比逻辑回归与线性回归区别一：模型不同本质上是在线性模型外面“裹”一个sigmoid激活函数，来表示概率的函数 总结的来看 区别二：损失函数Loss不同逻辑回归的为什么似然函数最大，参数就越有可能，越合理？最大似然估计：现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。 总结一下： 问题：为什么逻辑回归不采用线性回归的差平方来做？例如，其实目前离目标点还很远，但梯度已经为0了，这显然不合理。 区别三：如何调参的方式是一致的 化简逻辑回归损失函数左侧部分 化简逻辑回归右侧部分 化简，调参如下总结 生成模型与判别模型什么是生成模型和判别模型？从本质上讲，生成模型和判别模型是解决分类问题的两类基本思路 。分类问题，就是给定一个数据 x，要判断它对应的所属标签 y 生成模型就是要学习 x 和 y 的联合概率分布 P(x,y)，然后根据贝叶斯公式来求得条件概率 P(y|x)，预测条件概率最大的 y。 判别模型就是直接学习条件概率分布 P(y|x)。 两种模型案例 举个栗子？ 栗子1：假设你从来没有见过大象和猫，连听都没有听过，这时，给你看了一张大象的照片和一张猫的照片。Q : 你看过之后，有人牵了一头真的大象过来，问你只是大象还是猫？ 用判别模型的思路回答：你回想刚才看过的照片，大象比猫很明显有个长鼻子，所以眼前这个有着长鼻子的动物就是大象。 用生成模型的思路回答：你回想刚才看过的照片，然后用笔把它们画在了纸上，拿着纸和我家的大象做比较，你发现，眼前的动物更像是大象。 第一个解决问题的思路就是判别模型，因为你只记住了大象和猫之间的不同之处。第二个解决问题的思路就是生成模型，因为你实际上学习了什么是大象，什么是猫。 栗子2：有四个形式为(x,y)的样本。(1,0), (1,0), (2,0), (2,1）。假设，我们想从这四个样本中，学习到如何通过x判断y的模型。 用生成模型，我们要学习 P(x,y)。如下所示：我们学习到了四个概率值，它们的总和是1，这就是联合分布律P(x,y)。（因为这是离散的，连续的话叫联合概率密度） 用判别模型，我们要学习 P(y|x)，如下所示：因为这是条件分布律，每一行概率值相加都为1。 Q : 当 x=1 时，请问 y 是 0 还是 1 呢？ 用生成模型，我们会比较P(x=1,y=0) = 1/2P(x=1,y=1) = 0我们发现 P(x=1,y=0)的概率要比 P(x=1,y=1)的概率大，所以，我们判断：x=1时，y=0。 用判别模型，我们会比较：P(y=0|x=1) = 1P(y=1|x=1) = 0同样，P(y=0|x=1) 要比 P(y=1|x=1)大，所以，我们判断：x=1时，y=0。 我们看到，虽然最后预测的结果一样，但是得出结果的逻辑却是完全不同的。 生成模型为啥叫生成模型？生成模型之所以叫生成模型，是因为：它背后的思想是，x是特征，y是标签，什么样的标签就会生成什么样的特征。好比说，标签是大象，那么可能生成的特征就有大耳朵，长鼻子等等。当我们来根据x来判断y时，我们实际上是在比较，什么样的y标签更可能生成特征x，我们预测的结果就是更可能生成x特征的y标签。 为什么一般来说，判别模型表现得会比生成模型好？我们举一个栗子，现在有Class1 和 Class2 两类数据。现在训练集数据如图所示：Q : 问请问如下的测试集，他是应该属于Class1 还是 Class2 呢？我们这边用判别模型来计算，算出如下图所示的数据：然后用贝叶斯公式算出 P(C1 | x) ，就是当x1和x2全为1 的情况下的概率是多少。算出的结果是 ＜ 0.5 的，但从我们人的角度看，其实应该是属于Class1的，因为Class2里面压根就没有x1，x2同时为1的存在。这是为什么呢？因为我们的样本太少了，用判别模型是可以帮助我们在有限的数据样本中假象数据。 那用生成模型怎么做呢？ 所以判别模型的优势在于 样本量少的时候表现比判别模型好，因为它能自己脑补出一个假想模型 噪声对它影响较小，因为它没有过分依赖数据，它是按照自己假想模型走的 常见的生成模型和判别模型有哪些呢？生成模型 HMM（隐马尔可夫模型） 朴素贝叶斯 判别模型 逻辑回归 SVM（支持向量机） CRF（条件随机场） 最近邻 一般的神经网络 逻辑回归与深度学习逻辑回归解决多分类问题逻辑回归是解决分类问题的，实际中的问题大多是多分类的问题，多分类问题会用到softmax。 逻辑回归其实就是线性回归在外面加了个sigmoid激活函数（二分类）或者softmax激活函数（多分类）。 sigmoid激活函数 和 softmax函数的区别：通常在二分类中使用sigmoid作为最后的激活层。在多分类单标签中使用softmax作为激活层，取概率最高即可。多标签问题中使用sigmoid作为激活层，相当于把每一个类别都当成了二分类来处理。多分类问题解决 逻辑回归的局限性 用深度学习去解决这个问题这个怎么变换过来的啊？是需要用特征工程的方法的，而特征工程是需要我们人为地去建立一个特征函数去把这些点转化，实际上是比较难的，或者说比较费工夫的这个时候我们需要引入 深度学习","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"分类问题","slug":"分类问题","permalink":"http://example.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://example.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"分类问题 Classification 案例一：神奇宝贝是水系还是普通系？","slug":"分类问题 Classification 案例一：神奇宝贝是水系还是普通系？","date":"2021-08-04T06:21:06.000Z","updated":"2021-08-06T14:47:11.946Z","comments":true,"path":"2021/08/04/分类问题 Classification 案例一：神奇宝贝是水系还是普通系？/","link":"","permalink":"http://example.com/2021/08/04/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%20Classification%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E7%A5%9E%E5%A5%87%E5%AE%9D%E8%B4%9D%E6%98%AF%E6%B0%B4%E7%B3%BB%E8%BF%98%E6%98%AF%E6%99%AE%E9%80%9A%E7%B3%BB%EF%BC%9F/","excerpt":"","text":"@TOC 概念：（从概率生成模型到判别模型）概率生成模型：由数据学习联合概率密度分布 P(X,Y) ，然后求出条件概率分布P(Y|X) 作为预测的模型。例如：朴素贝叶斯、隐马尔可夫（em算法）判别模型：由数据直接学习决策函数 Y=f(X) 或者条件概率分布 P(Y|X) 作为预测的模型。例如：k近邻法、感知机、决策树、逻辑回归、线性回归、最大熵模型、支持向量机(SVM)、提升方法、条件随机场（CRF） 分类问题的思路 分类问题及其解决方法的讨论 1. 首先，什么是分类问题？ 2. 接着，分类问题该如何解决呢？ 建立概率生成模型的步骤（以朴素贝叶斯分类器为例）step1：求先验概率step2：确定数据属于哪一个分布，用最大似然估计出分布函数的参数step3：求出后验概率 生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出 分类问题及其解决方法的讨论什么是分类问题？说白了 就是输入一个事务的一些参数特征，通过数学模型，可以得到这个东西是什么。 这也包括二分问题（结果是由两面）与多分问题。 案例：举个栗子，输入一个神奇宝贝，输出：他是属于什么属性的？ 这是一个典型的多分问题，因为属性有好几种啊。举个栗子，输入一个神奇宝贝图片，输出：他是可达鸭么？ 这是一个典型的二分问题，因为结果只有两种 是还是不是。举个栗子，假设有两个类别（水系和普通系），每个类别有不同的精灵，现在我抓到一个精灵，那么它是属于水系和普通系的概率分别是多少。这也是个典型的二分问题。 如何解决分类问题？如何解决二分问题？问题分析对于二分类问题，定义一个function也就是数学model，当输出函数值大于0就划分为类别1，否则就为类别2. 而损失函数定义为在测试数据上误分类的次数。 那这个function我到底怎么定义呢？实际上，可以将其定义为一个概率模型。它可以是一个条件概率模型 P(C1 | X),当 P(C1 | X) &gt; 0.5 ,比如在神奇宝贝二分问题中，我们定义X是这张图片的参数，而C1表示是可达鸭，整个的意思就变为了在这些图片参数的条件下这张图片是可达鸭的概率是多少，概率大于一半，说明确实很有可能就是可达鸭。或者对于第二个栗子，同样的可以规定条件概率模型 P(C2 | X)，表示在捕捉了一个精灵后，他的参数条件下，是水系的概率是多少？（这边C2表示，捕捉的是水系） 这边我们以栗子2为例，假如我们捕捉了一只神奇宝贝(其实他就是可达鸭)，问他是水系的概率是多少？（理论上其实，他就是水系的，但机器需要通过概率论去推，需要包括以下的概率推导） 如上图所示： x就表示是可达鸭先验概率：这里是P(C1)——训练样本中的精灵是水系的概率;同理P(C2)——训练样本中的精灵是普通系的概率——————————————————————————————————————————————————————————————————P(x | C1)：这里是指在水系中是可达鸭的概率P(x | C2)：这里是指在普通系中是可达鸭的概率两者可以用最大似然估计法求出——————————————————————————————————————————————————————————————————后验概率：这里指抓到的神奇宝贝可达鸭是水系的概率。其用贝叶斯公式算出，公式如上图所示 求先验概率 P(C1)：训练样本中的精灵是水系的概率根据训练样本，分别算出水系和普通系的概率 选择概率分布函数，用最大似然估计出分布函数的参数（就是调参） 注： 当样本数据x取实数值时，采用正太分布(高斯分布) 当每种特征的数值都在0-1内时，采用伯努利分布 当每种特征取值在{1, 2 , 3 , …，K}，采用多项式分布（Multinomial Distribution） 首先，我们目标是求水系样本中的79个精灵中，抓到其中一种神奇宝贝的概率 P(x | C1) ，那么这个概率应该是跟精灵的属性有关的。这里我们选择两种属性（物防和法防）讨论，此时数据中（x,水系）中的x应该是一个神奇宝贝向量（[x1物防，x2法防] , 水系）。 2、然后，这里选择正太分布（二维）：也就是说在水系样本中的79个精灵中，抓到其中一种精灵的概率 P( x | C1) 呈正太分布。 接着，用最大似然估计法算出分布函数的参数似然函数L：x1,x2…x79同时出现的概率函数。最大似然估计：使似然函数最大时的参数估计。调整的参数最后求出后验概率 即P(C1 | x): 抓到的可达鸭是水系的概率利用的就是贝叶斯公式。整体每个部分的逻辑可以看下图所示：效果不好的解决方法实验结果分类后，准确率并不高。理论上考虑其他因素，即增加维度可以增大准确率。但效果还是不佳，这该怎么办。实验结果生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出回到如何用机器学习的三大步骤解决分类问题： 逻辑回归方法（判别模型）的引出 化简推到一下（纯数学）所以其实不用考虑，N1，N2，μ1，μ2，∑的值。 直接就是w和b两个参数，这也是为什么之前，我们将∑按权值分配后，图像由线性分类的原因。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"分类问题","slug":"分类问题","permalink":"http://example.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"},{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"梯度下降算法 进阶","slug":"梯度下降算法 进阶","date":"2021-08-04T05:16:06.000Z","updated":"2021-08-04T12:49:04.332Z","comments":true,"path":"2021/08/04/梯度下降算法 进阶/","link":"","permalink":"http://example.com/2021/08/04/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%20%E8%BF%9B%E9%98%B6/","excerpt":"","text":"@TOC 回顾梯度算法是一种迭代的算法，每看一个参数都会更新。 学习率η（Learning Rate）自定义的学习率对参数选择的影响学习率需要选取合适的值，过小会导致模型调参的速度太慢，但过大会导致错失掉了最佳参数点 如何调整学习率η？ 在最开始的时候，随机点离目标点很远，我们一般会选取一个比较大的学习率；当做了几期后，我们离目标点很近了，所以我们会减小学习率 缩减为 如下图所示的公式 不同的参数应该设置不同的学习率 下面是常用的 调整学习率的方法： AdagradAdagrad是解决不同参数应该使用不同的更新速率的问题。Adagrad自适应地为各个参数分配不同学习率的算法。 其原理为：里面比较核心的部分在 每次σ的取值。每次σ规定为之前所有g平方对应的之和的均方根。例如下图所示：其中 g 是每次的偏导值结合之前说的 η 值的变化 可以得到如下的公式推导： 提问: 发现一个现象，本来应该是随着gradient的增大，我们的学习率是希望增大的，也就是图中的g上标t；但是与此同时随着gradient的增大，我们的分母是在逐渐增大，也就对整体学习率是减少的，这是为什么呢？这是因为随着我们更新次数的增大，我们是希望我们的学习率越来越慢。因为我们认为在学习率的最初阶段，我们是距离损失函数最优解很远的，随着更新的次数的增多，我们认为越来越接近最优解，于是学习速率也随之变慢。 为什么化简之后是如上这个式子呢，其在图像上的意义是 一阶导数比上二阶数的值。如下图所示： Stochastic Gradient Descent（SGD随机梯度下降法） ？？？其和 普通的梯度下降算法区别在。普通遍历在求和的时候需要浪费大量时间，进而去掉求和产生了随机梯度下降算法。和下图看到的一样： 只是要注意一下标准的梯度下降和随机梯度下降的区别： 标准下降时在权值更新前汇总所有样例得到的标准梯度，随机下降则是通过考察每次训练实例来更新（就是随机选择一些按顺序的后续样本点来，而不是全体数据）。 对于步长 η 的取值，标准梯度下降的 η 比随机梯度下降的大。因为标准梯度下降的是使用准确的梯度，理直气壮地走，随机梯度下降使用的是近似的梯度，就得小心翼翼地走，怕一不小心误入歧途南辕北辙了。 当损失函数有多个局部极小值时，随机梯度反而更可能避免进入局部极小值中。 小批量随机梯度下降（batch gradient descent）如果在每次迭代中，梯度下降是用整个训练数据集来计算梯度的话，则会带来大量的计算量。因此提出批量梯度下降来进行优化。每次优化不再是对整体数据集来计算损失，取而代之使用随机采样小批量的样本来计算梯度。 Feature Scaling （特征缩放）其意思就是说要将所有特征有相同的规模。例如下图所示，X2 的范围明显大宇 X1，所以要将 X2 进行缩放。 为什么要做Feature Scaling？ 为什么要做Feature Scaling？如左图，X1 和 X2 数值规模大小相差很大，那 W2 这参数的变动会极大的影响损失函数，而 W1 影响度就很小。所以我们需要对 X2 进行特征缩放，使其与 X1 保持一个规模。并且其实从图像可以看出，如果按左图来做的话，我们很难找到一组合适的参数集合，因为他梯度下降的方法不是直线的而是曲线；而右图是近乎直线。（最里面圈的损失函数最小） 如何实现Feature Scaling？ 如上图所示，假如有R个数据样本，每个样本有X1 - Xi 个特征。我们用上面的公式计算出其应该的scale（其实说实在的这不就是化成标准正太分布么） 梯度下降算法数学总结提出问题：如下图所示（我怎么样才能在红圈里找到最小损失的那个点呢） 首先要回顾泰勒公式二元泰勒： 用泰勒展开损失函数理解为点乘，反向180度的时候，损失函数才是最小的：最终可以表达成： 梯度下降算法缺点在哪其不仅仅包括可能有找到是局部最优解问题，有可能在中间的时候就有偏微分为0的时候，而这时这个点可能离全局最优解点 很远。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"模型误差的来源分析","slug":"模型误差的来源分析","date":"2021-08-02T05:16:06.000Z","updated":"2021-08-04T12:48:42.267Z","comments":true,"path":"2021/08/02/模型误差的来源分析/","link":"","permalink":"http://example.com/2021/08/02/%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90/","excerpt":"","text":"@TOC 思路：1、首先，误差的两大来源 bias（偏差） 和 variance（方差） 是指什么2、然后，bias（偏差）和 variance（方差）是怎么产生的3、进一步，如何判断你的模型是 bias大（欠拟合）还是variance大（过拟合），如何解决 ？ 模型的两大来源bias（偏差）和variance（方差） 什么是bias和variance呢？思路： 首先要知道什么是误差 在了解什么是bias和variance 什么是误差？机器学习就是寻找一个函数，然后给它一个输入，就能得到一个理想的输出。f head是理论上找到的最佳函数，f star 是我们用模型预测出来的函数，两者的差值就是误差。 什么是bias和variance（偏差和方差）什么是bias（偏差）？举个栗子说明，下图是用一定样本数的均值m来估计假设的随机变量的平均值u，这是一种无偏估计（unbiased）。 也就是说，估计值的期望等于假设值（如上文的E(m)=u），即为无偏差，反之有偏差（bias）。当样本数越来越大时，m就越靠近u。 什么是variance（方差）方差表达的是数据的离散程度这里对于方差的估计是有差估计： 总结直观理解最后，bias和variance的直观理解： bias表示的是预测的f bar（f star预测值的期望）与f head（实际正确值）的距离 variance表示的是每次预测的f star（预测值）与f bar（预测值的期望）的距离（看图） bias和variance是怎么产生的 下面结合实际的实验说明，bias和variance是如何产生的。————————————————————————————————————————————————————bias如何知晓？，就要做多次实验，确定多个f star（一次实验一个预测值），然后求出f star 样本集的期望（E(f star)）那么首先，我们虚拟出100个神奇宝贝平行宇宙（相当于设置了100组实验），每个预祝一个神奇宝贝训练家捕捉10只神奇宝贝（相当于每组实验10个数据），如下图：然后思考，对于这样的数据，我们选用什么model比较好 ?哪一个model最后的bias比较小？如上图，不同宇宙的神奇宝贝数据非常随机，项次越高，模型越复杂 方差 variance对于方差而言，其表示结果的离散程度，项次越高，模型越复杂，则其离散程度肯定是越大的。因为模型越简单，收到数据的影响也就越低，比如：我极端一点，f(x)=c，数据根本不会影响model最后的预测值 偏差 bias图中看出，简单model可能并不包含目标，因此会造成较大的bias，而复杂的model是涵盖目标的，所以bias小。 偏差和方差的总结 简单的模型（次数小），bias会比较大，但variance会比较小，预测值更加集中。复杂的模型（次数大），bias会比较小，但variance会比较大，预测值更加的离散。因此，我们理想中的目标是找到一个平衡点，使bias和variance尽可能小。 判断你的模型是bias大（欠拟合）还是variance大（过拟合）并解决如何判断 bias过大（欠拟合）需要重新设计模型 模型考虑的特征没有全，也就是很多其实没有作用。关键特征可能还没考虑到，需要加入进去 模型应该需要更加的复杂，增加更高的次项。 variance过大（过拟合） 需要更多的数据，有些数据不够有特点 可以增加一个 正则项，加强模型的平滑度，使其预测值分布不要太离散 解决实际测试比共有数据集误差更大的问题将训练集，分为两部分。这叫做2-折交叉验证。一部分还是当做训练样本，帮助我们进行调参；另一部分用作验证，验证我的模型损失如何，是否合理（充当原来共有训练集的作用）。而现在原有训练集的部分用来充当实际样本数据，算出误差值，以便于真正在实际中误差太大。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"线性回归Regression 案例一：宝可梦进化CP值","slug":"线性回归Regression 案例一：宝可梦进化CP值","date":"2021-07-31T03:16:06.000Z","updated":"2021-09-03T16:06:24.512Z","comments":true,"path":"2021/07/31/线性回归Regression 案例一：宝可梦进化CP值/","link":"","permalink":"http://example.com/2021/07/31/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Regression%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E5%AE%9D%E5%8F%AF%E6%A2%A6%E8%BF%9B%E5%8C%96CP%E5%80%BC/","excerpt":"","text":"@TOC 本栗子：预测Pokemon精灵攻击力。输入：进化前的CP值、物种（Bulbasaur）、血量（HP）、重量（Weight）、高度（Height）输出：进化后的CP值 实现回归的步骤（机器学习的步骤）Step1 确定一个model - 线性模型 输入的特征包括：进化前的CP值（Xcp）、物种（Bulbasaur）（Xs）、血量（HP）（Xhp）、重量（Weight）（Xw）、高度（Height）（Xh） 先从简单的单个特征 进化前的CP值（Xcp）开始（后面改进再考虑多个特征）。 Step2 goodness of function（函数优化）——损失函数 确定model后（案例为线性模型），开始训练数据（使用的为训练样本集） 训练10个训练样本后得到10个的预测进化CP值（y）如下图所示。左侧为训练样本本身真实的进化CP值，右侧为横轴为进化前CP值（Xcp） 确定损失函数。损失函数用于评价一个模型的好坏。损失函数的值越小，那么模型越好。对于本案例，损失函数采用最简单的距离表示。即求实际进化后的CP值与模型预测的CP值差，来判定模型的好坏。 List item Step3 best function（找出最好的一个函数 即调参）——使用梯度下降法 案例采用梯度下降法来帮助选择 w 和 b 两个参数取何值时损失函数最小，也就意味着构建的模型越准确。 什么是梯度下降法？梯度指的是？………………………………………………………………………………………………………梯度？在单变量的函数中，梯度其实就是对应点函数的微分，代表着这个函数在某个给定点的切线的斜率在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向 即例如二维就是 偏导 i + 偏导 j………………………………………………………………………………………………………梯度下降法(SGD Stochastic gradient descent）？答： “下山最快路径”的一种算法我们是尝试使用偏导来衡量函数随自变量的值变化关系，选择变化更为平缓的那一处 对应的 w 和 b 作为调整后的参数 一维视角：只考虑一个参数 w 如上图 learning rate（学习率）：移动的步长 一般用η来表示………………………………………………………………………………………………………如何寻找一个最合适的 w ？步骤1：随机在横轴上选取一个 w0 。步骤2：计算微分，也就是当前的斜率，根据斜率来判定移动的方向。微分大于0应向右移动（增加 w ）；微分小于0向左移动（减少 w ）步骤3：根据学习率，按步骤2得到的方向移动重复步骤2和步骤3，直到找到最低点。横轴即对于的最佳 w 参数………………………………………………………………………………………………………如下图，是经过迭代多次后找到的最佳点（得是全局最优解） 。注：大部分损失函数都为正 二维视角：考虑两个参数 w 和 b如何寻找一个最合适的 w ？步骤1：随机在横轴上选取一个 w0 ，b0 。步骤2：计算各偏导，也就是当前偏导的斜率，根据斜率来判定移动的方向。微分大于0应向右移动（增加 w 或 b）；微分小于0向左移动（减少 w 或 b）步骤3：根据学习率，按步骤2得到的方向移动重复步骤2和步骤3，直到找到最低点。横轴即对于的最佳 w 和 b 参数 二维情况：梯度下降法的效果颜色约深的区域代表的损失函数越小？ 为什么呢？ 梯度算法的优缺点 总结一下梯度下降法： 我们要解决使损失函数L(w,b) 最小时参数的最佳值，梯度下降法是每次update参数值，直到损失函数最小时找到对应最佳的参数w，b 缺点1：求解的未必是全局最优解，有可能是局部最优解。 用下山的例子讲： 比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。……………………………………………………………………………………………………… 从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。 优点1：无论随机从哪个点出发，得到的最佳参数一定是同一组 用下山的例子讲： 因为山只有一处是最低点，在确保能找到全局最优解的情况下，无论人从山上哪一点出发，一定能找到这特定一处的最低洼处。 Step4 回归结果分析 经过上述三个步骤后，得到了训练后的“最佳参数w，b”，那么现在这个模型在测试集上是什么表现呢？得到的结果是：测试集的误差比在训练集上得到的损失值大 这个事非常正常。因为你训练集里面可能有些数据是不典型的，同样测试集中很多也包含了训练集中没有的因素。所以一个函数模型在实际应用中，效果基本上时打折扣的。一般会采用两种方式来加强这个函数模型： select another model（选择另一个模型）即增加维度，增加高此项多项式 consider the hidden factors（考虑其他隐藏因素） 优化模型方法一：增加高次项（一般用于拟合度不够的情况）回到Step1 尝试二次项、三次项、四次项、五次项…… 右上图为训练集损失，下图是测试集的损失。第五次过拟合导致了，测试集损失度很高。 选择合适的模型，即我到底要加到几次项呢？ 越复杂的模型所包含的函数也就越多，那么它包含理想模型的可能性也就越大，但是如果过分地去拟合理想模型，就会出现过拟合的情况。 横向比较各个多项次，训练集和测试集的失误。理论上，失误会随着高次项而变小，如果变大，则表示模型过拟合了。所以，如上图所示，在为3次的时候，训练集和测试集的误差差值最小。并且4次开始，以及存在过拟合现象了。因此本案例可选择3次项线性模型。 优化模型方法二：考虑其他隐藏因素回到Step1 输入的特征包括：进化前的CP值（Xcp）、物种（Bulbasaur）（Xs）、血量（HP）（Xhp）、重量（Weight）（Xw）、高度（Height）（Xh）除了之前考虑的进化前的CP值（Xcp），其他均为隐藏因素………………………………………………………………………………这里另外考虑的是神奇宝贝的种类？因为，有时候一个模型恰恰只能符合一种神奇宝贝，也就是不同神奇宝贝应该有不同的预测模型。 不同的神奇宝贝有不同的预测模型 拟合这个模型的曲线，可以看出不同神奇宝贝拟合成了不同的类线性直线 如果还考虑了其他隐藏元素。如图是横轴是对应的隐藏元素，纵轴是对应进化后的CP值。 如果都采用最高二次项，考虑所有其他的隐藏因素。该案例的数学模型就变成 如图所示的式子。 考虑更多的因素反而出现了过拟合的问题，说明有些因素跟本次实验的CP值预测没有关系！………………………………………………………………………………过拟合这么讨厌，到底如何减少过拟合问题呢？往下看！ 优化模型：防止过拟合（为损失函数加一个正则项）正则项是什么? 方法：正则化?比如先考虑一个参数w，正则化就是在损失函数上加上一个与w（斜率）相关的值（正则项），那么要是loss function越小的话，w也会越小，w越小就使function更加平滑（function没那么大跳跃） 正则化的缺点正则化虽然能够减少过拟合的现象，但是因为加在损失函数后面的值是平白无故加上去的，所以正则化过度的话会导致bias偏差增大 ？？？？ 总结1）实现参数的稀疏有什么好处吗？一个好处是可以简化模型，避免过拟合。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据表现性能极差。另一个好处是参数变少可以使整个模型获得更好的可解释性。2）参数值越小代表模型越简单吗？是的。为什么参数越小，说明模型越简单呢，这是因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。","categories":[{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"线性回归","slug":"线性回归","permalink":"http://example.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"}]}],"categories":[{"name":"深度学习基础","slug":"深度学习基础","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"},{"name":"python工具类","slug":"python工具类","permalink":"http://example.com/categories/python%E5%B7%A5%E5%85%B7%E7%B1%BB/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/categories/Pytorch/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/categories/Keras/"},{"name":"数据集加载","slug":"数据集加载","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD/"},{"name":"深度学习基础-邱锡鹏","slug":"深度学习基础-邱锡鹏","permalink":"http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%82%B1%E9%94%A1%E9%B9%8F/"},{"name":"优化器","slug":"优化器","permalink":"http://example.com/categories/%E4%BC%98%E5%8C%96%E5%99%A8/"},{"name":"损失函数与激活函数","slug":"损失函数与激活函数","permalink":"http://example.com/categories/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://example.com/categories/Tensorflow/"},{"name":"python框架","slug":"python框架","permalink":"http://example.com/categories/python%E6%A1%86%E6%9E%B6/"},{"name":"机器学习基础-李宏毅","slug":"机器学习基础-李宏毅","permalink":"http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"yolo","slug":"yolo","permalink":"http://example.com/tags/yolo/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"argparse","slug":"argparse","permalink":"http://example.com/tags/argparse/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"},{"name":"GoogleNet","slug":"GoogleNet","permalink":"http://example.com/tags/GoogleNet/"},{"name":"Keras","slug":"Keras","permalink":"http://example.com/tags/Keras/"},{"name":"刘二","slug":"刘二","permalink":"http://example.com/tags/%E5%88%98%E4%BA%8C/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://example.com/tags/Kaggle/"},{"name":"数据集","slug":"数据集","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"},{"name":"CNN","slug":"CNN","permalink":"http://example.com/tags/CNN/"},{"name":"分类问题","slug":"分类问题","permalink":"http://example.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"},{"name":"DataLoader","slug":"DataLoader","permalink":"http://example.com/tags/DataLoader/"},{"name":"多维度特征","slug":"多维度特征","permalink":"http://example.com/tags/%E5%A4%9A%E7%BB%B4%E5%BA%A6%E7%89%B9%E5%BE%81/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://example.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"机器学习","slug":"机器学习","permalink":"http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"邱锡鹏","slug":"邱锡鹏","permalink":"http://example.com/tags/%E9%82%B1%E9%94%A1%E9%B9%8F/"},{"name":"神经网络","slug":"神经网络","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://example.com/tags/Tensorflow/"},{"name":"RNN","slug":"RNN","permalink":"http://example.com/tags/RNN/"},{"name":"优化器","slug":"优化器","permalink":"http://example.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"},{"name":"线性模型","slug":"线性模型","permalink":"http://example.com/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"name":"非线性模型","slug":"非线性模型","permalink":"http://example.com/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"name":"激活函数","slug":"激活函数","permalink":"http://example.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},{"name":"Numpy","slug":"Numpy","permalink":"http://example.com/tags/Numpy/"},{"name":"Pandas","slug":"Pandas","permalink":"http://example.com/tags/Pandas/"},{"name":"Matplotlib","slug":"Matplotlib","permalink":"http://example.com/tags/Matplotlib/"},{"name":"李宏毅","slug":"李宏毅","permalink":"http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"无监督学习","slug":"无监督学习","permalink":"http://example.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"name":"聚类","slug":"聚类","permalink":"http://example.com/tags/%E8%81%9A%E7%B1%BB/"},{"name":"降维","slug":"降维","permalink":"http://example.com/tags/%E9%99%8D%E7%BB%B4/"},{"name":"PCA","slug":"PCA","permalink":"http://example.com/tags/PCA/"},{"name":"K-means","slug":"K-means","permalink":"http://example.com/tags/K-means/"},{"name":"HAC","slug":"HAC","permalink":"http://example.com/tags/HAC/"},{"name":"半监督学习","slug":"半监督学习","permalink":"http://example.com/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"name":"卷积核","slug":"卷积核","permalink":"http://example.com/tags/%E5%8D%B7%E7%A7%AF%E6%A0%B8/"},{"name":"过滤器","slug":"过滤器","permalink":"http://example.com/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"name":"Maxpooling","slug":"Maxpooling","permalink":"http://example.com/tags/Maxpooling/"},{"name":"模型优化","slug":"模型优化","permalink":"http://example.com/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"},{"name":"反向传播","slug":"反向传播","permalink":"http://example.com/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"},{"name":"线性回归","slug":"线性回归","permalink":"http://example.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"}]}