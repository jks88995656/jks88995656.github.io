<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只柴犬</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-08-30T02:42:06.929Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>凯凯超人</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Keras  MNIST（数字识别）数据集分类（普通全神经网络）</title>
    <link href="http://example.com/2021/08/30/Keras%20%20MNIST%EF%BC%88%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB%EF%BC%88%E6%99%AE%E9%80%9A%E5%85%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89/"/>
    <id>http://example.com/2021/08/30/Keras%20%20MNIST%EF%BC%88%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB%EF%BC%88%E6%99%AE%E9%80%9A%E5%85%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89/</id>
    <published>2021-08-30T02:40:01.000Z</published>
    <updated>2021-08-30T02:42:06.929Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras  MNIST（数字识别）数据集分类">TOC</a></p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure><h1 id="载入MNIST-数据"><a href="#载入MNIST-数据" class="headerlink" title="载入MNIST 数据"></a>载入MNIST 数据</h1><p>该数据集一共有训练集 6w 张，测试集 1w 张<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_images,train_labels),(test_images,test_labels)= mnist.load_data()</span><br></pre></td></tr></table></figure></p><p>可以查看一下图像和标签  是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数字5</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签5</span></span><br><span class="line"><span class="built_in">print</span>(train_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 数字7</span></span><br><span class="line"><span class="built_in">print</span>(test_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签7</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以 打印 一下图片 看看是什么样子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(test_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/a745fc12bef94f93b93f80b2e89bb0d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="图片数据处理"><a href="#图片数据处理" class="headerlink" title="图片数据处理"></a>图片数据处理</h1><h2 id="查看图片原有-shape"><a href="#查看图片原有-shape" class="headerlink" title="查看图片原有 shape"></a>查看图片原有 shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_images (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images&#x27;</span>, train_images.shape)</span><br><span class="line"><span class="comment"># train_labels (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><h2 id="图片数据处理：将图片压平"><a href="#图片数据处理：将图片压平" class="headerlink" title="图片数据处理：将图片压平"></a>图片数据处理：将图片压平</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment"># 将 (60000, 28, 28) -&gt; (60000, 784)  压平图片</span></span><br><span class="line">train_images_scale = train_images.reshape(train_images.shape[<span class="number">0</span>], train_images.shape[<span class="number">1</span>] * train_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line">test_images_scale = test_images.reshape(test_images.shape[<span class="number">0</span>], test_images.shape[<span class="number">1</span>] * test_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_images变换后 (60000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images变换后&#x27;</span>, train_images_scale.shape)</span><br><span class="line"><span class="comment"># test_images变换后 (10000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_images变换后&#x27;</span>, test_images_scale.shape)</span><br></pre></td></tr></table></figure><h2 id="标签数据处理：转换成-one-hot-格式"><a href="#标签数据处理：转换成-one-hot-格式" class="headerlink" title="标签数据处理：转换成 one hot 格式"></a>标签数据处理：转换成 one hot 格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换 one hot 格式  共十个分类</span></span><br><span class="line"><span class="comment"># np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</span></span><br><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br><span class="line">test_labels_hot = np_utils.to_categorical(test_labels,num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_labels (60000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>,train_labels_hot.shape)</span><br><span class="line"><span class="comment"># test_labels (10000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_labels&#x27;</span>,test_labels_hot.shape)</span><br></pre></td></tr></table></figure><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><p>只有输入和输出层， 输入压平图像 维度为784   输出为 10分类<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 输入压平图像 维度为784   输出为 10分类</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">784</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;softmax&quot;</span>))</span><br></pre></td></tr></table></figure><br>优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新定义 sgd 优化器  加速一下学习率</span></span><br><span class="line">sgd = SGD(learning_rate=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p> 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次<br>训练完一轮6w张，表示一个epoch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次</span></span><br><span class="line"><span class="comment"># 训练完一轮6w张，表示一个epoch</span></span><br><span class="line">model.fit(train_images_scale,train_labels_hot,batch_size=<span class="number">32</span>,epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/576f033299d04330a371c33eb6a2abac.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="训练过程"></p><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/eb52f38d3e944137a3ade6e5b9d1e1a9.png#pic_center" alt="在这里插入图片描述"></p><h1 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看下输入的形状</span></span><br><span class="line"><span class="built_in">print</span>(test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>).shape)</span><br><span class="line"><span class="comment"># 模型预测  输出每个分类的 概率</span></span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>))))</span><br><span class="line"><span class="comment"># 选取最大的那个 就是预测的标签</span></span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>)))))</span><br><span class="line"><span class="comment"># 实际该图片的 标签</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输入的图像 变换后形状<br><img src="https://img-blog.csdnimg.cn/4302df2ba0e44c04897aaf00f6955dad.png#pic_center" alt="在这里插入图片描述"><br>模型预测 数据 输出的概率分类结果<br><img src="https://img-blog.csdnimg.cn/3ffa135e578d486c8404588781678e0c.png#pic_center" alt="在这里插入图片描述"><br>选出其中最大的 以及 实际图片标签 均为 数字7<br><img src="https://img-blog.csdnimg.cn/5a585701d439431aa1853283c8592b46.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras  MNIST（数字识别）数据集分类&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;导入需要的包&quot;&gt;&lt;a href=&quot;#导入需要的包&quot; class=&quot;headerlink&quot; title=&quot;导入需要的包&quot;&gt;&lt;/a&gt;导入需要的包&lt;/h1&gt;&lt;p&gt;首先导入</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>keras 包方法集合</title>
    <link href="http://example.com/2021/08/29/keras%20%E5%8C%85%E6%96%B9%E6%B3%95%E9%9B%86%E5%90%88/"/>
    <id>http://example.com/2021/08/29/keras%20%E5%8C%85%E6%96%B9%E6%B3%95%E9%9B%86%E5%90%88/</id>
    <published>2021-08-29T02:18:01.000Z</published>
    <updated>2021-08-30T02:45:09.137Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="keras 包方法集合">TOC</a></p><h1 id="keras-util-库-np-utils"><a href="#keras-util-库-np-utils" class="headerlink" title="keras.util 库  np_utils"></a>keras.util 库  np_utils</h1><h2 id="np-utils-to-categorical"><a href="#np-utils-to-categorical" class="headerlink" title="np_utils.to_categorical"></a>np_utils.to_categorical</h2><p>np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>如将 $[1,2,3,……4]$ 转化成：<br><img src="https://img-blog.csdnimg.cn/379535ca64bc4e5987673ecd12af0c75.png#pic_center" alt="在这里插入图片描述"><br>这样的形态。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;keras 包方法集合&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;keras-util-库-np-utils&quot;&gt;&lt;a href=&quot;#keras-util-库-np-utils&quot; class=&quot;headerlink&quot; title=&quot;keras.util 库</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Numpy" scheme="http://example.com/tags/Numpy/"/>
    
    <category term="Pandas" scheme="http://example.com/tags/Pandas/"/>
    
    <category term="Matplotlib" scheme="http://example.com/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>各类 优化器(调参工具) 详解与选择</title>
    <link href="http://example.com/2021/08/28/%E5%90%84%E7%B1%BB%20%E4%BC%98%E5%8C%96%E5%99%A8(%E8%B0%83%E5%8F%82%E5%B7%A5%E5%85%B7)%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://example.com/2021/08/28/%E5%90%84%E7%B1%BB%20%E4%BC%98%E5%8C%96%E5%99%A8(%E8%B0%83%E5%8F%82%E5%B7%A5%E5%85%B7)%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2021-08-28T02:40:01.000Z</published>
    <updated>2021-08-30T02:52:24.871Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="各类 优化器 详解与选择">TOC</a></p><h1 id="sgd"><a href="#sgd" class="headerlink" title="sgd"></a>sgd</h1><h1 id="adam"><a href="#adam" class="headerlink" title="adam"></a>adam</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;各类 优化器 详解与选择&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;sgd&quot;&gt;&lt;a href=&quot;#sgd&quot; class=&quot;headerlink&quot; title=&quot;sgd&quot;&gt;&lt;/a&gt;sgd&lt;/h1&gt;&lt;h1 id=&quot;adam&quot;&gt;&lt;a href=&quot;#adam&quot; </summary>
      
    
    
    
    <category term="优化器" scheme="http://example.com/categories/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    
    
    <category term="优化器" scheme="http://example.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Keras 构建 线性模型和非线性模型</title>
    <link href="http://example.com/2021/08/24/Keras%20%E6%9E%84%E5%BB%BA%20%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2021/08/24/Keras%20%E6%9E%84%E5%BB%BA%20%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-08-24T09:22:01.000Z</published>
    <updated>2021-08-24T09:28:44.809Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras 构建 线性模型和非线性模型">TOC</a></p><h1 id="预测线性模型"><a href="#预测线性模型" class="headerlink" title="预测线性模型"></a>预测线性模型</h1><p>使用的数据 是我们随机生成的<br>、<br>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># matplotlib.use(&#x27;TkAgg&#x27;)</span></span><br><span class="line"><span class="comment"># print(matplotlib.get_backend())</span></span><br><span class="line"><span class="comment"># Sequential按顺序构成的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="comment"># Dense全连接层</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br></pre></td></tr></table></figure><p>先准备我们需要的数据。随机生成100个随机值x，并随机产生100个噪声值。我们按 $y=0.1x+0.2$ 的公式，得到对应的y标签值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用numpy 生成100个 随机点</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 测试集，但其实都是随机的 用x_data 也可以</span></span><br><span class="line">x_pre = np.random.rand(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 噪声 使得每个点不是 均匀在一条直线上</span></span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.01</span>,x_data.shape)</span><br><span class="line">y_data = x_data * <span class="number">0.1</span> + <span class="number">0.2</span> + noise</span><br></pre></td></tr></table></figure><p>可以将 100个点的分布图画出。 注意图的显示可能有问题，自行解决一下哦。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><blockquote><p>Q：为什么我们要引入 噪声呢？<br>答：引入噪声可以让我们的数据更加的离散分布 在 我们设计的线性模型上。 使得假设的数据更加的合理。 如下图所示 。 <font color="orange">橙色</font>的是我们设定的线性模型，<font color="blue">蓝色</font>的是 加入噪声以后的数据分布<br><img src="https://img-blog.csdnimg.cn/92fd1e153f2845aba178b80a7845b8cb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><p>使用 keras 中的 Sequential （顺序构成的模型） 构建模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个顺序模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 在模型中添加一个全连接层</span></span><br><span class="line"><span class="comment"># units 输出的维度</span></span><br><span class="line"><span class="comment"># input_dim 输入的维度</span></span><br><span class="line">model.add(Dense(units=<span class="number">1</span>,input_dim=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># sgd 随机梯度下降法</span></span><br><span class="line"><span class="comment"># mse Mean Squared Error 均方误差</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;sgd&#x27;</span>,loss=<span class="string">&#x27;mse&#x27;</span>)</span><br></pre></td></tr></table></figure><p>之后我们就按照批次训练。 共训练3001个批次。有两种写法。<br>方法一：<br>用一个循环体，循环3001次； 每500次 打印一次 损失值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练3001个批次</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3001</span>):</span><br><span class="line">    <span class="comment">#每次训练一个批次</span></span><br><span class="line">    cost = model.train_on_batch(x_data,y_data)</span><br><span class="line">    <span class="comment"># 每500个 batch 打印一次 cost值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;cost:&quot;</span>,cost)</span><br></pre></td></tr></table></figure><br>方法二： 直接使用 model.fit () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x_data,y_data,epochs=<span class="number">3001</span>)</span><br></pre></td></tr></table></figure><p> 可以查看 参数值 W （权重）和 b（偏置值）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W,b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;W：&#x27;</span>,W,<span class="string">&#x27;b:&#x27;</span>,b)</span><br></pre></td></tr></table></figure><p>预测 测试集的 结果 使用 model.predict () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集 输入网络中，得到预测值 y_pred</span></span><br><span class="line">y_pred = model.predict(x_pre)</span><br></pre></td></tr></table></figure><p>可以再 把预测的 图打出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_pre,y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>我们的训练 使用方法二 <strong>结果如图所示：</strong><br><img src="https://img-blog.csdnimg.cn/45c3db30524a45a48e919881f6c36710.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="预测非线性模型"><a href="#预测非线性模型" class="headerlink" title="预测非线性模型"></a>预测非线性模型</h1><p>使用的数据 也是我们随机生成的</p><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便） 注意SGD 需要 tensorflow.keras.optimizers 导入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="comment"># Sequential按顺序构成的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="comment"># Dense全连接层</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Activation</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br></pre></td></tr></table></figure><p>先准备我们需要的数据。用等差数列生成200个值x，并随机产生200个噪声值。我们按 $y=x^{2}$ 的公式，得到对应的y标签值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用numpy 生成 200 个随机点</span></span><br><span class="line">x_data = np.linspace(-<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">x_pre = np.linspace(-<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">200</span>)</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.02</span>,x_data.shape)</span><br><span class="line">y_data = np.square(x_data) + noise</span><br></pre></td></tr></table></figure><p>可以将 200个点的分布图画出。 注意图的显示可能有问题，自行解决一下哦。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>使用 keras 中的 Sequential （顺序构成的模型） 构建模型。 <font color="red">与线性模型的区别在，我们需要 增加激活函数，并且增加一个 中间层（含有10个神经元）。</font>  <font color="blue">并且增加一点 sgd 的学习率，不然学习度太慢，需要的训练次数就会非常大。</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个顺序模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 在模型中添加一个全连接层</span></span><br><span class="line"><span class="comment"># units 输出的维度  维度就是神经元个数</span></span><br><span class="line"><span class="comment"># input_dim 输入的维度</span></span><br><span class="line"><span class="comment"># 需要的神经模型为 1-10-1</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">1</span>,activation=<span class="string">&#x27;tanh&#x27;</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">1</span>,input_dim=<span class="number">10</span>,activation=<span class="string">&#x27;tanh&#x27;</span>))</span><br><span class="line"><span class="comment"># sgd 随机梯度下降法</span></span><br><span class="line"><span class="comment"># mse Mean Squared Error 均方误差</span></span><br><span class="line"><span class="comment"># sgd 的学习率太小 训练次数可能非常多</span></span><br><span class="line"><span class="comment"># 需要修改一下 sgd的学习率</span></span><br><span class="line">sgd = SGD(lr=<span class="number">0.3</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;mse&#x27;</span>)</span><br></pre></td></tr></table></figure><p>之后我们就按照批次训练。 共训练3001个批次。有两种写法。<br>方法一：<br>用一个循环体，循环3001次； 每500次 打印一次 损失值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练3001个批次</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3001</span>):</span><br><span class="line">    <span class="comment">#每次训练一个批次</span></span><br><span class="line">    cost = model.train_on_batch(x_data,y_data)</span><br><span class="line">    <span class="comment"># 每500个 batch 打印一次 cost值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;cost:&quot;</span>,cost)</span><br></pre></td></tr></table></figure><br>方法二： 直接使用 model.fit () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x_data,y_data,epochs=<span class="number">3001</span>)</span><br></pre></td></tr></table></figure><p> 可以查看 参数值 W （权重）和 b（偏置值）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W,b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;W：&#x27;</span>,W,<span class="string">&#x27;b:&#x27;</span>,b)</span><br></pre></td></tr></table></figure><p>预测 测试集的 结果 使用 model.predict () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集 输入网络中，得到预测值 y_pred</span></span><br><span class="line">y_pred = model.predict(x_pre)</span><br></pre></td></tr></table></figure><p>可以再 把预测的 图打出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_pre,y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>我们的训练 使用方法二 <strong>结果如图所示：</strong><br><img src="https://img-blog.csdnimg.cn/79b89f1dec6e4327a2679c2c9882d6c7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras 构建 线性模型和非线性模型&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;预测线性模型&quot;&gt;&lt;a href=&quot;#预测线性模型&quot; class=&quot;headerlink&quot; title=&quot;预测线性模型&quot;&gt;&lt;/a&gt;预测线性模型&lt;/h1&gt;&lt;p&gt;使用的数据 是我</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
    <category term="线性模型" scheme="http://example.com/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="非线性模型" scheme="http://example.com/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>各类 激活函数 详解与选择</title>
    <link href="http://example.com/2021/08/24/%E5%90%84%E7%B1%BB%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://example.com/2021/08/24/%E5%90%84%E7%B1%BB%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2021-08-24T09:22:01.000Z</published>
    <updated>2021-08-24T09:28:29.357Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="各类 激活函数 详解与选择">TOC</a></p><h1 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h1><h1 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h1><h1 id="sigmiod"><a href="#sigmiod" class="headerlink" title="sigmiod"></a>sigmiod</h1><h1 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;各类 激活函数 详解与选择&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Relu&quot;&gt;&lt;a href=&quot;#Relu&quot; class=&quot;headerlink&quot; title=&quot;Relu&quot;&gt;&lt;/a&gt;Relu&lt;/h1&gt;&lt;h1 id=&quot;tanh&quot;&gt;&lt;a href=&quot;#t</summary>
      
    
    
    
    <category term="损失函数与激活函数" scheme="http://example.com/categories/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
    
    <category term="激活函数" scheme="http://example.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>各类 损失函数 详解与选择</title>
    <link href="http://example.com/2021/08/23/%E5%90%84%E7%B1%BB%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://example.com/2021/08/23/%E5%90%84%E7%B1%BB%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2021-08-23T09:22:01.000Z</published>
    <updated>2021-08-24T09:28:21.357Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="各类 损失函数 详解与选择">TOC</a></p><h1 id="mean-squared-error-均方误差"><a href="#mean-squared-error-均方误差" class="headerlink" title="mean_squared_error 均方误差"></a>mean_squared_error 均方误差</h1><p><img src="https://img-blog.csdnimg.cn/a55e390e62dd4e0b8bd1c9ff5cfb7826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;各类 损失函数 详解与选择&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;mean-squared-error-均方误差&quot;&gt;&lt;a href=&quot;#mean-squared-error-均方误差&quot; class=&quot;headerlink&quot; title=&quot;mean_s</summary>
      
    
    
    
    <category term="损失函数与激活函数" scheme="http://example.com/categories/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
    
    <category term="激活函数" scheme="http://example.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow 代码学习</title>
    <link href="http://example.com/2021/08/22/Tensorflow%20%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/"/>
    <id>http://example.com/2021/08/22/Tensorflow%20%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-08-22T06:05:01.000Z</published>
    <updated>2021-08-24T10:26:08.692Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Tensorflow 代码学习">TOC</a></p><p>机器学习的整体思路为：</p><p><img src="https://img-blog.csdnimg.cn/d73975270b58452892a3cd9133defd42.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="用TensorFlow-预测线性模型"><a href="#用TensorFlow-预测线性模型" class="headerlink" title="用TensorFlow 预测线性模型"></a>用TensorFlow 预测线性模型</h1><p>我们以这个做最简单的栗子，题目描述如下图所示：<br><img src="https://img-blog.csdnimg.cn/e862ba592668447dadbb087a5a43ef87.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="问题描述"><br>这就是一个 最简单的数据模型。</p><p>首先我们要引入需要的包，这边使用的是 keras API包<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><br>先构建模型，构建的为一层的神经网络，输入只有一个变量x<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([keras.layers.Dense(units=<span class="number">1</span>,input_shape=[<span class="number">1</span>])])</span><br></pre></td></tr></table></figure><br>再设置 优化器 和 损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;sgd&#x27;</span>,loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)</span><br></pre></td></tr></table></figure><br>准备训练数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xs = np.array([-<span class="number">1.0</span>,<span class="number">0.0</span>,<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>],dtype=<span class="built_in">float</span>)</span><br><span class="line">ys = np.array([-<span class="number">3.0</span>,-<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">3.0</span>,<span class="number">5.0</span>,<span class="number">7.0</span>],dtype=<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure><br>训练模型 迭代500次<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(xs,ys,epochs=<span class="number">500</span>)</span><br></pre></td></tr></table></figure><br>使用模型，对一个 测试集 x 进行预测 y 并输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.predict([<span class="number">10.0</span>]))</span><br></pre></td></tr></table></figure></p><p>总体代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#构建模型</span></span><br><span class="line"><span class="comment">#构建一层的神经网络，并且输入只有1个值</span></span><br><span class="line">model = keras.Sequential([keras.layers.Dense(units=<span class="number">1</span>,input_shape=[<span class="number">1</span>])])</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;sgd&#x27;</span>,loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#准备训练数据</span></span><br><span class="line">xs = np.array([-<span class="number">1.0</span>,<span class="number">0.0</span>,<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>],dtype=<span class="built_in">float</span>)</span><br><span class="line">ys = np.array([-<span class="number">3.0</span>,-<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">3.0</span>,<span class="number">5.0</span>,<span class="number">7.0</span>],dtype=<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">model.fit(xs,ys,epochs=<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用模型，对一个x 进行预测 y</span></span><br><span class="line"><span class="built_in">print</span>(model.predict([<span class="number">10.0</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>结果如下所示：<br><img src="https://img-blog.csdnimg.cn/e82d07bccb7441059c354a3bde71f0e8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="实验结果"><br>预测的结果是 18.977886   按照 正确的数学模型 $y = 2x - 1$ 结果应该是 19 。但深度学习不可能完美预测这个值，只能是近似。</p><h2 id="需要理解的几个点"><a href="#需要理解的几个点" class="headerlink" title="需要理解的几个点"></a>需要理解的几个点</h2><blockquote><p>Q1 ：请问 <code>model.fit(xs,ys,epochs=500)</code> 的 epochs 500 是什么意思？<br>就是针对同一批数据，利用各类算法（比如梯度下降算法），优化训练的次数，理论上训练次数越多，损失函数越小，准确度越高。</p><p>Q2：如何看待这个模型是不是正确的？ 第一，需要看输出的 loss 是不是越来越小 ，accuracy 越来越高。 如果loss<br>不是越来越小，那就说明有问题 第二，你可以看看在测试集 上表现怎么样。</p><p>Q3：请问 这个 epochs 越多越好么？ 当然不是，正常来说 模型在测试集上的表现 是不如训练集的。 要选取一个合适的 epochs 值，不然会出现 过拟合的现象。<img src="https://img-blog.csdnimg.cn/56bdd71ef3784049ad7ddd4003317a7e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>  <strong>注意：过拟合 是在测试集上的概念。是训练集上表现不错，但测试集表现不尽人意，叫做过拟合</strong></p></blockquote><h1 id="用TensorFlow-做全神经网络的图像识别分类"><a href="#用TensorFlow-做全神经网络的图像识别分类" class="headerlink" title="用TensorFlow 做全神经网络的图像识别分类"></a>用TensorFlow 做全神经网络的图像识别分类</h1><p>对 Fashion MNIST 进行图像分类 类别包括<br>0 T-shirt/top(体恤) 1 Trouser(裤子) 2 Pullover(套头衫) 3 Dress(连衣裙) 4 Coat(外套) 5 Sandal(凉鞋) 6 Shirt(衬衫) 7 Sneaker(运动鞋) 8 Bag(袋子) 9 Ankle boot(短靴）<br><img src="https://img-blog.csdnimg.cn/fd0a80f2af654b2d83c71cef332f3e53.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="总体代码"><a href="#总体代码" class="headerlink" title="总体代码"></a>总体代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">fashion_mnist = keras.datasets.fashion_mnist</span><br><span class="line">(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data()</span><br><span class="line"><span class="comment">#具体值 每一个数字都是灰度值</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#可以可视化的 查看其中的图片</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建一个全连接的神经网络</span></span><br><span class="line">model = keras.Sequential()</span><br><span class="line"><span class="comment">#输入层</span></span><br><span class="line">model.add(keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line"><span class="comment">#中间层 128个神经元 激活函数使用relu</span></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">128</span>,activation=tf.nn.relu))</span><br><span class="line"><span class="comment">#输出层 10个神经元 激活函数使用softmax</span></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>,activation=tf.nn.softmax))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中间层  参数共有 100480个</span></span><br><span class="line"><span class="comment">#为 784 × 128 = 100352  还要再加上 每个神经元都有的 bias 100352+128=100480</span></span><br><span class="line"><span class="comment"># 输出层  参数共有 1290个</span></span><br><span class="line"><span class="comment"># 同理 为 128 × 10 + 10 = 1290</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer 优化器   loss：损失函数</span></span><br><span class="line"><span class="comment"># 当标签是除了0，1以外有其他数字的  用sparse_categorical_crossentropy</span></span><br><span class="line"><span class="comment">#为 one-hot  只有一个1 如： [0,0,0,1]用 categorical_crossentropy</span></span><br><span class="line">train_images_scaled = train_images/<span class="number">255</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.optimizers.Adam(),loss=tf.losses.sparse_categorical_crossentropy,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">model.fit(train_images_scaled,train_labels,epochs=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">test_images_scaled = test_images/<span class="number">255</span></span><br><span class="line"><span class="comment"># # 输出 loss 和 accuracy</span></span><br><span class="line"><span class="comment"># model.evaluate(test_images_scaled,test_labels)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(np.shape(test_images[<span class="number">0</span>]/<span class="number">255</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要满足输入的维度, 并从</span></span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images[<span class="number">0</span>]/<span class="number">255</span>).reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images[<span class="number">0</span>]/<span class="number">255</span>).reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>))))</span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>先引入 需要的包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure><h2 id="加载Fashion-MNIST数据集"><a href="#加载Fashion-MNIST数据集" class="headerlink" title="加载Fashion MNIST数据集"></a>加载Fashion MNIST数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fashion_mnist = keras.datasets.fashion_mnist</span><br><span class="line">(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data()</span><br></pre></td></tr></table></figure><p>可以查看一下 图片内容是什么  是个 28 × 28 的二维数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#具体值 每一个数字都是灰度值</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以可视化的 查看一下 这张图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可以可视化的 查看其中的图片</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/c40c76a3833444aa9b950977e09d3a95.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="构造神经元网络模型"><a href="#构造神经元网络模型" class="headerlink" title="构造神经元网络模型"></a>构造神经元网络模型</h2><p>有两种表达方式<br>方式一 ：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建一个全连接的神经网络</span></span><br><span class="line">model = keras.Sequential()</span><br><span class="line"><span class="comment">#输入层</span></span><br><span class="line">model.add(keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line"><span class="comment">#中间层 128个神经元 激活函数使用relu</span></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">128</span>,activation=tf.nn.relu))</span><br><span class="line"><span class="comment">#输出层 10个神经元 激活函数使用softmax</span></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>,activation=tf.nn.softmax))</span><br></pre></td></tr></table></figure><br>方式二 ： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model=tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">512</span>, activation=tf.nn.relu),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax)</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>可以 用summary 函数  查看各层的信息 包括参数等;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/4329a84b31e948dc82268215a010427c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>Q： 中间层参数 100480 怎么来的？</strong><br> 中间层  参数共有 100480个  ：为 784 × 128 = 100352  还要再加上 每个神经元都有的 bias 100352+128=100480<br> 输出层  参数共有 1290个 ： 同理 为 128 × 10 + 10 = 1290</p></blockquote><h2 id="归一化与训练数据"><a href="#归一化与训练数据" class="headerlink" title="归一化与训练数据"></a>归一化与训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_images_scaled = train_images/<span class="number">255</span>  <span class="comment">#归一化</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.optimizers.Adam(),loss=tf.losses.sparse_categorical_crossentropy,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">model.fit(train_images_scaled,train_labels,epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>Q：为什么只对 1875个 进行训练？ epoch 5 的 含义是什么？</strong><img src="https://img-blog.csdnimg.cn/152d19d34215451c90b946d3d3bcbf7e.png#pic_center" alt="在这里插入图片描述"><br> 训练没有问题。正在对1875批次（每批次32张图像）而不是1875张图像进行模型训练。     1875 × 32 = 60000张图像</p></blockquote><h2 id="评估模型-与-测试数据"><a href="#评估模型-与-测试数据" class="headerlink" title="评估模型 与 测试数据"></a>评估模型 与 测试数据</h2><p>评估模型的 loss 和 accuracy 使用 <font color="red">evaluate (测试集全体数据，测试集全体标签) 方法</font><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test_images_scaled = test_images/<span class="number">255</span></span><br><span class="line"><span class="comment"># # 输出 loss 和 accuracy</span></span><br><span class="line">model.evaluate(test_images_scaled,test_labels)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>如果要对 测试集图像 进行预测 需要使用<font color="red"> predict (测试集数据) 方法 </font>  输出最后的输出内容为 10维向量（因为一共0-9 10个分类 输出层已经设定好了） 然后再用 numpy的<font color="red">  argmax  </font> 取得向量中 值最大的那个 就是对应 预测的标签。<br><strong>要注意输入的维度 必须要与 输入层设定的维度 保持一致</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要满足输入的维度, 并从</span></span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images[<span class="number">0</span>]/<span class="number">255</span>).reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line"><span class="comment">#输出预测的标签</span></span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images[<span class="number">0</span>]/<span class="number">255</span>).reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>))))</span><br><span class="line"><span class="comment">#对比一下 真实的标签是什么</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><h2 id="可以设定自动终止训练"><a href="#可以设定自动终止训练" class="headerlink" title="可以设定自动终止训练"></a>可以设定自动终止训练</h2><p>当损失值 小于 0.4 就终止 批次训练<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self,epoch,logs=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="keyword">if</span>(logs. get(<span class="string">&#x27;loss&#x27;</span>)&lt; <span class="number">0.4</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\ nLoss is low so cancelling training!&quot;</span>)</span><br><span class="line">            self.model.stop_training=<span class="literal">True</span></span><br><span class="line">            <span class="built_in">print</span>(model.predict((test_images[<span class="number">0</span>] / <span class="number">255</span>).reshape(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">            <span class="built_in">print</span>(np.argmax(model.predict((test_images[<span class="number">0</span>] / <span class="number">255</span>).reshape(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))))</span><br><span class="line">            <span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br><span class="line">callbacks=myCallback()</span><br><span class="line">mnist=tf.keras.datasets.fashion_mnist</span><br><span class="line">(training_images, training_labels),(test_images, test_labels)=mnist.load_data()</span><br><span class="line">training_images_scaled=training_images/<span class="number">255.0</span></span><br><span class="line">test_images_scaled=test_images/<span class="number">255.0</span></span><br><span class="line">model=tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">512</span>, activation=tf.nn.relu),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax)</span><br><span class="line">])</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">model.fit(training_images_scaled, training_labels, epochs=<span class="number">5</span>, callbacks=[callbacks])</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Tensorflow 代码学习&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;机器学习的整体思路为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/d73975270b58452892a3cd9133defd42.png?x</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>Numpy、Pandas、Matplotlib  常用代码</title>
    <link href="http://example.com/2021/08/21/Numpy%E3%80%81Pandas%E3%80%81Matplotlib%20%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/"/>
    <id>http://example.com/2021/08/21/Numpy%E3%80%81Pandas%E3%80%81Matplotlib%20%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/</id>
    <published>2021-08-21T02:18:01.000Z</published>
    <updated>2021-08-24T09:29:06.390Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Numpy、Pandas、Matplotlib  常用代码">TOC</a></p><h1 id="Numpy-常用代码"><a href="#Numpy-常用代码" class="headerlink" title="Numpy 常用代码"></a>Numpy 常用代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数组</span></span><br><span class="line">n = numpy.arange(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数组，并做2行3列的分隔</span></span><br><span class="line">m = numpy.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据，分隔成3位数组</span></span><br><span class="line">t = numpy.arange(<span class="number">27</span>).reshape(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(t)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载文本，为int方式</span></span><br><span class="line">tx1 = numpy.loadtxt(<span class="string">&quot;numpy.txt&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>, dtype=<span class="string">&quot;int&quot;</span>)</span><br><span class="line"><span class="comment"># 横列替换</span></span><br><span class="line">tx2 = numpy.loadtxt(<span class="string">&quot;numpy.txt&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>, dtype=<span class="string">&quot;int&quot;</span>, unpack=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(tx1)</span><br><span class="line"><span class="built_in">print</span>(tx2)</span><br><span class="line"><span class="comment"># 1:2横截取，[1,2]为选取</span></span><br><span class="line">tx3 = tx1[<span class="number">1</span>:<span class="number">2</span>,[<span class="number">1</span>,<span class="number">2</span>]]</span><br><span class="line"><span class="built_in">print</span>(tx3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 竖拼接</span></span><br><span class="line">tx4 = numpy.vstack((tx1, tx2))</span><br><span class="line"><span class="built_in">print</span>(tx4)</span><br><span class="line"><span class="comment"># 横拼接</span></span><br><span class="line">tx5 = numpy.hstack((tx1, tx2))</span><br><span class="line"><span class="built_in">print</span>(tx5)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br></pre></td></tr></table></figure><h2 id="函数简介"><a href="#函数简介" class="headerlink" title="函数简介"></a>函数简介</h2><h3 id="arrange-函数：用于创建数值范围并返回数组对象"><a href="#arrange-函数：用于创建数值范围并返回数组对象" class="headerlink" title="arrange 函数：用于创建数值范围并返回数组对象"></a>arrange 函数：用于创建数值范围并返回数组对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numpy.arrange([<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>],dtype=numpy.int6或dtype=<span class="string">&#x27;i8&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="linspace-函数：-用于创建等差数组"><a href="#linspace-函数：-用于创建等差数组" class="headerlink" title="linspace 函数： 用于创建等差数组"></a>linspace 函数： 用于创建等差数组</h3><p>numpy.linspace(start,stop,num,endpoint,retstep,dtype)</p><p>dtype：默认为 float64<br>num：设置生成的元素个数<br>endpoint：设置是否包含结束值（stop），False为不包含，<strong>默认为True</strong><br>retstep：设置是否返回步长（即公差），False表示返回，<strong>默认为False</strong>。当值为 True时，返回值为 二元组，包括数组与步长。</p><h3 id="logspace-函数：-用于创建等比数组"><a href="#logspace-函数：-用于创建等比数组" class="headerlink" title="logspace 函数： 用于创建等比数组"></a>logspace 函数： 用于创建等比数组</h3><p>numpy.logspace(start,stop,num,endpoint,base,dtype)</p><p>start：开始值，值为$base^{start}$    =》 base为底的 start次幂<br>stop：结束值，值为$base^{stop}$    =》base为底的 stop次幂<br>base：底数<br>dtype：默认数据类型 float64<br>endpoint：True为包含结束值，默认为True</p><h2 id="numpy-练习题一"><a href="#numpy-练习题一" class="headerlink" title="numpy 练习题一"></a>numpy 练习题一</h2><p><strong>numpy 的基本用法</strong><br>1.导入numpy库<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><br>2.建立一个一维数组 a 初始化为[4,5,6],<br>(1)输出a 的类型（type）<br>(2)输出a的各维度的大小（shape）<br>(3)输出 a的第一个元素（值为4）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span>(a.dtype)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><br>3.建立一个二维数组 b,初始化为 [ [4, 5, 6],[1, 2, 3]]<br> (1)输出各维度的大小（shape）<br> (2)输出 b(0,0)，b(0,1),b(1,1) 这三个元素（对应值分别为4,5,2）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b = np.array([[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">0</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">1</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">0</span>][<span class="number">0</span>],b[<span class="number">0</span>][<span class="number">1</span>],b[<span class="number">1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure></p><p>4  (1)建立一个全0矩阵 a, 大小为 3x3; 类型为整型（提示: dtype = int）(2)建立一个全1矩阵b,大小为4x5; (3)建立一个单位矩阵c ,大小为4x4; (4)生成一个随机数矩阵d,大小为 3x2.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">c = np.zeros([<span class="number">3</span>,<span class="number">3</span>],dtype=<span class="built_in">int</span>)</span><br><span class="line">d = np.ones([<span class="number">4</span>,<span class="number">5</span>],dtype=<span class="built_in">int</span>)</span><br><span class="line">e = np.identity(<span class="number">4</span>,dtype=<span class="built_in">int</span>)</span><br><span class="line">f = np.random.rand(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"><span class="built_in">print</span>(f)</span><br></pre></td></tr></table></figure><br>5  建立一个数组 a,(值为[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]] ) ,(1)打印a; (2)输出 下标为(2,3),(0,0) 这两个数组元素的值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">2</span>][<span class="number">3</span>],a[<span class="number">0</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure><br>6.把上一题的 a数组的 0到1行 2到3列，放到b里面去，（此处不需要从新建立a,直接调用即可）(1),输出b;(2) 输出b 的（0,0）这个元素的值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b = a[<span class="number">0</span>:<span class="number">2</span>,<span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"> <span class="comment">#？？？ 用ndarray不会</span></span><br></pre></td></tr></table></figure><br>7 把第5题中数组a的最后两行所有元素放到 c中，（提示： a[1:2, :]）(1)输出 c ; (2) 输出 c 中第一行的最后一个元素（提示，使用 -1 表示最后一个元素）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">c = a[<span class="number">1</span>:]</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(c[<span class="number">0</span>][-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><br>8.建立数组a,初始化a为[[1, 2], [3, 4], [5, 6]]，输出 （0,0）（1,1）（2,0）这三个元素（提示： 使用 print(a[[0, 1, 2], [0, 1, 0]]) ）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment">#花式索引 第一种</span></span><br><span class="line"><span class="built_in">print</span>(a[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br></pre></td></tr></table></figure><br>9.建立矩阵a ,初始化为[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]，输出(0,0),(1,2),(2,0),(3,1) (提示使用 b = np.array([0, 2, 0, 1]) print(a[np.arange(4), b]))<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#花式索引 第二种</span></span><br><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = np.array([<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(a[np.arange(<span class="number">4</span>),b])</span><br></pre></td></tr></table></figure><br>10.对9 中输出的那四个元素，每个都加上10，然后重新输出矩阵a.(提示： a[np.arange(4), b] += 10 ）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数组广播</span></span><br><span class="line"><span class="built_in">print</span>(a[np.arange(<span class="number">4</span>),b]+<span class="number">10</span>)</span><br></pre></td></tr></table></figure><br><strong>numpy 的 array 数学操作</strong></p><ol><li>执行 x = np.array([1, 2])，然后输出 x 的数据类型<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>12.执行 x = np.array([1.0, 2.0]) ，然后输出 x 的数据类类型<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>13.执行 x = np.array([[1, 2], [3, 4]], dtype=np.float64) ，y = np.array([[5, 6], [7, 8]], dtype=np.float64)，然后输出 x+y ,和 np.add(x,y)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x+y)</span><br><span class="line"><span class="built_in">print</span>(np.add(x,y))</span><br><span class="line"><span class="comment">#总结：在numpy中，add和“+”是一样的</span></span><br></pre></td></tr></table></figure>14  利用 13题目中的x,y 输出 x-y 和 np.subtract(x,y)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x-y)</span><br><span class="line"><span class="built_in">print</span>(np.subtract(x,y))</span><br><span class="line"><span class="comment">#总结：在numpy中，subtract和“-”是一样的</span></span><br></pre></td></tr></table></figure>15  利用13题目中的x，y 输出 x*y ,和 np.multiply(x, y) 还有 np.dot(x,y),比较差异。然后自己换一个不是方阵的试试。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.multiply(x,y))</span><br><span class="line"><span class="built_in">print</span>(np.dot(x,y))</span><br><span class="line"><span class="built_in">print</span>(x*y)</span><br><span class="line"></span><br><span class="line"><span class="comment">##总结：np.multiply()：数组和矩阵对应位置相乘，输出与相乘数组/矩阵大小一致。</span></span><br><span class="line"><span class="comment"># np.dot():执行矩阵乘法运算，若秩为1，则执行对应位置相乘再相加。</span></span><br><span class="line"><span class="comment"># *：对array执行对应位置相乘</span></span><br></pre></td></tr></table></figure>16 利用13题目中的x,y,输出 x / y .(提示 ： 使用函数 np.divide())<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.divide(x,y))</span><br><span class="line"><span class="built_in">print</span>(x/y)</span><br><span class="line"><span class="comment">## np.divide()与 / 效果相同</span></span><br></pre></td></tr></table></figure>17 利用13题目中的x,输出 x的 开方。(提示： 使用函数 np.sqrt() )<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.sqrt(x))</span><br></pre></td></tr></table></figure>18.利用13题目中的x,y ,执行 print(x.dot(y)) 和 print(np.dot(x,y))<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x.dot(y))</span><br><span class="line"><span class="built_in">print</span>(np.dot(x,y))</span><br><span class="line"><span class="comment">##总结：二维数组矩阵之间dot函数运算得到的乘积是矩阵乘积，一维数组是两个向量的内积</span></span><br></pre></td></tr></table></figure>19.利用13题目中的 x,进行求和。提示：输出三种求和<br>(1)print(np.sum(x)): (2)print(np.sum(x，axis =0 )); (3)print(np.sum(x,axis = 1))<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(x))</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(x,axis=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(x,axis=<span class="number">1</span>))</span><br><span class="line"><span class="comment">##总结：axis为0是压缩行,即将每一列的元素相加,将矩阵压缩为一行</span></span><br><span class="line"><span class="comment">## axis为1是压缩列,即将每一行的元素相加,将矩阵压缩为一列，再转置</span></span><br></pre></td></tr></table></figure>20.利用13题目中的 x,进行求平均数（提示：输出三种平均数(1)print(np.mean(x)) (2)print(np.mean(x,axis = 0))(3) print(np.mean(x,axis =1))）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.mean(x))</span><br><span class="line"><span class="built_in">print</span>(np.mean(x,axis = <span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(np.mean(x,axis =<span class="number">1</span>))</span><br><span class="line"><span class="comment">##总结：axis为0是压缩行,即将每一列的元素相加,将矩阵压缩为一行,再取平均值</span></span><br><span class="line"><span class="comment">## axis为1是压缩列,即将每一行的元素相加,将矩阵压缩为一列，再转置，再取平均值</span></span><br></pre></td></tr></table></figure>21.利用13题目中的x，对x 进行矩阵转置，然后输出转置后的结果，（提示： x.T 表示对 x 的转置）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x.T)</span><br></pre></td></tr></table></figure>22.利用13题目中的x,求e的指数（提示： 函数 np.exp()）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.exp(x))</span><br></pre></td></tr></table></figure>23.利用13题目中的 x,求值最大的下标（提示(1)print(np.argmax(x)) ,(2) print(np.argmax(x, axis =0))(3)print(np.argmax(x),axis =1))<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>,<span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>,<span class="number">5</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(np.argmax(x))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(x, axis =<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(x,axis =<span class="number">1</span>))</span><br><span class="line"><span class="comment">##总结： numpy.argmax(array, axis) 用于返回一个numpy数组中最大值的索引值。</span></span><br><span class="line"><span class="comment"># axis=0则竖着看，当axis=0，是在列中比较，选出最大的 行 索引</span></span><br><span class="line"><span class="comment"># axis=1则横着看, 当axis=1，是在行中比较，选出最大的 列 索引</span></span><br></pre></td></tr></table></figure>24,画图，y=x*x 其中 x = np.arange(0, 100, 0.1) （提示这里用到 matplotlib.pyplot 库）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x=np.arange(<span class="number">0</span>,<span class="number">100</span>,<span class="number">0.1</span>)</span><br><span class="line">y=x*x</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>25.画图。画正弦函数和余弦函数， x = np.arange(0, 3 * np.pi, 0.1)(提示：这里用到 np.sin() np.cos() 函数和 matplotlib.pyplot 库)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x=np.arange(<span class="number">0</span>, <span class="number">3</span>*np.pi, <span class="number">0.1</span>)</span><br><span class="line">y1=np.sin(x)</span><br><span class="line">y2=np.cos(x)</span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h1 id="Pandas-常用代码"><a href="#Pandas-常用代码" class="headerlink" title="Pandas 常用代码"></a>Pandas 常用代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文件</span></span><br><span class="line">df = pandas.read_csv(<span class="string">&quot;BeijingPM20100101_20151231.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 展示</span></span><br><span class="line"><span class="comment"># print(df.head())</span></span><br><span class="line"><span class="comment"># print(df.info())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接时间</span></span><br><span class="line">period = pandas.PeriodIndex(year=df[<span class="string">&quot;year&quot;</span>], month=df[<span class="string">&quot;month&quot;</span>], day=df[<span class="string">&quot;day&quot;</span>], hour=df[<span class="string">&quot;hour&quot;</span>], freq=<span class="string">&quot;H&quot;</span>)</span><br><span class="line"><span class="comment"># 将时间数据赋值</span></span><br><span class="line">df[<span class="string">&quot;dataTime&quot;</span>] = period</span><br><span class="line"><span class="comment"># 设置索引</span></span><br><span class="line">df.set_index(<span class="string">&quot;dataTime&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># # print(period)</span></span><br><span class="line"><span class="comment"># print(df.head())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过月份统计</span></span><br><span class="line">df = df.resample(<span class="string">&quot;M&quot;</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># (统计)缺失</span></span><br><span class="line">data = df[<span class="string">&quot;PM_US Post&quot;</span>].dropna()</span><br><span class="line"></span><br><span class="line"><span class="comment"># pylot展示</span></span><br><span class="line">x = data.index</span><br><span class="line">y = data.values</span><br><span class="line"></span><br><span class="line">pyplot.figure(figsize=(<span class="number">20</span>, <span class="number">8</span>), dpi=<span class="number">80</span>)</span><br><span class="line">pyplot.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(x)), y)</span><br><span class="line">pyplot.xticks(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(x), <span class="number">3</span>), x[::<span class="number">3</span>])</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><h1 id="Matplotlib-常用代码"><a href="#Matplotlib-常用代码" class="headerlink" title="Matplotlib 常用代码"></a>Matplotlib 常用代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">7</span>]</span><br><span class="line">y_1 = [<span class="number">10</span>, <span class="number">15</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">15</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">17</span>]</span><br><span class="line">y_2 = [<span class="number">17</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">19</span>, <span class="number">17</span>, <span class="number">13</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">15</span>, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">pyplot.figure(figsize=(<span class="number">20</span>, <span class="number">12</span>), dpi=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整字体</span></span><br><span class="line">matplotlib.rc(<span class="string">&quot;font&quot;</span>, family=<span class="string">&quot;MicroSoft YaHei&quot;</span>,weight=<span class="string">&quot;bold&quot;</span>, size=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变刻度</span></span><br><span class="line"><span class="comment"># pyplot.xticks([ i + 1 for i in range(max(x))], [ &quot;time&quot; + str(i + 1) for i in range(max(x))], rotation=45)</span></span><br><span class="line"><span class="comment"># 第一个参数x轴 第二个展示的内容 rotation 旋转</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 描述</span></span><br><span class="line">pyplot.xlabel(<span class="string">&quot;时间&quot;</span>)</span><br><span class="line">pyplot.ylabel(<span class="string">&quot;温度&quot;</span>)</span><br><span class="line">pyplot.title(<span class="string">&quot;折线图&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 折线图</span></span><br><span class="line">pyplot.plot(x, y_1)</span><br><span class="line"><span class="comment"># pyplot.plot(x, y_2)</span></span><br><span class="line"><span class="comment"># 散点图</span></span><br><span class="line"><span class="comment"># pyplot.scatter(x, y_1)</span></span><br><span class="line"><span class="comment"># pyplot.scatter(x, y_2)</span></span><br><span class="line"><span class="comment"># 柱状图</span></span><br><span class="line"><span class="comment"># pyplot.bar(x, y_1)</span></span><br><span class="line"><span class="comment"># pyplot.bar(x, y_2)</span></span><br><span class="line"><span class="comment"># 横版柱状图</span></span><br><span class="line"><span class="comment"># pyplot.barh(range(len(x)), y_1, height=0.3)</span></span><br><span class="line"><span class="comment"># pyplot.barh(range(len(x)), y_2, height=0.3)</span></span><br><span class="line"><span class="comment"># 直方图</span></span><br><span class="line"><span class="comment"># pyplot.hist(x, (max(x)-min(x))//1)</span></span><br><span class="line">pyplot.xticks(<span class="built_in">range</span>(<span class="built_in">min</span>(x), <span class="built_in">max</span>(x) + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># pyplot.grid()</span></span><br><span class="line"><span class="comment"># 保存图片</span></span><br><span class="line"><span class="comment"># pyplot.savefig(&quot;link.png&quot;)</span></span><br><span class="line"></span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Numpy、Pandas、Matplotlib  常用代码&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Numpy-常用代码&quot;&gt;&lt;a href=&quot;#Numpy-常用代码&quot; class=&quot;headerlink&quot; title=&quot;Numpy 常用代码&quot;&gt;&lt;/a&gt;</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Numpy" scheme="http://example.com/tags/Numpy/"/>
    
    <category term="Pandas" scheme="http://example.com/tags/Pandas/"/>
    
    <category term="Matplotlib" scheme="http://example.com/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习（Unsupervised Learning）之 聚类与降维</title>
    <link href="http://example.com/2021/08/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Unsupervised%20Learning%EF%BC%89%E4%B9%8B%20%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/"/>
    <id>http://example.com/2021/08/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Unsupervised%20Learning%EF%BC%89%E4%B9%8B%20%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/</id>
    <published>2021-08-19T14:36:01.000Z</published>
    <updated>2021-08-20T10:39:03.082Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="无监督学习 Unsupervised  Learning">TOC</a></p><p><img src="https://img-blog.csdnimg.cn/9d69260c8d1442429d24ca7190dd4e5f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>总结 无监督学习 的要点：<br>1、无监督学习的概念</p><pre><code> - 什么叫无监督学习（输入都是无label的数据，没有训练集之说，也就是**只能从一些无label的数据中自己寻找规律**） - 无监督学习的两大任务：“化繁为简”（聚类、降维）、“无中生有”</code></pre><p>2、聚类Clustering（K-means、HAC）<br>3、降维Dimension Reduction（PCA）</p></blockquote><h1 id="无监督学习的具体分类？"><a href="#无监督学习的具体分类？" class="headerlink" title="无监督学习的具体分类？"></a>无监督学习的具体分类？</h1><p><img src="https://img-blog.csdnimg.cn/a3a3f30810614f508add1fd431cb5047.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="无监督学习的分类"></p><ul><li>化繁为简：找一个函数，将本来复杂的输入，变成比较简单的输出。<strong>比如找一个函数，可以把所有的树都变成抽象的树</strong>。因此我们拥有一大堆各种不同的图像的数据，但不知它的 output 长什么样子。</li><li>无中生有：找一个函数，随机给它一个input（比如一个数字1），然后output一棵树，输入数字2，output另外一棵树，输入3，又是另外一棵树。<strong>输入一个随机数，就自动画一张图出来，不同的数画出来的图不一样</strong>。这个任务里面，要找的可以画图的函数，只有output没有input。只有一大堆的图像，但是不知道输入什么数字才可以得到这些图像。</li></ul><h2 id="化繁为简包括-聚类"><a href="#化繁为简包括-聚类" class="headerlink" title="化繁为简包括 聚类"></a>化繁为简包括 聚类</h2><p><img src="https://img-blog.csdnimg.cn/436d9bf4a5a94f78b4ed65378066bcec.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="K-means聚类"></p><blockquote><p><strong>Q：什么是 聚类？</strong></p><p><font color="red"><strong>假设做图像的聚类，现在有一大堆的图像，然后把它们分成各类</strong>。</font>如上图左边的图像都属于 簇1，右边的图像都属于 簇2，上方的图像都属于 簇3。<strong>这就像给图像贴标签，把类似的图像，都用同一个簇表示</strong>，就做到化繁为简这件事情。</p><p><strong>Q：聚类的注意点是什么？</strong><br> 是 这些数据到底有多少个簇！ 这和神经网络需要设计几层一样，是需要算法工程师的个人经验的。</p><p><font color="red"><strong>这个簇不能太多也不能太少。</strong> </font>比如多到说9张图像9个簇，那聚类就没有意义，直接每个图像一个簇就好了，或者说全部图像都是一个簇，也跟没有做一样。</p></blockquote><p>聚类方法最常用的就是K-means，有一大堆未标注数据 $x^{1}$ 到 $x^{n}$ ，每一个 $x$ 代表一张图像，做成 K 个簇。</p><h3 id="K-means聚类算法怎么做？"><a href="#K-means聚类算法怎么做？" class="headerlink" title="K-means聚类算法怎么做？"></a>K-means聚类算法怎么做？</h3><p>先找簇的中心，假如每一个对象都用一个向量表示，有 K 个簇就需要 $c^{1}$  到 $c^{K}$  个中心。可以从训练数据里<strong>随机找 K个对象出来作为初始化中心。</strong><br>而后对所有数据，决定属于哪一个簇。假设 $x^{n}$  和 $c^{i}$  最接近，那么 $x^{n}$ 就属于 $c^{i}$ ，用 $b_{n}^{i}$ 表示。然后更新簇，所有属于 $c^{i}$ 的数据做平均，就是第 $i$ 个簇新的中心，更新要反复进行。</p><blockquote><p><strong>Q：为什么 是从数据集中挑选 K 个样本做初始化 簇中心？</strong><br><strong>答</strong>：之所以从数据集挑选K个样本做初始化簇中心，有一个很重要的原因是，如果是纯粹随机的（不从数据集里挑），那很可能在第一次分配这个簇中心的时候，没有任何一个样本跟这个中心很像，也可以说这个簇没有任何样本，再次更新就会出错。</p></blockquote><p>K-means 用更简单的话来说：<br>其算法思想大致为：先从样本集中随机选取 K 个样本作为簇中心，并计算所有样本与这 K 个“簇中心”的距离，对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中，对于新的簇计算各个簇的新的“簇中心”。循环反复。</p><blockquote><p><strong>Q：总结一下 K-means 算法的 主要流程</strong></p><ol><li>簇个数 K 的选择</li><li>初始化簇中心（可以从你的train data里面随机找K个x出来，就是你的k个center）</li><li>while（收敛——聚类结果不再变化）<br>　　{<br>　　　　 各个样本点到“簇中心”的距离 ；<br>  　　 根据新划分的簇，更新“簇中心”（求均值）;<br>　　}</li></ol></blockquote><h3 id="层次凝聚聚类算法（HAC）怎么做？"><a href="#层次凝聚聚类算法（HAC）怎么做？" class="headerlink" title="层次凝聚聚类算法（HAC）怎么做？"></a>层次凝聚聚类算法（HAC）怎么做？</h3><p>首先 我们要做一个树结构 （<strong>其过程 非常像 哈夫曼树的构造</strong>）<br><img src="https://img-blog.csdnimg.cn/463ed5f3dfb84f1bb525e37a39dfd42b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="HAC构建树"></p><p>假设有5个样本做层次聚类，先要做一个树结构。计算两两样本的相似度，挑出最相似的数据对。</p><p>比如第一个和第二个样本最相似，那就合并（比如用平均值代表），5个样本变为4个样本；再计算相似度，配对的是4，5样本，然后把他们合并（平均值），变成3个样本；接着计算相似度，配对的是黄色数据点和剩下的蓝色数据点，再次合并（平均），最后只剩红色和绿色，那么最后平均起来得到root。根据5笔数和之间的相似度，就建立出了一个树结构。</p><p><strong>但树结构只是告诉我们说哪些样本比较像，还没有做聚类。</strong></p><blockquote><p><strong>Q：那怎么做聚类呢？或者说我怎么看我分的那几个聚类？</strong><br><strong>答</strong>： 看你怎么切，如图上面不同颜色的切线。</p><ul><li>比如在上图蓝线初切一刀，意味着把数据分成3簇，1、2为一簇，3单独为一簇，4、5为一簇。</li><li>在红色线切一刀，则1、2、3为一簇，4、5为一簇。</li><li>在绿色点切一刀，则1、2为一簇，3、 4、 5单独为一簇。</li></ul><p><strong>Q：层次聚类 和 K-means的差别？</strong></p><ul><li>在K-means里要自己决定K的值，也就是你要分多少个簇。<ul><li>在层次聚类里要决定的是在哪里切一刀，如果切比较容易考虑的话，那层次聚类可能更好。</li></ul></li></ul></blockquote><h2 id="化繁为简包括-降维"><a href="#化繁为简包括-降维" class="headerlink" title="化繁为简包括 降维"></a>化繁为简包括 降维</h2><blockquote><p><strong>Q：什么是降维？</strong><br><strong>答</strong>：降维意思是说，原本高维的东西，其实是可以用低维去表示它。就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。换句话说，可以减少数据的维度。就是 </p><script type="math/tex; mode=display">z = Wx</script><p><img src="https://img-blog.csdnimg.cn/0f4adf004785425fa8e753ffc1e8c025.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>Q：为什么降维有用？</strong><br><strong>答</strong>：假设数据分布如上图左边，在3D空间里分布是螺旋的样子，但是用3D描述数据分布比较浪费的，直觉上也可以感觉可以摊开变成右边2D的样子，只需要2D的空间就可以描述3D的信息。在3D空间里面解比较麻烦，那就在2D里做这个任务。<br>考虑一个实际的简单栗子：<br>每一个input的数字都是28 × 28的矩阵来描述。但是实际上，多数28 × 28矩阵转成一个图像看起来都不像数字，在28 × 28空间里是数字的矩阵是很少的。所以要描述一个数字，或许不需要用到28 × 28维，远比28 × 28维少。<br><img src="https://img-blog.csdnimg.cn/1d2cd0f03fb540a886301947ef7bef60.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>所以举一个极端的例子，有一堆3，从像素点看要用28 × 28维来描述每张图像。实际上，只要用一个维度就可以表示，中间的是3，其他的3都是中间的3左转右转10、 20度。所以唯一需要记录的就是中间的3，左转和右转了多少度，即只需要角度的变化，就可以知道28维空间中的变化。</p><p><strong>Q：怎么做降维？</strong><br><strong>答</strong>：找一个函数，input是一个向量x，output是另外一个向量z（z的维度比x小）。</p><ul><li><p>在降维里最简单的方法是特征选择，把数据的分布拿出来看一下，    如在二维平面上发现数据集中在 $x$ 维度，所以 $y$ 这个维度没什么用，那么就把他拿掉，等于是降维这件事。<strong>特征选择不一定有用，有可能case里面任何一个维度都不能拿掉。</strong></p></li><li><p>另一个常见的方法是<strong>PCA</strong>，函数是一个很简单的线性函数，input x和output z之间的关系就是一个线性的transform，即 $x$ 乘上一个矩阵 $W$ 得到 $z$ 。现在不知道  $z$ 长什么样子，要根据一大堆的 $x$ 把 $W$ 找出来。</p></li></ul></blockquote><h3 id="分布式表示（Distributed-Representation）"><a href="#分布式表示（Distributed-Representation）" class="headerlink" title="分布式表示（Distributed Representation）"></a>分布式表示（Distributed Representation）</h3><blockquote><p><strong>Q：光做聚类的话是非常以偏概全的。为什么呢？</strong><br><strong>答</strong>：因为在聚类思想中，每个样本都必须属于某一个簇。就好像念力分成6大类，每个人都会被分配到6个大类其中一类。但这样分配太过粗糙，比如某个人的能力既有强化系的特性又有放出系的特性，只分为一类就会丢失很多信息。<img src="https://img-blog.csdnimg.cn/1ee8b3f2f1754e63bdc4826eba1aa3dc.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><font color="red">**只分为一类就是以偏概全了，应该要用一个向量来表示每个对象，向量的每个维度代表了某一种特质（属性）。这件事情叫做Distributed Representation**。</font> 比如上图所示，这个人每个系都可以有固定的能力占比。<font color="green">如果对象是一个高维的东西，例如图像，现在用它的特性来表示，就会把它从高维空间变成低维空间，这件事情叫做降维。</font> Distributed Representation和**降维**是一样的东西，不同的称呼。### 主成分分析（PCA）函数是一个很简单的线性函数，input x和output z之间的关系就是一个线性的transform，即 $x$ 乘上一个矩阵 $W$ 得到 $z$ 。现在不知道  $z$ 长什么样子，要根据一大堆的 $x$ 把 $W$ 找出来。PCA的实现一般有两种： - 一种是用特征值分解去实现的 - 一种是用奇异值分解去实现的#### PCA-用特征值分解实现![在这里插入图片描述](https://img-blog.csdnimg.cn/3acdbf6ffca44f85b12bb6a0719ae8c9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)刚才讲过PCA要做的事是找 $W$ ，假设一个比较简单的case，考虑一个维度的case。假设要把我们的数据投射到一维空间上，即 $z$ 只是一维的向量。 $w^{1}$ 是W的第一行，和 $x$ （列向量）做内积得到一个标量 $z_{1}$ 。> **Q：$w^{1}$应该长什么样子？**> 首先假设 $w^{1}$ 的长度是1，即 $||w^{1}||_{2}=1$ 。如果$||w^{1}||_{2}=1$，$w^{1}$ 是高维空间中的一个向量，那么 $z_{1}$ 就是就是 $x$ 在 $w^{1}$ 上的投影长度。现在要求出每一个 $x$ 在 $w^{1}$ 上的投影，那 $w^{1}$ 应该长什么样子？举个例子，假设上图右上方是 $x$ 的分布，$x$ 都是二维的，每个点代表一只宝可梦，横坐标是攻击力，纵坐标是防御力。<font color="blue">现在要把二维投影到一维，应该要选什么样的 $w^{1}$ ?</font>  可以选 $w^{1}$ 为上图右上方右斜方向，也可以选左斜方向，**选不同的方向，最后得到的投影的结果会不一样**。那总要给我们一个目标，我们才知道要选什么样的 $w^{1}$ ，现在目标是经过投影后得到的 $z_{1}$ 的分布越大越好。我们不希望投影后所有的点都挤在一起，把本来数据点之间的奇异度消去。我们希望投影后，数据点之间的区别仍然看得出来，那么我们可以找投影后方差越大的那个 $w^{1}$ 。看上面的例子，如果是右斜方向，那么方差较大，左斜方向方差则较小，所以更可能选择右斜方向作为 $w^{1}$ 。从上面的例子里看， $w^{1}$ 代表了宝可梦的强度，宝可梦可能有一个隐藏的向量代表它的强度，这个隐藏的向量同时影响了防御力和攻击力，所以防御力和攻击力会同时上升。![在这里插入图片描述](https://img-blog.csdnimg.cn/dd45a73b7b0640d1bbb675b9132e5044.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)开始计算：　　　　1、把方差式子展开，转化成协方差（具体转化过程不描述了）　　　　2、<font color="red">结论：我们要找的 $w^{1}$ 就是协方差矩阵 $S$ 的最大特征值所对应的特征向量， $w^{2}$ 就是协方差矩阵 $S$ 的第二大特征值所对应的特征向量，以此类推 .</font><h4 id="PCA-用奇异值分解（SVD）"><a href="#PCA-用奇异值分解（SVD）" class="headerlink" title="PCA - 用奇异值分解（SVD）"></a>PCA - 用奇异值分解（SVD）</h4><p>特征值分解是一个提取矩阵特征很不错的方法，但是<strong>特征值分解只是对方阵而言的</strong>，在现实的世界中，我们看到的<strong>大部分矩阵都不是方阵</strong>。奇异值分解是一个能适用于任意的矩阵的一种分解的方法。<br><img src="https://img-blog.csdnimg.cn/31679a1b2ba54844b2799661d9c1ff6c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>假设现在考虑手写数字识别，我们知道手写数字其实是由一些基本成分组成的，这些基本成分可能是笔画。例如斜的直线，横的直线，比较长的直线，小圈、大圈等等，这些基本成分加起来以后得到一个数字。</p><p>基本成分我们写作 $u^{1},u^{2},u^{3}…$，这些基本的成分其实就是一个一个的向量。考虑 MNIST数据集 ，一张图像是28 × 28像素，就是28 × 28维的向量。基本成分其实也是28 × 28维的向量，把这些基本成分向量加起来，得到的向量就代表了一个数字。</p><p>如果写成公式的话，就如上图最下方所示的公式。$x$ 代表某一张图像的像素，用向量表示。$x$ 会等于 $u^{1}$ 这个成分乘上 $c<em>{1}$ ，加上 $u^{2}$ 这个成分乘上 $c</em>{2}$，一直加到 $u^{K}$ 这个成分乘上$c_{K}$，再加上 $\bar{x}$（$\bar{x}$ 是所有图像的平均）。所以每一张图像，就是一堆成分的线性组合加上所有图像的平均所组成的。</p><p>例如数字7是 $u^{1},u^{3},u^{5}$ 加起来的结果，那么对数字7来说，公式里的 $c<em>{1}=1 ,c</em>{2}=0, c<em>{3}=1…$，所以可以用  $c</em>{1},c<em>{2},c</em>{3}…,c<em>{K}$ 来表示一张图像，如果成分远比像素维度小的话，那么用$\begin{bmatrix}<br>c</em>{1}\<br>c<em>{2}\<br>…\<br>c</em>{K}\<br>\end{bmatrix}$表示一张图片是会比较有效的比如7可以由向量 $\begin{bmatrix}<br>1\<br>0\<br>1\<br>0\<br>1\<br>…\<br>\end{bmatrix}$ 描述。</p><p><img src="https://img-blog.csdnimg.cn/dc78fa3c54bc4b2f94be1efcc5bdf034.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>我们把公式里的 $\bar{x}$ 移到左边，$x$ 减 $\bar{x}$ 等于一堆成分的线性组合，写作 $\hat{x}$  。</p><blockquote><p><strong>Q：如果我们不知道K个u（成分）是什么，那怎么找出这K个向量？</strong> 找K个u，让$x−\bar{x}$ 和 $\hat{x}$  越接近越好，$||(x-\bar{x})-\hat{x}||<em>{2}$ 称为重构误差，代表没办法用成分描述的部分。接下来，最小化 $||(x-\bar{x})-\hat{x}||</em>{2}$，损失函数如上图 $L$。</p><p>回忆下PCA，$w<em>{1},w</em>{2},w<em>{3}…w</em>{K}$ 是 $x$ 协方差矩阵的特征向量，事实上 $L$ 的解就是PCA的 $w<em>{1},w</em>{2},w<em>{3}…w</em>{K}$。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/cf51702d73ef4fa3ac7f2dd6c47ec6d6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="PCA实例"><a href="#PCA实例" class="headerlink" title="PCA实例"></a>PCA实例</h1><h2 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h2><p><img src="https://img-blog.csdnimg.cn/9581a527b4a0468f97458d5954e6bb89.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>以把每一张数字图像拆成成分的线性组合，每一个成分也是一张图像（28 × 28 维的向量），所以可以把成分画在图上变成一张图像。</p><font color="blue">通过PCA画出前30个成分如上图所示，白色的地方代表有笔画。用这些成分做线性组合，就可以得到0-9的数字，所以这些成分叫做Eigen-digit。</font> Eigen（本征）是说，这些成分都是协方差矩阵的特征向量。## 人脸识别![在这里插入图片描述](https://img-blog.csdnimg.cn/9e97b877a5114592ac90e2b7f5f3c0f6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)上图右上方有一大堆人脸，找它们前30个主成分。找出来就如上图最下方所示，每张图像都是哀怨的脸，叫做Eigen-face。把这些脸做线性组合，就可以得到所有的脸。> **Q：但这边有没有觉得有问题，因为主成分找出来的是成分，但是现在找出来的几乎都是完整的脸，也不像是成分啊？像前面的数字识别，成分看起来也像是玛雅文字，而不是笔画，看起来也不是成分啊？**![在这里插入图片描述](https://img-blog.csdnimg.cn/1215e23adb994fd6b8380a73b1e64527.png#pic_center)> **答**：仔细想想PCA的特性，$α_{1},α_{2}$ 这种权重可以是任何值，可以是正的，也可以是负的。所以当我们用这些主成分组成一张图像的时候，<font color="blue">可以把这些成分相加，也可以把这些成分相减，这就会导致你找出的东西不见得是一个图的基本的结构。</font>> > 比如我画一个9，那可以先画一个8，然后把下面的圆圈减掉，再把一杠加上去。我们不一定是把成分加起来，也可以相减，<font color="blue">所以说就可以先画一个很复杂的图，然后再把多余的东西减掉。这些成分不见得就是类似笔画的这种东西。</font>> > 如果要得到类似笔画的东西，就要用另一个技术*NMF（非负矩阵分解）*。PCA可以看成是对矩阵X做SVD，SVD就是一种矩阵分解的技术。**如果使用NMF，就会强迫所有成分的权重都是正的，正的好处就是一张图像必须由成分叠加得到，不能说先画一个复杂的东西再去掉一部分，再来就是所有成分的每个维度都必须是正的。**所以在同样的任务上，例如手写数字的测试上，使用NMF时，找出来的主成分会如下图所示。![在这里插入图片描述](https://img-blog.csdnimg.cn/ac0fc298e03847819527bc49a3198b45.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)你会发现，白色图案类似于笔画，找出来的主成分就成了笔画了。![在这里插入图片描述](https://img-blog.csdnimg.cn/36c61755d6ce40318f4df22c47721ab5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)看脸的话，会发现如上图所示。比较像脸的一部分，比如人中、眉毛、嘴唇、下巴。## 宝可梦![在这里插入图片描述](https://img-blog.csdnimg.cn/c19d6fca60984ad6a435fd0750f485fa.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)有800种宝可梦，每种宝可梦可以用6个特征来表示。所以每个宝可梦就是6维的数据点，6维向量。现在用PCA来分析，PCA里常有的问题是到底需要几个成分，即到底要把数据降到几维。这个一般取决于你的目的是什么，比如你想做可视化，分析宝可梦特性之间的关系，6维没办法可视化的，那就投影到二维。要用几个主成分就好像是神经网络需要几层，每层几个神经元一样。一个常见决定使用几个主成分的方法是，去计算每个主成分（特征向量）对应的特征值，这个特征值代表在该主成分上投影数据的方差。现在的例子里宝可梦是6维的，那就有6 × 6维的协方差矩阵，所以有6个特征值，如上图计算每个特征值比例，结果是0.45，0.18，0.13，0.12，0.07，0.04。那第5、6个主成分的作用比较小，意味着投影数据的方差很小，宝可梦的特性在这两个主成分上信息很少。那么分析宝可梦特性只需要前4个主成分。![在这里插入图片描述](https://img-blog.csdnimg.cn/8fcb23f86b76476e9b66585379b04cbb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)PCA后选择4个主成分，每个主成分是一个6维向量（因为原来每个特征都要投影，那就有6种投影数据）。每个宝可梦可以想成是4主成分向量做线性组合的结果，且每只宝可梦组合的权重不同。看第一个主成分PC1，数值都是正的，如果给它的权重大，意味着宝可梦6维都是强的，给它的权重小，意味着宝可梦6维都是弱的，所以第一个主成分，代表了这只宝可梦的强度。看第二个主成分PC2，Def防御力是正值，速度是负值，那么增加权重的时候，会增加防御力并减小速度。把第一个和第二个主成分画出来如上图最下方，图上有800个点，每个点代表一只宝可梦。![在这里插入图片描述](https://img-blog.csdnimg.cn/995d8f5d622b4b3691e432d0e2554a0e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)第三个主成分PC3，特殊防御力是正的，攻击力和HP都是负的，也就是说这是用攻击力和HP来换取特殊防御力的宝可梦。第四个主成分PC4，HP是正的，攻击力和防御力是负的，这是用攻击力和防御力换取生命值的宝可梦。把第三、第四主成分画出来如上图最下方，维度是去相关的。## 矩阵分解-推荐系统有时候，你会有两种东西，两种对象，它们之间受到某种共通的潜在因素操控。假设现在做一个调查，调查每个人手上买的公仔的数目，有5个宅男同学A,B,C,D,E，横轴的公仔人物是凉宫春日、御坂美琴、小野寺、小唯，调查结果如下图。![在这里插入图片描述](https://img-blog.csdnimg.cn/7557d3f48de34c97be70ec2ca4212e74.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)看这个矩阵可以发现，买凉宫春日的人，比较有可能有御坂美琴；买小野寺的人，也比较有可能买小唯。这说明人和公仔有一些共同的特性，有共同的因素在操控这些事情发生。动漫宅获取可以分成两种，一种是萌傲娇的，一种萌天然呆的。每个人都是萌傲娇和萌天然呆平面上的一个点，可以用一个向量表示，那么看上图，A是偏萌傲娇。每一个公仔角色，可能有傲娇属性或者天然呆属性，所以每一个角色，也是平面上一个点，可以用一个向量描述。如果某个人的属性和角色的属性匹配的话，他们背后的向量就很像（比如做内积的时候值很大），那么A就会买很多的凉宫春日。他们匹配的程度取决于潜在因素是不是匹配的。![在这里插入图片描述](https://img-blog.csdnimg.cn/d5f8553af83f4034bbcff5b07c95251a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)所以ABC的属性如上图最左边所示，A、B是萌傲娇的，B稍微没有那么傲娇，C是萌天然呆。每个动漫角色后面也有傲娇、天然呆这两种属性，如果人物属性和角色属性匹配的话，人买角色的可能性就很大。![在这里插入图片描述](https://img-blog.csdnimg.cn/006698ec686f439ea7e5161e55e7213f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)上图中右下方矩阵公式中，右边两个矩阵的N应该是M，代表M个人。我们知道的只有人买的角色的数目，然后凭着这种关系去推论每个人和每个动漫人物背后的潜在因素。每个人背后都有一个向量，代表萌傲娇或者萌天然呆的程度。每个角色后面也有一个序列，代表是傲娇或天然呆的属性。我们可以把购买的公仔数量合起来看做是一个矩阵X ，行数是人的数量，列数是公仔角色的数量。现在有一个假设，矩阵X里的每个元素都来自于两个向量的内积。为什么A会有5个凉宫春日的公仔，是因为 $r^{A}·r^{1}$ 的内积很大，约等于5。这件事情用数学公式表达的话，可以把 $r^{A}$ 到 $r^{M}$ 按列排起来，把 $r^{1}$ 到 $r^{4}$ 按行排起来，<font color="red">K是潜在因素的个数，一般没办法知道，需要自己测试出来。</font><blockquote><p><strong>Q：矩阵X的每个维度是什么？</strong><br>我们要做的事情就是找一组rA到rE，找一组r1到r4 ，让两个矩阵相乘后和矩阵X越接近越好，就是最小化重构误差。这个就可以用SVD来解，把Σ并到左边或右边变成两个矩阵就可以了。<br><img src="https://img-blog.csdnimg.cn/b2a2e51b938840b49fad8bcf3521f71b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>有时候有些信息是缺失的，比如上图所示的，你不知道A、B、C手上有没有小野寺，可能在那个地区没有发行，所以不知道发行的话到底会不会买。那用SVD就很怪，也可以把缺失值用0代替，但也很奇怪。</p><p>Q：那有缺失值怎么办呢？<br>可以用梯度下降的方法来做，写一个损失函数，让$r^{i}$（每个人背后的潜在因素）和$r^{j}$（角色背后的潜在因素）的内积和角色购买数量越接近越好。现在重点是，在<br>summation over<br>元素的时候，可以避开缺失的数据，如果值是缺失的，就不计算。有了损失函数后，就可以使用梯度下降了。<img src="https://img-blog.csdnimg.cn/7da15f3364a64482b6e8a8ed3d751acb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>根据刚才的方法实际计算一下，假设潜在因素的数量是2。那么A到E都是二维的向量，每个角色也是二维的向量。<br>数值代表了属性的程度，把大的用红色框框圈出来，会发现A、B萌同一组属性，C、D、E萌同一种属性，1,2有同样的属性，3,4有同样的属性。没有办法知道每个属性代表什么，要先找出这些潜在因素，再去分析它的结果。有了这些潜在因素数据，就可以用来预测缺失值。已经知道了$r^{A}$和$r^{3}$，那只要$r^{A}$和$r^{3}$做内积就可以了。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/d1cb16fb971b456ebe4b9e64b666e212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>之前的model可以做得更精致一点，刚才说A背后的潜在因素乘上 春日 背后的潜在因素，得到的结果就是矩阵里的数值。但是事实上，可能还会有其他因素操控这些数值。<br>那么更精确的写法就可以写成。</p><script type="math/tex; mode=display">r^{A}⋅r^{1}+b_{A}+b_{1}≈5</script><p>$b<em>{A}$是跟 $A$ 有关的标量，代表了 $A$ 有多喜欢买公仔，有的人就是喜欢买公仔，也不是喜欢某个角色。$b</em>{1}$是跟 春日 有关的标量，代表了角色有多想让人购买，这个事情是跟属性无关的，本来人就会买这个角色。</p><p>然后修改损失函数如上图所示，使用梯度下降求解即可。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;无监督学习 Unsupervised  Learning&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/9d69260c8d1442429d24ca7190dd4e5f.png?x-oss-p</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="无监督学习" scheme="http://example.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="聚类" scheme="http://example.com/tags/%E8%81%9A%E7%B1%BB/"/>
    
    <category term="降维" scheme="http://example.com/tags/%E9%99%8D%E7%BB%B4/"/>
    
    <category term="PCA" scheme="http://example.com/tags/PCA/"/>
    
    <category term="K-means" scheme="http://example.com/tags/K-means/"/>
    
    <category term="HAC" scheme="http://example.com/tags/HAC/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习 Semi-Supervised</title>
    <link href="http://example.com/2021/08/17/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20Semi-Supervised/"/>
    <id>http://example.com/2021/08/17/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20Semi-Supervised/</id>
    <published>2021-08-17T14:36:01.000Z</published>
    <updated>2021-08-19T07:05:04.586Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="半监督学习 Semi-Supervised">TOC</a></p><blockquote><p>总结 半监督学习 的要点：<br>Q1：什么是Semi-Supervised？<br>Q2：Semi-Surpervised在生成模型中如何实现的（EM算法）？<br>Q3：Semi-Surpervised基于Low-density Separation（低密度分离）假设是如何实现的？<br>Q4：Semi-Surpervised基于Smoothness Assumption（平滑）假设是如何实现的？</p></blockquote><h1 id="什么是Semi-Supervised？"><a href="#什么是Semi-Supervised？" class="headerlink" title="什么是Semi-Supervised？"></a>什么是Semi-Supervised？</h1><p><img src="https://img-blog.csdnimg.cn/a671d2d30c944294a8b44fad632f7435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>大家知道在监督学习里，有一大堆的训练数据（由input和output对组成）。例如上图所示 $x^{j}$是一张图片，$y^{r}$ 是类别的 <em>label</em>。<br><strong>半监督学习是说，在label数据上面，有另外一组unlabeled的数据</strong>，写成$x^{u}$ (只有 input 没有 output )，有U笔 unlabeled 的数据。<br>通常做半监督学习的时候，我们常见的情景是 <strong>unlabeled 的数量远大于labeled 的数量</strong>（U&gt;&gt;R)。</p><blockquote><p><strong>举个栗子：现在我们要做一个猫狗分类</strong></p><ul><li>如果只考虑 labeled data，我们分类的分界线会画在中间；    </li><li>如果把 unlabeled data 也考虑进去，我们可能会根据 unlabeled data 的分布，分界线画成图中的斜线； 　<br>semi-supervised earning使用 unlabel 的方式往往伴随着<strong>一些假设</strong>，学习有没有用，取决于你这个<font color="red"><strong>假设合不合理</strong></font>。（比如灰色的点也可能是个狗不过背景跟猫照片比较像）<img src="https://img-blog.csdnimg.cn/994e98b4ab274acc8f394810b34dc7bf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul></blockquote><p>半监督学习可以分成两种：</p><ul><li>一种叫做<strong>转换学习</strong>，unlabeled 数据就是 testing set ，使用的是testing set的特征。</li><li>另一种是<strong>归纳学习</strong>，不考虑testing set，学习model的时候不使用testing set。</li></ul><blockquote><p><strong>Q：用 testing set 作为 unlabeled 数据，不是相当于用到了未来数据吗？</strong><br><strong>答</strong>：用了 label 数据才算是用了未来数据，用 testing set 的特征不算是使用未来数据。<br><strong>Q：什么时候使用转换学习或者归纳学习？</strong><br><strong>答</strong>：看 testing set是不是给你了。在一些比赛里，testing set 它是给你的，那么就可以使用转换学习。但在真正的应用中，一般是没有 testing set 的，这时候就只能做归纳学习。<br><strong>Q：为什么使用半监督学习？</strong><br><strong>答</strong>：缺少 lable 的数据，比如图片，收集图片很容易，但是标注label很困难。半监督学习利用未标注数据做一些事。<br><strong>Q：用沙雕的简单日常语言，讲一讲什么是 半监督学习</strong><br><strong>答</strong>：对人类来说，可能也是一直在做半监督学习，比如小孩子会从父母那边做一些监督学习，看到一条狗，问父亲是什么，父亲说是狗。之后小孩子会看到其他东西，有狗有猫，没有人会告诉他这些动物是什么，需要自己学出来。</p></blockquote><h1 id="Semi-Surpervised在生成模型中如何实现的（EM算法）"><a href="#Semi-Surpervised在生成模型中如何实现的（EM算法）" class="headerlink" title="Semi-Surpervised在生成模型中如何实现的（EM算法）"></a>Semi-Surpervised在生成模型中如何实现的（EM算法）</h1><p><img src="https://img-blog.csdnimg.cn/33bc117e72974fe68bb8635b54369078.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>Q：在生成模型中为什么使用半监督学习？</strong><br><strong>答</strong>：在监督学习中，有一堆用来训练的样本（就是label），你知道它们分别属于类别1，还是类别2。会去估算类别1和类别2的先验概率<em>P(C1),P(C2)</em> ，然后计算类条件概率 <em>P(x|C1),P(x|C2)</em>  。<br>假设 <em>P(x|Ci)</em> 服从一个高斯分布。 假设类别1的数据是从均值为 <em>μ1</em>，协方差为 <em>Σ</em> 的分布中取出来的，而类别2的数据是从均值为 <em>μ2</em> ，协方差也为 <em>Σ</em> 的分布中取出来的（之前讲过共享协方差，效果会好一点）。<br>然后可以计算后验概率 <em>P(C1|x)</em> ，决定一个决策边界在哪里。<img src="https://img-blog.csdnimg.cn/81307014a68d4ae2a427e46e9212d4eb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如果今天有一些未标注数据，如上图绿点，那仍然假设均值和方差是μ1,μ2,Σ显然不合理。<br>如上图左下所示，Σ应该比较接近圆圈（蓝色圆圈），也许在类1采样的时候有问题，所以采样到奇怪的分布（蓝色椭圆）。如上图右下，类2的μ2不应该在橙色椭圆内，而应该在更下面。<br>这样会使先验概率受到影响，本来两个分布，正例数据是一样多，但是加入未标注数据之后，你可能会觉得类2的正例数据更多（先验概率就更大）。总之加入未标注数据后，会影响对均值和协方差的估测，继而影响类条件概率，最后影响了你的决策边界。</p></blockquote><p>通俗的讲，回顾有监督学习中的生成模型，由于data都是有label的，<em>P(Ci)</em> 是已知的，<em>P(x|Ci)</em> 是通过我们基于高斯分布的假设用最大似然估计出来的；现在半监督学习中的生成模型，data的一部分是unlabel的，<em>P(Ci)</em> 是不确定的（隐变量），<em>P(x|Ci)</em>的假设模型也不能套用原来的 <em>u</em> 等参数，这时候需要用<font color="red"><strong>EM算法</strong>(Expectation-Maximization algorithm，又译为期望最大化算法)</font></p><h2 id="EM算法-具体怎么做"><a href="#EM算法-具体怎么做" class="headerlink" title="EM算法 具体怎么做"></a>EM算法 具体怎么做</h2><p>EM算法适用于带有无法观测的隐变量的概率模型估计<br>初始化一组参数，如果是二分类任务，就是初始化类1和类2的先验概率、均值和协方差，可以随机初始化，用已经有标注的数据估测，统称为 <em>θ</em></p><ul><li><strong>第一步（E步）</strong>，用labeled data算出来的高斯模型参数 <em>θ</em> 代入公式去求出每一笔未标注数据（unlabeled data）的后验概率（属于类1 的概率）的 <em>P(C1|Xu)</em>；</li><li><strong>第二步（M步）</strong>，用<strong>极大似然估计</strong>更新 <em>P(Ci)</em> 以及高斯模型参数 <em>θ</em> ，求出 <em>P(x|Ci)</em>，进一步求出新的后验概率 <em>P(Ci|Xu)</em> ，重复这两步直到收敛（似然概率最大）</li><li>至于为什么更新参数是要加入<em>P(Ci|Xu)</em> 这一项，是因为EM算法的思想是把不确定的data用一个概率来表示label，而每一笔不确定的data都有可能来自 类C1 和 类C2，看右下图：<img src="https://img-blog.csdnimg.cn/7035355bb99c4d298af9344cdfb7614c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="**加粗样式**在这里插入图片描述"></li></ul><blockquote><p>Q：EM 算法背后的理论是什么？<br>答：<strong>原来只有标注数据的时候</strong>，目标是最大化一个似然函数，那么给定θ，每一笔训练数据的似然函数值是可以计算的，然后把所有的似然函数值相加，就是总的似然函数值，然后找 <em>θ</em> 最大化。<em>θ</em> 有显式解，求最大值点（导数为0）。<br><strong>现在加入未标注数据后</strong>，我们不知道未标注数据来自哪一个类别，那么未标注数据出现的概率就是和  C1的联合概率+和C2的联合概率（相当于是$\sum<em>{C}P(x^{u},C^{i})$ 。接下来目标就是最大化 $P</em>{\theta }(x^{u})$ ，但是 $P_{\theta }(x^{u})$  的式子是非凸的，所以使用<strong>EM算法求解</strong>。<img src="https://img-blog.csdnimg.cn/5872e4d039e4413295b66b457cec8e23.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h1 id="Semi-Surpervised基于Low-density-Separation（低密度分离）"><a href="#Semi-Surpervised基于Low-density-Separation（低密度分离）" class="headerlink" title="Semi-Surpervised基于Low-density Separation（低密度分离）"></a>Semi-Surpervised基于Low-density Separation（低密度分离）</h1><p><img src="https://img-blog.csdnimg.cn/ea72d9a8bb3448088c224becd5be516f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>低密度分离的假设是，不确定的data的label要不是1，要不是0（“非黑即白”）。低密度的意思是，两个Class的分界处是低密度的（分得比较开的）</strong></p><blockquote><p><strong>Q：这个世界是非黑即白的，什么是非黑即白？</strong><br><strong>答</strong>：假设现在有一大堆的data，有标注数据，有非标注数据，在两个类别之间会有一个明显的鸿沟。给一些标注数据，可以把边界分在上图右边的线，也可以把边界分在上图左边的线。但是考虑非标注数据，那么左边的边界会好一点，在边界处，两个类别的密度是低的（不会出现data）</p></blockquote><h2 id="Self-training-Entropy-based-Regularization-基于熵的正则化"><a href="#Self-training-Entropy-based-Regularization-基于熵的正则化" class="headerlink" title="Self-training + Entropy-based Regularization(基于熵的正则化)"></a>Self-training + Entropy-based Regularization(基于熵的正则化)</h2><h3 id="self-training"><a href="#self-training" class="headerlink" title="self-training"></a>self-training</h3><p><img src="https://img-blog.csdnimg.cn/007143d015a34ef9b00537c7dfc0c066.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>低密度分离最代表性、最简单的方法是self-training，非常直觉。</p><p>我们有一些标注数据，和一些未标注数据。接下来：</p><ul><li>从标注数据训练一个model   f* (用DNN，deep、shallow还是其他机器学习的方法都可以)</li><li>根据f*  标注未标注数据，丢入$x^{u}$ ，得到$y^{u}$  ，${(x^{u} ,y^{u} )}^{R+U}_{u=l}$ 叫做伪标签数据（称为Pseudo-label伪标签）</li><li>接下来，从伪标签数据集移除一些数据加到标注数据集（移除哪些数据需要自己决定，设计一些启发式的规则，或者给权重，有些数据的标签比较确定，那就给大的权重）</li><li>有了更多的标注数据之后，回头再去训练model f*</li></ul><blockquote><p><strong>Q：self-training在回归上有用吗？</strong><br><strong>回归问题用self-training不影响f∗，所以回归问题不能用self-training方法。</strong><img src="https://img-blog.csdnimg.cn/16b0c52a9a89411f80d53e91dc040f16.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>self-training 很像是刚才生成模型里面用的EM算法，唯一的差别是在做 self-training 的时候，用的是<strong>硬标签</strong>，生成模型里用的是<strong>软标签（概率）</strong>。在做 self-training 的时候，会强制分配一个数据属于某一个类别，在生成模型里，使用的是<strong>后验概率</strong>，部分属于类1，部分属于类2。</p></blockquote><h3 id="Entropy-based-Regularization-基于熵的正则化-—-self-training的进阶版"><a href="#Entropy-based-Regularization-基于熵的正则化-—-self-training的进阶版" class="headerlink" title="Entropy-based Regularization(基于熵的正则化) — self-training的进阶版"></a>Entropy-based Regularization(基于熵的正则化) — self-training的进阶版</h3><p><img src="https://img-blog.csdnimg.cn/26e158dbdc714533a4f0173142f2db23.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>熵：一个事件的不确定程度</strong></p><p>Entropy-based Regularization（基于熵的正则化）是self-training的进阶版，self-training里用概率划分类别，可能觉得比较武断，那就可以用Entropy-based的这个方法。</p><p>Entropy-based是说，如果使用神经网络，output是一个分布，我们不去限制output具体属于哪一个类别，而是假设分布很集中（非黑即白的世界）。如上图，假设做5个类别分类的 model：</p><ul><li>第一个 类别1的概率为1，其他类别概率为0，是good</li><li>第二个 类别5的概率为1，其他类别概率为0，是good</li><li>第三个 所有类别的概率很平均，是bad，不符合<strong>低密度分离的假设（非黑即白）</strong></li></ul><blockquote><p><strong>Q：怎么用数值的方法评估分布是集中还是不集中？</strong></p><p><font color="red"><strong>使用熵，分布的熵告诉你集中还是不集中。可以用 每个类别的概率 乘以 log（每个类别的概率），再对类别个数求和取负数。 这个式子来评估。</strong> </font><br>这个其实就是理解成损失函数，因为这边讲究的是非黑即白，所以其实是一致的。损失函数 也可以用分布的距离来描述。<br>我们希望model的output在标注集上正确，在未标注集上的<strong>熵越小越好</strong>。</p><ul><li>第一个分布，熵为0，分布集中 <ul><li>第二个分布，熵也为0，分布集中 </li><li>第三个分布，熵为 ln(5) ，分布比较散</li></ul></li></ul><p>根据这个目标，重新设计损失函数。原来只是希望model在标注集上的output和label距离越近越好，用交叉熵来评估它们之间的距离。</p></blockquote><ul><li><strong>现在在原来的基础上，加上未标注集的output分布的熵。</strong></li><li>然后在未标注集部分乘上一个权重，来表明偏向标注部分还是未标注部分。</li><li>上图右下的损失函数可以算微分，那就使用<strong>梯度下降最小化这个损失函数</strong>，迭代求解参数。加入未标注部分，作用就类似于正则化（在原来损失函数后加一个L1正则或者L2正则），这里则加入一个未标注集熵来防止过拟合，所以称之为基于熵的正则化。</li></ul><h2 id="Semi-supervised-SVM（半监督SVM"><a href="#Semi-supervised-SVM（半监督SVM" class="headerlink" title="Semi-supervised SVM（半监督SVM)"></a>Semi-supervised SVM（半监督SVM)</h2><p><img src="https://img-blog.csdnimg.cn/ddbfa33ce3a4446581d49d9494a497ed.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>Q：SVM 支持向量机（Support Vector Machine）是什么？</strong><br>SVM是找边界，给你两个类别的数据，SVM找一个边界，这个边界一方面要有最大的间隔（让两个class分的越开越好），一方面要有最小的分类错误。<br>如上图，假设现在有一些未标注数据，半监督SVM会穷举所有可能的label。<br>上图中，有四笔未标注数据，每笔数据既可以属于 class1，也可以属于 class2，可能的情况如上图右边所示（还有很多种其他的可能）。然后对每个可能的结果，都去做一个SVM，边界如上图红色线。然后再去找让间隔最大，错误最小的那一种结果。在例子里可能是黑色框这种结果。</p></blockquote><h1 id="Semi-Surpervised基于Smoothness-Assumption（平滑性）假设是如何实现的"><a href="#Semi-Surpervised基于Smoothness-Assumption（平滑性）假设是如何实现的" class="headerlink" title="Semi-Surpervised基于Smoothness Assumption（平滑性）假设是如何实现的"></a>Semi-Surpervised基于Smoothness Assumption（平滑性）假设是如何实现的</h1><h2 id="平滑性假设与高密度区域"><a href="#平滑性假设与高密度区域" class="headerlink" title="平滑性假设与高密度区域"></a>平滑性假设与高密度区域</h2><font color="red">假设：x的分布是不平均的，在某些地方很集中，在某些地方又很分散。如果 x1 和 x2 在一个高密度的区域很相似的话，两者的标签也会很像。</font><blockquote><p><strong>Q：什么叫在高密度区域下呢？</strong><br> 意思是说可以用高密度的路径做连接<img src="https://img-blog.csdnimg.cn/211207011e154abc9dc1609ad946223f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>举个例子，假设数据的分布如上图右边所示，像一个血轮眼。现在有3笔数据：x1,x2,x3，x1和x2中间是一个高密度区域（x1和x2由一个高密度区域连接），有很多数据(中间想成平原地带，地势平坦，人烟很多)，而x2和x3之间数据稀少(中间想成一座山，人烟稀少)，那么走平原会比走山容易，x2走到x1更容易（更相似）。</p></blockquote><p><strong>Q：举一个现实中，相似图形的栗子？为什么会有高密度区域假设？</strong><br>因为在真实情况下，这个假设成立的可能性很高。<br><img src="https://img-blog.csdnimg.cn/a7767f1d4ea04b788d2f9f17bfcec96f.png#pic_center" alt="在这里插入图片描述"><br>我们考虑手写数字识别的例子，有两个2一个3，如果计算像素点相似度的话，可能上图右边的2和3更像。但是从所有数据中看，<strong>左边的2到右边的2中间会有很多连续的形态</strong>。所以根据平滑度假设，左边的2和右边的2更像，因为右边的2和3之间没有过渡的形态。<br><img src="https://img-blog.csdnimg.cn/dfdb44a5a6af4aad901c4185bd21b74c.png#pic_center" alt="在这里插入图片描述"><br>看人脸识别也是一样的，比如左脸像和右脸像差很多，两个人的左脸像计算像素点相似度的话，可能比同一个人的两张侧脸像更高。但是如果收集到足够多的未标注数据，会找到两个侧脸像的很多过渡形态，根据高密度区域假设，这两张侧脸像就是同一个人。</p><p><img src="https://img-blog.csdnimg.cn/12dc2e73270c49b08efeee9c7c8c1d74.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>高密度区域假设，在文件上非常有用，假如现在要区分天文学和旅游的文章。</p><p>天文学的文章会出现asteroid、bright，而旅游的文章会出现yellowstone、zion。如果未标注文章和标注文章的词语有重叠，那可以很容易分类。但真实情况情况是，未标注文章和标注文章可能没有任何词语重叠，因为世界上的词语太多了，一篇文章词汇不会很多，每篇文章的词语是非常稀疏的。<br><img src="https://img-blog.csdnimg.cn/87bec6b6bb8c4603aacffd11ce899920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>但是收集到够多的数据的话，就可以说上图d1、d5像，d5、d6像，传播下去就可以说d1、d3是一类，d2、d4是一类。</p><h2 id="方法一：聚类，而后标注（图像上不太行）"><a href="#方法一：聚类，而后标注（图像上不太行）" class="headerlink" title="方法一：聚类，而后标注（图像上不太行）"></a>方法一：聚类，而后标注（图像上不太行）</h2><p><img src="https://img-blog.csdnimg.cn/45202fb11c71422c8437bdd3c23e91a5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>Q：如何实践平滑度假设？</strong><br>最简单的方法是聚类、然后标记。<br>假如数据分布如上图，橙色是class 1，绿色是class 2，蓝色是未标注数据。接下来做聚类，可能把所有数据分成3个簇。在簇1里，class 1的label最多，那簇1里所有数据标记为class 1，同样的簇2和簇3都标记为class 2。把标记后的数据拿去learn就结束了。</p><p>这个方式不一定有用，因为要求簇正确，这个方法有效的假设是同一个class的东西聚集在一起。但是在图像里要把同一个class的东西聚集在一起没有那么容易。之前深度学习讲过，不同class的图像可能会很像，同一个class可能会不像，只用像素点做聚类，结果八成是不好的。没办法把同一个class的数据聚集在一起，那未标注数据就没有用。</p><p>所以要有用，就要有一个好的方法来描述一张图像，比如用 <em>Deep Autoencoder</em> 抽特征，然后再做聚类。</p></blockquote><h2 id="方法二：基于图的方法"><a href="#方法二：基于图的方法" class="headerlink" title="方法二：基于图的方法"></a>方法二：基于图的方法</h2><p><img src="https://img-blog.csdnimg.cn/95a83cefc106475f915f00fe756c317b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>另一个方法是引入图结构，来表达通过高密度路径进行连接这件事情。</p><p>把现在所有的数据点都建成一个图，每个数据点就是图上的一个点，想办法计算它们之间的奇点，想办法把它们之间的边建出来。</p><p><strong>所谓的高密度路径的意思是说，如果有两个点，在图上是相连的，那它们就是同一个class，如果没有相连，就算距离很近，也走不到。</strong></p><blockquote><p><strong>Q：生活中，如何构建图？</strong><br>有些时候，图的表示可以很自然的得到。</p><ul><li>举例说网页的分类，你有记录网页和网页之间的超链接，那超链接就很自然的告诉你网页是怎么连接的。</li><li>又举例论文的分类，论文和论文之间有引用的关系，这种引用的关系也是另外一种图的边，可以很自然地把这种图画出来。</li></ul></blockquote><h3 id="怎么自己想办法构建图？"><a href="#怎么自己想办法构建图？" class="headerlink" title="怎么自己想办法构建图？*"></a>怎么自己想办法构建图？*</h3><p>其实的图的好坏对结果的影响是非常严重的，但是自己用什么方法做还是很启发的，用自己觉得合适的方式做就可以了。 通常的做法是：</p><p>先定义两个对象之间的相似度，比如图像可以是基于像素点的相似度（可能效果不好），也可以是基于自动编码器抽取出来的特征计算相似度（效果可能好一点）</p><p>定义完相似度后，就可以构建图了（添加边），图有很多种：</p><ul><li>K近邻的图，现在有一大堆数据，可以计算数据与数据之间的相似度，然后设置k例如3，就是3个最相似的点相连</li><li>e-Neighborhood的图，只有相似度超过某个阈值的点才会相连</li></ul><p><strong>所谓的边也不是只有相连和不相连这两种选择，可以给边一些权重，让边跟两个数据点的相似度成正比。</strong>相似度可以用Gaussian Radial Basis Function来定义<img src="https://img-blog.csdnimg.cn/755264db6fe240d798d40d43d90ea95f.png#pic_center" alt=""></p><h3 id="怎么计算这个相似度"><a href="#怎么计算这个相似度" class="headerlink" title="怎么计算这个相似度"></a>怎么计算这个相似度</h3><p>可以先算xi,xj的欧式距离，乘以一个参数取负号，再取e为底的指数函数。取exp很有必要，在经验上最后效果比较好。因为取exp，下降速度很快，只有当xi,xj非常靠近时，奇点才会大，距离远一点奇点就会下降很快变得很小。这样才能制造如上图右下方所示的，两个距离近的橙色点有连接，绿色点和橙色点虽然距离也近，但是使用了exp导致只有很近很近的点才有连接，即使远一点点就不会有连接了，有这样的机制才能避免跨海沟的连接(橙色点和绿色点连接)。</p><blockquote><p><img src="https://img-blog.csdnimg.cn/f5e991680cf44ff08f79229892cd8926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>基于图的方法是，如果现在在图上面有一些标注数据，比如上图左上方，已经知道了蓝色圈的数据属于class 1，那么跟他们有相连的数据点属于class 1的概率也会上升。每一笔数据会去影响它的邻居。</p><ul><li>光会影响邻居还不够，因为有连接说明本来就很像，那很像的input<br> ，output本来也就很像。这种方法真正的精髓是，<strong>class是会传递的</strong>，虽然一个点没有与标注数据直接相连，但是有连接路径，那么class<br> 1就会随着边传递。<ul><li>例如上图右上方，所有数据点构建成一个图（理想的例子），然后有一个蓝色点属于class 1，一个红色点属于class 2。经过基于图的方法，蓝色点会传递，红色点也会传递，如上图右下方所示。</li></ul></li></ul></blockquote><p><strong>要让基于图的这种半监督学习方法有用的话，一个重要的原则是你的数据要多，如果数据不够多，例如下图所示，中间有断开，那信息就传递不过去。</strong><img src="https://img-blog.csdnimg.cn/9ce410bae05d47f4b6a06931ef349d7a.png#pic_center" alt=""></p><h2 id="考试一般的考题，定量的计算图"><a href="#考试一般的考题，定量的计算图" class="headerlink" title="考试一般的考题，定量的计算图"></a>考试一般的考题，定量的计算图</h2><p>定量的使用方式是在这个图的结构上面定一个东西，叫做label的平滑度，来表明这个label有多符合平滑度假设。</p><h3 id="怎么定平滑度？"><a href="#怎么定平滑度？" class="headerlink" title="怎么定平滑度？"></a>怎么定平滑度？</h3><p><img src="https://img-blog.csdnimg.cn/42dc067087334546bbf3a44bdb7f9d88.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>看上图两个例子，这两个例子都有4个数据点，数据点之间连接的数字代表了边的权重。现在给两个例子的数据不同的label，左边例子的label是1,1,1,0，右边例子的label是0,1,1,0，那谁更平滑呢？</p><p>直观感觉就是左边例子更平滑，但我们需要定量描述。常见的方法是，考虑两两相连的点（不管有label还是没有label），在所有的配对上，计算label差值的平方，然后乘上权重 ，最后求和。</p><p>所以左边这个例子的S就是0.5，右边例子的S是3，S越小越平滑。</p><h3 id="用矩阵来表达"><a href="#用矩阵来表达" class="headerlink" title="用矩阵来表达"></a>用矩阵来表达</h3><p><img src="https://img-blog.csdnimg.cn/3993d1ab2b6b4e79baeee94790c115f3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>S可以稍微整理下，写成向量形式如上图。</strong></p><p>把y串成一个向量，y包括标注数据和未标注数据，所以有 R+U 维。</p><p><strong>L是<em>(R+U)×(R+U)</em>的矩阵，叫做图拉普拉斯，L的定义是 D−W ，W是两两数据点之间的权重，D是W每行值之和（放在对角线）。</strong></p><h3 id="再加一个正则化项"><a href="#再加一个正则化项" class="headerlink" title="再加一个正则化项"></a>再加一个正则化项</h3><p>现在可以用 y转置Ly 来评估现在得到的label有多平滑，<strong>式子里面的y是label的值，取决于神经网络的参数</strong>。那么如果要把平滑度考虑到神经网络里时，就是在原来的损失函数里加上λS（λ是某一个想要调的参数）。λS像一个正则化项，在调整参数时，不只是让标注数据的output跟真正的label越近越好，同时还要让output的label在标注数据和未标注数据上符合平滑度假设。平滑度假设由S衡量。</p><p>不一定要在output上计算平滑度，在深度神经网络里，可以把平滑度计算放在网络的任何地方。你可以假设你的output是平滑度，也可以把某个隐藏层乘上一些别的transform，它也要平滑，也可以要求每个隐藏层的output都是平滑的。</p><h3 id="转换的想法-Better-Representation"><a href="#转换的想法-Better-Representation" class="headerlink" title="转换的想法 Better Representation"></a>转换的想法 Better Representation</h3><p>我们观察到的世界其实是比较复杂的，在背后有一些比较简单的向量，比较简单的东西在操控这个复杂的世界。那只要看透假象，直指核心，就可以让学习变得比较容易。</p><p>例如上图右方剪胡子，胡子的变化是很复杂的，但是胡子受头操控，头的变化是有限的。所以胡子是观测，而头就是Better Representation。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;半监督学习 Semi-Supervised&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;总结 半监督学习 的要点：&lt;br&gt;Q1：什么是Semi-Supervised？&lt;br&gt;Q2：Semi-Surpervised在生成模型中如何实现的（EM</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="半监督学习" scheme="http://example.com/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络CNN（Convolutional Neural Network）</title>
    <link href="http://example.com/2021/08/12/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%EF%BC%88Convolutional%20Neural%20Network%EF%BC%89/"/>
    <id>http://example.com/2021/08/12/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%EF%BC%88Convolutional%20Neural%20Network%EF%BC%89/</id>
    <published>2021-08-12T14:36:01.000Z</published>
    <updated>2021-08-19T07:03:47.166Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="卷积神经网络CNN（Convolutional Neural Network）">TOC</a></p><blockquote><p>总结CNN的要点：<br>Q1：什么是CNN？为什么要用CNN？为什么图像处理一般都采用CNN？<br>Q2：实现CNN的步骤？</p><p><font color="red">input——&gt;convolution——&gt;max pooling——&gt;…——&gt;flatten——&gt;fully connected network——&gt;output </font><br>Q3：如何用 <em>keras</em> 搭建一个CNN?<br>Q4：CNN的应用?什么时候适用CNN效果最好？<br>满足三个图像三个特性的时候。但要考虑第三点取子样是否合理。</p></blockquote><h1 id="CNN的概述"><a href="#CNN的概述" class="headerlink" title="CNN的概述"></a>CNN的概述</h1><h2 id="什么是CNN？"><a href="#什么是CNN？" class="headerlink" title="什么是CNN？"></a>什么是CNN？</h2><p>CNN也叫convnet，中文名称为卷积神经网络，是计算机视觉领域常用的一种深度学习模型。</p><h2 id="为什么要用CNN？"><a href="#为什么要用CNN？" class="headerlink" title="为什么要用CNN？"></a>为什么要用CNN？</h2><p>其可以简化DNN模型，可以减少不必要的神经元节点。特别是用在图像处理上。</p><h3 id="为什么图像处理一般都采用CNN？"><a href="#为什么图像处理一般都采用CNN？" class="headerlink" title="为什么图像处理一般都采用CNN？"></a>为什么图像处理一般都采用CNN？</h3><font color="green">**CNN的参数比全连接神经网络少得多**，为什么CNN只用较少的参数就可以用于处理图像呢？</font><p>这是因为图像具有以下三点特征：<br>1、一些模式比整张图片小得多，例如“鸟喙”就比整张图片小得多；<img src="https://img-blog.csdnimg.cn/1e9efb0804a64876a33e2794d96039a0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>2、同样的模式可能出现在图像的不同区域，例如“鸟喙”可能出现在图片的左上方也可能出现在图像的中间；<img src="https://img-blog.csdnimg.cn/c4a2de1adb4d4573865ec9f20b9e4a96.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>3、对图像的降采样不会改变图像中的物体。<img src="https://img-blog.csdnimg.cn/3e746d5fef0140029fe62095452d8389.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br><strong>CNN的卷积层的设计对应着前两点，池化层的设计对应着第三点。</strong> 如下图所示：<br><img src="https://img-blog.csdnimg.cn/81dc5d9f161c47b4b7054b5964eda897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""></p><h1 id="实现CNN的步骤"><a href="#实现CNN的步骤" class="headerlink" title="实现CNN的步骤"></a>实现CNN的步骤</h1><p>input——&gt;convolution——&gt;max pooling——&gt;…——&gt;flatten（压平）——&gt;fully connected network——&gt;output</p><h2 id="卷积层（减少训练参数）"><a href="#卷积层（减少训练参数）" class="headerlink" title="卷积层（减少训练参数）"></a>卷积层（减少训练参数）</h2><p><img src="https://img-blog.csdnimg.cn/81dc5d9f161c47b4b7054b5964eda897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="CNN流程图"></p><ul><li><p>对应Property1：每一个Filter（待训练的参数，即卷积核）代表一个<strong>局部特征探测器</strong>（他是一个可以特点图形的表示器），假设现在两个特征探测器（<em>Filter1</em> 和 <em>Filter2</em>）</p><ul><li><p>卷积核1的结果 值最大的那个点所在的图片部分，就是我们要找的内容<br><img src="https://img-blog.csdnimg.cn/afe8e2823a47491bba71d054aa0ade85.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></li><li><p>卷积核2的结果。与卷积核1的结构共同组成了 feature map<img src="https://img-blog.csdnimg.cn/c3841af956b24b6d8721af30d207cb4c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></li></ul></li></ul><ul><li>对应Property2：用 <em>Filter1</em> 就能探测出在不同位置的同一个flatten，而不需要用不同的Filter</li></ul><p>如果图片是彩色的，也就是说它是三通道的，还没卷积之前，可以说有3个通道（RGB），意味着每一个像素由3个数值表示。如下图所示：<br>卷积核是立方体3×3×3，图片为9×9×3。图片中同样选择卷积核大小，与卷积核累和，但要注意我们并不是把RGB三层，分开算，应该算合为一体的。<img src="https://img-blog.csdnimg.cn/991470ca8069474fabd2088db993f9eb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="RGB卷积"></p><h3 id="与全连接方式的对比"><a href="#与全连接方式的对比" class="headerlink" title="与全连接方式的对比"></a>与全连接方式的对比</h3><p>全连接层，图像处理的神经网络图如下所示，每个输入与各神经元都有链接（有固定的权值）<img src="https://img-blog.csdnimg.cn/b34b02cb98274e53a6600a5c685d3674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="全连接层"><br>下面是CNN神经网络图，其主要的目的是减少参数（减少权值数量）。并且可以存在一定的共享权值，下面颜色相同的权值边值应该相同，这样调参可以更快。</p><p><img src="https://img-blog.csdnimg.cn/250d06b21d4546dea59a0a6502d3a3c0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="CNN"></p><h2 id="池化层（Maxpooling）"><a href="#池化层（Maxpooling）" class="headerlink" title="池化层（Maxpooling）"></a>池化层（Maxpooling）</h2><p>目的是减少每一个特征的维度，也就是减少后面flatten的输入特征数量。Maxpooling这边我们取每个框内的最大点。<img src="https://img-blog.csdnimg.cn/f52e46c700fe492ab79a470370e223ff.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="Maxpooling的规程"><br>在整理一下，变成一个新的feature map</p><p>合计上两步得到：<br><img src="https://img-blog.csdnimg.cn/8065213d0fff4e30b41d596b0cde4d65.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>做一次卷积+池化后，把原来的 <em>6 × 6</em> 图像变成了 <em>2 × 2</em> 图像，<em>2 × 2</em>图像的深度（每个像素点用多少个值表示）<strong>取决于有多少个过滤器，如果有 <em>50</em> 个过滤器，<em>2 × 2</em> 图像就有 <em>50</em> 维</strong>，上图是只用两个过滤器，那就是 <em>2</em> 维。<br>所以上图右边，就是一个新的比较小的图像，每个过滤器代表了一个channel(通道)。</p><h2 id="重复上两步操作"><a href="#重复上两步操作" class="headerlink" title="重复上两步操作"></a>重复上两步操作</h2><p>可多次重复上述两个步骤，将输出的结果变得最小化。</p><blockquote><p><strong>Q: 假设我们第一次卷积的时候，我们有25个卷积核，那输出的结果 feature map中应该有25个矩阵。那请问，第二次卷积的时候，输出的feature map 应该是 25×25=625 个矩阵么？</strong><br>答：不对，做完第一次卷积得到25个矩阵，做完第二次后还是25个矩阵。例如输入是三个通道 <em>(RGB)</em> 的 <em>6 × 6</em> 矩阵数据（即一个立方体，6 × 6  × 3），有两个过滤器（也是立方体，三个通道，<em>3 × 3 × 3</em>），则输出为<em>4  × 4 × 2</em>。（<em>4 = 6-2 ； 2</em>是过滤器个数  <strong>过滤器决定通道数</strong>）<img src="https://img-blog.csdnimg.cn/a4aeaa3dd23d4ff48bce2e89c97553c3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><p><img src="https://img-blog.csdnimg.cn/e634fad522fe498a9b5cb04557fe7a05.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""></p><h2 id="Flatten（压平）"><a href="#Flatten（压平）" class="headerlink" title="Flatten（压平）"></a>Flatten（压平）</h2><p><img src="https://img-blog.csdnimg.cn/699741afda84434985ef450ad1cffeea.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="Flatten"><br>flatten(压平)的意思是，把特征图拉直，然后丢到一个全连接神经网络里。</p><h1 id="CNN-in-Keras"><a href="#CNN-in-Keras" class="headerlink" title="CNN in Keras"></a>CNN in Keras</h1><ul><li>卷积前，一个pixel用多少个数值表示，取决于通道数</li><li>卷积后，一个pixel用多少个数值表示，取决于Filter个数，而通道数决定Filter的高</li><li>上一个卷积层有多少个Filter，下一层卷积input就有多少个通道<br><img src="https://img-blog.csdnimg.cn/75db9d57e892418d9c68b6d52c714c4b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul><p><img src="https://img-blog.csdnimg.cn/686fdcfd5a734bfa998f6d84def07fea.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(<span class="number">25</span>,<span class="number">3</span>,<span class="number">3</span>),input_shape=(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">model.add(MaxPooling2D((<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">model.add(Conv2D(<span class="number">50</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">model.add(MaxPooling2D((<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">model.add(Dense(output_dim=<span class="number">100</span>))</span><br><span class="line">model.add(Activation(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(output_dim=<span class="number">10</span>))</span><br><span class="line">model.add(Activation(<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure><blockquote><p><strong>Q：为什么上图第二次 每个卷积核的 参数是 225？</strong><br>答：因为是 3×3×25  那为什么是25而不是50呢？</p></blockquote><h1 id="CNN在学什么？"><a href="#CNN在学什么？" class="headerlink" title="CNN在学什么？"></a>CNN在学什么？</h1><h2 id="CNN卷积和池化部分在做什么？"><a href="#CNN卷积和池化部分在做什么？" class="headerlink" title="CNN卷积和池化部分在做什么？"></a>CNN卷积和池化部分在做什么？</h2><p><img src="https://img-blog.csdnimg.cn/919125d4f1964cb9b520a639ed94e2e6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ul><li>分析第一个层的卷积核是比较容易的，里面每个卷积核就是一个3 <em> 3 的矩阵，对应3 </em> 3 范围内的9个像素点，<strong>只要看到矩阵的值，就知道在检测什么</strong>（有明显的特征）。</li><li>第二层的卷积核没法知道在做什么，虽然也是 <em>3 × 3</em> 的矩阵，总共 50 个。<strong>但是这些卷积核的输入不是像素点，而是在上一层做完卷积和池化后的输出</strong>。就算知道第二层卷积核的矩阵值，也不知道在检测什么。<strong>另外第二层卷积核考虑的不是原图 <em>3 × 3</em> 的像素点，而是比原图 <em>3 × 3</em> 像素点更大的范围，因为在第一层的池化后，压缩了原图 <em>3 × 3</em> 的区域，第二层卷积核是在压缩后的图像里再选取 <em>3 × 3</em>  像素点，相当于扩大了原图检测的范围。</strong></li></ul><blockquote><font color="red">Q：那怎么分析第二层卷积核在做什么？</font><p>第二层的50个卷积核，每个卷积核的输出是一个 <em>11 × 11</em> 的矩阵。把第k个卷积核输出拿出来如上图左下，矩阵元素表示为  $a_{ij}^{k}$  (第k个卷积核，第i个行，第j个列)。接下来 定义一个“<em>Degree of the activation of the k-th  filter</em>”（第k个卷积核的激活程度），值代表第k个卷积核的被激活程度（<strong>input和第k个卷积核侦测的东西有多匹配</strong>）。</p><p>第k个卷积核被激活程度表示为：$a^{k}=\sum<em>{i=1}^{11}\sum</em>{j=1}^{11}a_{ij}^{k}$ ，<em>11 × 11</em> 矩阵所有元素值之和。</p><p><strong>Q：找一张图像，可以让第k个卷积核被激活程度最大，如果做到这件事情？</strong><br><strong>称 <em>input</em> 的图像为 <em>x</em>，目标是找一个让 $a^{k}$ 最大的 <em>x</em>，如何找到这个 <em>x</em>？</strong></p><p><font color="red">使用梯度上升</font>，因为我们的目标是最大化 $a^{k}$  。现在是把 <em>x</em> 当做我们要找的参数，对 <em>x</em> 用梯度上升。原来CNN的 <em>input</em> 是固定的，<em>model</em> 的参数使用梯度下降求解。现在反过来，<em>model</em> 的参数是固定的，使用个梯度上升更新 <em>x</em>，让被激活程度最大。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/b5522ea6a04d49bfa39a3f94db140610.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">上图左下，是随便取12个卷积核后对 <em>x</em> 做梯度上升后的结果，每个卷积核都找到一张图像，这张图像让这个卷积核的被激活程度最高。如果有50个卷积核，理论上可以找50张图像。</p><blockquote><p><strong>Q：这12张图像有一个共同的特征：是某种纹路在图上不断反复。为什么会这样？</strong><br>看第三张图像，都是小小的斜条纹，这意味着第三个卷积核是在检测是否有斜的条纹。因为卷积核考虑的范围是很小的，所以原图像上任何地方出现一个小小的斜纹的话，这个卷积核（过滤器）就会被激活，输出值就会很大。如果原图像所有范围都是这种小小的条纹，那这个卷积核的被激活程度就最大。</p></blockquote><p>你会发现每个过滤器都是在检测某一种图案（某一种线条），例如上图左下第3个过滤器是检测斜条纹，第4个是检测短、直的线条，第6个是检测斜成一定程度的线条等等。</p><font color="red">每个卷积核（过滤器）都在检测不同角度的线条。</font><h2 id="全连接的隐藏层都在干什么？"><a href="#全连接的隐藏层都在干什么？" class="headerlink" title="全连接的隐藏层都在干什么？"></a>全连接的隐藏层都在干什么？</h2><p><img src="https://img-blog.csdnimg.cn/cc01d0cb741b4bab93047f401e34b40f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>做完卷积和池化后，会做flatten(压平)，把压平后的结果丢到神经网络里去。</p><blockquote><p><strong>Q：在这个神经网络的隐藏层里，每个神经元都在干什么？</strong><br>答：如法炮制之前的做法，定义第 <em>j</em> 个神经元的输出是 $a<em>{j}$ ，然后找一张图像 <em>x</em>，使 $a</em>{j}$ 最大。<br>找到的图像如上图左下所示，9张图像，是对应神经元的输出最大。你会发现跟刚才卷积核（过滤器）观察的图案很<strong>不一样</strong>，<font color="red"><strong>卷积核观察的是类似纹路的东西，因为卷积核只考虑了原图像的一部分区域</strong></font>。<br><strong>输出通过压平后，现在每个神经元是去看整张图像</strong>，能使神经元激活程度最高的图像不再是纹路这种小图案，而是一个完整的图形，虽然看起来完全不像是数字，但神经元被激活后也的确在侦测一个完整的数字。</p></blockquote><h2 id="考虑最后的输出？"><a href="#考虑最后的输出？" class="headerlink" title="考虑最后的输出？"></a>考虑最后的输出？</h2><p><img src="https://img-blog.csdnimg.cn/c02bf451565e4f01bacf11dc6922219c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>如果最后的输出是10维的，每一维对应一个数字。把某一维拿出来，找一张图像使那个维度的输出最大。例如现在要找一张图像，使输出层上对应数字1的神经元的输出最大，理论上这张图像看起来就是数字1<br>但是实际的图像如上图左边所示，每张图像分别代表0,1,2,3,4,5,6,7,8</p><blockquote><p><strong>Q：那为什么是这种是乱七八糟的雪花状呢，而不是我们能看清的数字呢？</strong><br>答：因为神经网络的视角，他就是和人不一样的。他就认为这些雪花图像是不一样的，对于0-8数字。与我们人的思维不同。<br><img src="https://img-blog.csdnimg.cn/f84a6c6c9e4c468a805407e8bfee004c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>Q：能不能让这些图像看起来更像数字？</strong><br>我们知道，一张图像是不是一个数字，有一些基本的假设。比如上图左边，人类看起来显示不是数字。那么我们对x做一些<strong>正则项约束</strong>，告诉机器，虽然有些 <em>x</em>（图像） 可以让  <em>y</em>  很大，但是这些 <em>x</em> 的确不是数字。<br><strong>Q：那加些什么约束呢？</strong><br>比如最简单的想法，图像上的白点是有墨水（笔画）的地方，对一个数字来说，有白点的部分是有限的，数字的笔画只占图的一小部分，所以我们要对 <em>x</em> 做一些限制。<br>假设 $x<em>{ij}$ 是图像像素点的值，每张图像有 <em>28 × 28</em> 个像素点。<strong>把所有像素点的值取绝对值并求和（相当于L1正则）</strong>，我们希望找一个 <em>x</em> ，让 $y</em>{i}$ 越大的同时，也让像素点绝对值之和越小。那我们找出来的图像大部分的地方就不是白色的。<br>最后得到的结果如上图右边所示，和左边的图看起来，已经可以隐约看出来是个数字了。</p></blockquote><h1 id="CNN的应用"><a href="#CNN的应用" class="headerlink" title="CNN的应用"></a>CNN的应用</h1><h2 id="Deep-Dream"><a href="#Deep-Dream" class="headerlink" title="Deep Dream"></a>Deep Dream</h2><p>你给机器一张图像，<strong>机器会在这张图像里面，加上它学习到的东西</strong>。<br><img src="https://img-blog.csdnimg.cn/b3c9858f5d9849d8995dd1183150f92e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>比如把上图丢到CNN里面去，然后把某个卷积核或者某个全连接隐藏层拿出来（一个向量），假设是 $\begin{bmatrix}<br>3.9\<br>-1.5\<br>2.3\<br>:\<br>\end{bmatrix}$</p><p>然后把3.9、2.3调大（本来是正的值调大），-1.5调小（负的值调小），正的更正，负的更负。找一个图像使卷积核或者隐藏层（拿出来的）的输出是调整后的向量。<strong>这么做的意思是让CNN夸大化它看到的东西。</strong><br><img src="https://img-blog.csdnimg.cn/0884c7de1c4e44b2ae5e302590b5ee18.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>找到的图像会变成上图所示，出现很多奇怪的东西。右边看起来是一头熊，原来是一颗石头。对机器来说，本来就觉得石头像一头熊，强化它的认知后，就学习出来更像一头熊的图案。这个就是Deep Dream。</p><h2 id="Deep-Style"><a href="#Deep-Style" class="headerlink" title="Deep Style"></a>Deep Style</h2><p><em>input</em> 一张图像，然后让机器去修改这张图像，让它有另一张图的风格，比如让上图看起来是呐喊。<br><img src="https://img-blog.csdnimg.cn/153f1e11b39943d8b64e7f352c289410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><img src="https://img-blog.csdnimg.cn/952a46ca31584df885c1722e5f79912d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>Q：卷积核和过滤器的区别</strong></p><ul><li><p>卷积核就是由<strong>长和宽</strong>来指定的，是一个<strong>二维</strong>的概念。</p><ul><li>过滤器是是由<strong>长、宽和深度</strong>指定的，是一个<strong>三维</strong>的概念。</li><li>过滤器可以看做是卷积核的集合。</li><li>过滤器比卷积核高一个维度——深度。</li></ul><p>——————————————————————————————————</p></li></ul><p><strong>Q：怎么做到图像风格转变呢？</strong><br><strong>把原来的图像丢给CNN，得到CNN过滤器的输出，代表一张图像里有什么样的内容</strong>。 然后把呐喊这张图也丢到CNN里，也得到过滤器的输出，但这时候考虑的不是过滤器输出的绝对值，而是考虑<strong>过滤器和过滤器输出之间的关系</strong>，<strong>这个关系代表了一张图像的风格</strong>。接下来用同一个CNN找一张图像，这张图像的内容像原图像的内容（过滤器的输出类似），同时这张图像的风格像呐喊的风格（过滤器输出之间的关系类似）。<br><strong>找一张图片同时<font color="red">最大化</font>内容和风格（使用<font color="red">梯度上升更新参数</font>），得到的结果就像两张图片结合一样。</strong></p></blockquote><h2 id="CNN应用在围棋上"><a href="#CNN应用在围棋上" class="headerlink" title="CNN应用在围棋上"></a>CNN应用在围棋上</h2><p><img src="https://img-blog.csdnimg.cn/d2233ab8dcdf4ca8ad1e4dc4ebf550b6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>要让机器下围棋，不一定要用CNN，一般的神经网络也可以做这件事情。只要学习一个网络，也就是找一个函数，输入是棋盘，输出是棋盘上的位置，根据棋盘的盘势，判断下一步落子的位置。<br>输入是<em>19 ×19</em> 向量，向量每一维是棋盘上的一个位置（是黑子则值为1，是白子则值为-1，反之则为0），丢到一个全连接的神经网络，输出也是<em>19 ×19</em> 的向量（每一维对应棋盘一个位置），那这样机器就可以学会下围棋了。</p><h3 id="为什么CNN可以用在下围棋上？"><a href="#为什么CNN可以用在下围棋上？" class="headerlink" title="为什么CNN可以用在下围棋上？"></a>为什么CNN可以用在下围棋上？</h3><p>但实际采用CNN会得到更好的效果！为什么呢？<br>之前举的例子都是把CNN用在图像上面，<em>input</em> 是一个矩阵。用到下棋上，只要把 <em>19 ×19</em>  的向量表示为 <em>19 ×19</em>  的矩阵。对CNN来说，<strong>就是把棋盘和棋子当成一个图像，然后输出下一步落子的位置</strong>。</p><blockquote><p><img src="https://img-blog.csdnimg.cn/604669df79b5475aa23db1f14d14b3e5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>收集很多棋谱，告诉CNN，看到落子在5之五，输出天元的位置为1，其他位置为0<br>看到5之五和天元都有棋子，输出就是5之五的位置为1，其他位置为0<br>这个是监督的部分，AlphaGo还有强化学习的部分</p></blockquote><h2 id="总结一下什么时候用CNN-为什么围棋适用？"><a href="#总结一下什么时候用CNN-为什么围棋适用？" class="headerlink" title="总结一下什么时候用CNN?为什么围棋适用？"></a>总结一下什么时候用CNN?为什么围棋适用？</h2><p><strong>图像要有该有的特性，开头讲过的根据<font color="red">三个特性</font>设计出了CNN的网络结构，在处理图像的时候特别有效。</strong></p><blockquote><p><strong>Q：为什么围棋很适用CNN？</strong><img src="https://img-blog.csdnimg.cn/36ddc9027bbe4537b72ff41a6553c1dd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>答：因为围棋有一些特性和图像处理是很相似的。</p><p><font color="red">围棋是有图像的第一个和第二个特性</font></p><ul><li><p><strong>在一张图像上面，有一些图案是比整张图像小的，比如鸟嘴。在围棋也有同样的现象</strong>，比如看到一些棋子摆放的图案，就要做一些相应的事情（比如上图黑子叫吃的时候，白子要落在下方保证不被吃）。不需要看整个棋盘，只需要看一个小小的范围，就可以侦测白子是不是属于被叫吃的状态。<br>AlphaGo里第一层的过滤器就是用的 5 × 5 过滤器，显然设计这个过滤器的人觉得围棋上最基本的图案在 5 × 5 范围内就可以被侦测出来。</p></li><li><p>图<strong>像还有个特性是相同的图案会出现在不同的区域，在围棋上也有同样的特征。</strong>例如叫吃的图案，可以出现在棋盘左上角，也可以出现在棋盘右下角，图案代表了同样的意义（叫吃），所以可以用同一个检测器来处理这些在不同位置的图案。</p></li></ul><p>Q：困惑的是图像的第三个特性，对原图像做子采样不会影响人看到的这张图像的样子，基于第三个特性有了池化层，但Alpha<br>并没有采用池化层（就是做子采样）？<br><strong>因为不能做子采样。比如丢弃棋盘的奇数行和偶数列，想想也应该是不可以的。</strong></p><p>也许AlphaGo里的CNN架构有特殊的地方。AlphaGo论文附录里描述了它的网络结构，input是一个<em>19 ×19 ×48</em> 的图像，<em>19<br>×19</em> 是棋盘可以理解，<strong>但48是怎么来的？</strong><br>对AlphaGo来说，把每一个位置都用48个值来描述（卷积后有48个通道）。本来我们只要描述一个位置是不是白子、黑子就可以了，而AlphaGo加上了领域知识（看这个位置是不是出于叫吃的状态等等）。</p><p>AlphaGo有做zero padding(零填充)，在原来<em>19 ×19</em> 的图像外围补上 <em>0</em> 值变成 <em>23 × 23</em> 的图像，第一层用的是 <em>5 × 5</em> 过滤器，总共 <em>k</em> 个过滤器（paper里用的是192个过滤器），步长<em>stride=1</em>，有用到 <em>ReLu</em> 作为激活函数，有2到12层的过滤器层，最后变成 <em>21 × 21</em> 的图像，接下来再使用 <em>3 × 3</em> 的过滤器，步长 <em>stride=1</em>。最后发现AlphaGo没有使用池化，针对围棋特性设计CNN结构的时候，是不需要池化这个结构的。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;卷积神经网络CNN（Convolutional Neural Network）&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;总结CNN的要点：&lt;br&gt;Q1：什么是CNN？为什么要用CNN？为什么图像处理一般都采用CNN？&lt;br&gt;Q2：实现C</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="CNN" scheme="http://example.com/tags/CNN/"/>
    
    <category term="卷积核" scheme="http://example.com/tags/%E5%8D%B7%E7%A7%AF%E6%A0%B8/"/>
    
    <category term="过滤器" scheme="http://example.com/tags/%E8%BF%87%E6%BB%A4%E5%99%A8/"/>
    
    <category term="Maxpooling" scheme="http://example.com/tags/Maxpooling/"/>
    
  </entry>
  
  <entry>
    <title>为什么要Deep？深而不是宽</title>
    <link href="http://example.com/2021/08/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81Deep%EF%BC%9F%E6%B7%B1%E8%80%8C%E4%B8%8D%E6%98%AF%E5%AE%BD/"/>
    <id>http://example.com/2021/08/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81Deep%EF%BC%9F%E6%B7%B1%E8%80%8C%E4%B8%8D%E6%98%AF%E5%AE%BD/</id>
    <published>2021-08-10T14:36:01.000Z</published>
    <updated>2021-08-10T17:02:00.593Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="为什么要Deep？深而不是宽">TOC</a></p><blockquote><p><strong>Q：为什么要用要用深度？而不是广度？</strong><br>答：1.因为深度可以用少量的数据，就完成对数据的分类。<br>2.深度，每个层次都是基于上个层次得到的（其实就是学习的过程），我们可以将神经元的数量减少。如果层次很少的话，会导致神经元可能非常多。可以类比逻辑电路。<img src="https://img-blog.csdnimg.cn/3889abe5617e426d91f92e842ba3af83.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center=300×300" alt="在这里插入图片描述"></p><h1 id="模组化"><a href="#模组化" class="headerlink" title="模组化"></a>模组化</h1><p>在比较浅层网络与深层网络时，要让“矮胖”的网络和“高瘦”的网络的参数数目相等，这样比较才公平。<br><strong>但即便是在深层网络参数较少的情况下，深层网络也会比浅层网络表现好。</strong><br>这是因为<font color="red"> 深层”其实相当于“模组化”</font>，第一个隐层是最基本的分类器，第二个隐层是用第一个隐层建造的分类器，以此类推。</p><p><strong>举个栗子，为什么说深度好！</strong><br>左边第一幅图可以看到，我们需要分四个类，包括长发女，长发男，短发女，短发男。一共四类，其中长发男的数据样本很少，那区分这个类的能力就非常的弱。<br>这个时候，我们就可以先分为两个神经元，一个区分男女，一个区分长发短发，这样中间加一层，可以使得数据样本少的类 鉴定的效果更好。<br><img src="https://img-blog.csdnimg.cn/dc3ef77e42724c1ebbcb6d0d29067206.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>对于分类一个图像来说，深度使得模块化。</strong><img src="https://img-blog.csdnimg.cn/81cf2b79b06440e789ad2a03937c7e2e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center=300×300" alt="在这里插入图片描述"></p></blockquote><h1 id="类比逻辑电路"><a href="#类比逻辑电路" class="headerlink" title="类比逻辑电路"></a>类比逻辑电路</h1><p><img src="https://img-blog.csdnimg.cn/8635e890e73f4834900ea683446b9f0a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center=300×300" alt="在这里插入图片描述"><br>浅层网络确实可以表示任意函数，但是使用深层结构更有效率。<br><strong>好比逻辑门电路，用两层逻辑门就可以实现任何布尔函数，但是用多层结构更简单、需要的逻辑门更少。</strong><br><img src="https://img-blog.csdnimg.cn/3889abe5617e426d91f92e842ba3af83.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center=300×300" alt="在这里插入图片描述"></p><p>神经网络也是如此，单隐层网络可以表示任何连续函数，但是多层结构表示起来更简单、需要的神经元更少，<strong>所以比较不容易overfitting，或只需较少的data。</strong>而且，深层结构可以比较有效率地使用data。</p><h1 id="类比图形"><a href="#类比图形" class="headerlink" title="类比图形"></a>类比图形</h1><p><img src="https://img-blog.csdnimg.cn/e4e96dfdc69b4c47b96640efa2f70be8.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center=300×300" alt="在这里插入图片描述"><br>1层hidden layer与3层hidden layer（相同数目的参数），3层的效果更好。<br>但理论上，3层可达到的效果，1层也能达到：要在1层learn的时候，target从真实label改为3层的output，这样1层的结果会接近3层的结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;为什么要Deep？深而不是宽&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Q：为什么要用要用深度？而不是广度？&lt;/strong&gt;&lt;br&gt;答：1.因为深度可以用少量的数据，就完成对数据的分类。&lt;br&gt;2.深度，每个层次都是基于</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 Deep Learning 模型优化</title>
    <link href="http://example.com/2021/08/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"/>
    <id>http://example.com/2021/08/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/</id>
    <published>2021-08-09T14:36:01.000Z</published>
    <updated>2021-08-09T14:37:14.552Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="深度学习 Deep Learning 后续优化">TOC</a></p><p><strong>深度学习  怎么评价效果与改进？</strong></p><ol><li>先检查 训练阶段 是否有比较好的结果<br>training优化方法：　<ol><li>换激活函数（Sigmoid、ReLU、Maxout、Tanh、Softmax）</li><li>优化器——优化梯度下降和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）</li></ol></li><li>training没问题了，再检查testing是否有比较好的结果<br>　testing（过拟合）优化：<ol><li>参数在过拟合之前就停止更新</li><li>正则化Regularization</li><li>dropout</li></ol></li></ol><h1 id="如何优化模型"><a href="#如何优化模型" class="headerlink" title="如何优化模型"></a>如何优化模型</h1><h2 id="谈什么才是overfitting？"><a href="#谈什么才是overfitting？" class="headerlink" title="谈什么才是overfitting？"></a>谈什么才是overfitting？</h2><p>首先我们在明确一下，深度学习的流程，其与机器学习基本一致。如下图所示：<img src="https://img-blog.csdnimg.cn/0f42918289e14d23a7c8eada3056de07.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>判断  <em>过拟合</em> 需要看两个数据集上的结果（training set → good， testing set → bad）。<br><strong>在试图解决overfitting之后仍要再看一下training set上的结果！</strong></p><blockquote><p><strong>误区：</strong><br>不能看见所有不好的 <em>performance</em> 都归因到 <em>overfitting</em>。如只看下右图，不能断言56-layer有 <em>overfitting</em>，要看模型在training set上的表现。根据下左图，可以发现原因是训练的时候没有训练好，即这个层次设计可能本身就是有问题的，不然为啥20层的挺好，56层没道理差啊（这不能叫 <em>underfitting</em>，<em>underfitting</em>：参数不够多，模型能力不足）。<img src="https://img-blog.csdnimg.cn/d9f2b9e2e027466286e85d6b3998b713.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="对症下药：训练error，测试error分别用什么方法"><a href="#对症下药：训练error，测试error分别用什么方法" class="headerlink" title="对症下药：训练error，测试error分别用什么方法"></a>对症下药：训练error，测试error分别用什么方法</h2><p>在读到深度学习的方法时，要思考该方法是解决什么问题。<br><strong>是解决training set上的performance不好，还是解决testing set上的performance不好。</strong>比如，Dropout是为了解决testing set上结果不好的问题，如果是training set上结果不好而用Dropout，不会有好的结果。<br>下图是 分别解决各问题，可以采用的方法：<img src="https://img-blog.csdnimg.cn/f74d3b5a294e40648e9f9646b7c8e063.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="模型Train阶段Error-具体解决"><a href="#模型Train阶段Error-具体解决" class="headerlink" title="模型Train阶段Error 具体解决"></a>模型Train阶段Error 具体解决</h2><p>如下图是，MNIST手写数字识别，激活函数用sigmoid，training data上的accuracy与层数的关系曲线：<img src="https://img-blog.csdnimg.cn/91b2987e4b43432f88b04cd027a9ec56.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>层数&gt;7时，performance下降，<strong>原因不是 <em>overfitting</em>! 因为train的时候就没train好。</strong><br>那模型压根没有训练好，可以采用的方法上面也给出了包括：换激活函数（Sigmoid、ReLU、Maxout、Tanh、Softmax）。激活函数可能会带来什么样的问题？以sigmoid为例说：会出现梯度消失。</p><h3 id="方法一：换激活函数的问题-——-梯度消失"><a href="#方法一：换激活函数的问题-——-梯度消失" class="headerlink" title="方法一：换激活函数的问题 —— 梯度消失"></a>方法一：换激活函数的问题 —— 梯度消失</h3><h4 id="什么是梯度消失"><a href="#什么是梯度消失" class="headerlink" title="什么是梯度消失"></a>什么是梯度消失</h4></blockquote><p>有梯度时，参数才会往梯度最小的地方改变；没有梯度了，参数就停止更新了。<br>前面层的学习速率明显低于后面层（后向传播），这就是梯度消失。<img src="https://img-blog.csdnimg.cn/f82182495a2f4ec5bd000941331d346a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="为什么会有梯度消失？"><a href="#为什么会有梯度消失？" class="headerlink" title="为什么会有梯度消失？"></a>为什么会有梯度消失？</h4><blockquote><p>角度一： 用sigmoid会出现梯度消失的问题（<strong>参数的变化经过sigmoid会逐层衰减，对最后的loss影响很小</strong>）<br><img src="https://img-blog.csdnimg.cn/052d1db3b2ef4b628a984db51778bf5a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如上图所示，我刚开始增加的△w，由于 <em>sigmoid函数</em> ，自变量越大，改变量越小的缘故。数的变化经过sigmoid会逐层衰减，对最后的loss影响很小。也就是对C的偏导会越来越小，导致梯度接近于0，而消失。</p><p>角度二：<br>假设现在存在一个网络结构：<br><img src="https://img-blog.csdnimg.cn/2de3a4da8a8c4ad1b0c6ca4fe6f5787d.png#pic_center" alt="在这里插入图片描述"><br>其整个数学模型可以表示为：<img src="https://img-blog.csdnimg.cn/f0e9bab2192e48d5b30ff91f0cb7ac58.png#pic_center" alt="在这里插入图片描述"><br>若要对于 <em>w1</em><br>求梯度，根据链式求导法则，得到的解为：<img src="https://img-blog.csdnimg.cn/f685ef885b0541198b35f65f257e4296.png#pic_center" alt="在这里插入图片描述"><br>这明显数学模型的值，随着隐层的增加，越来越小。不过这个前提是 原先的w（权重）并不是很大，原本相乘就是小于1的。如果w太大的话，第一项相乘就大于1，这就会导致最后的梯度爆炸。</p></blockquote><h4 id="如何解决梯度消失问题-——-换ReLU激活函数"><a href="#如何解决梯度消失问题-——-换ReLU激活函数" class="headerlink" title="如何解决梯度消失问题 —— 换ReLU激活函数"></a>如何解决梯度消失问题 —— 换ReLU激活函数</h4><h5 id="ReLU-与-MaxOut"><a href="#ReLU-与-MaxOut" class="headerlink" title="ReLU 与 MaxOut"></a>ReLU 与 MaxOut</h5><p>梯度消失是因为 <em>sigmoid</em> 引起的，要解决当然要换一个激活函数。采用的方法是换 <em>ReLU激活函数</em>（原型 input<0时，输出为0，input>0,输出为原值；可变型）<img src="https://img-blog.csdnimg.cn/44d11711c43a4495af5a232ba87f8df6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>ReLU输出0或x，输出0的ReLU神经元相当于不存在，网络变得瘦长，但是整个网络仍然是<strong>非线性的</strong>，只有当input改变十分微小的时候才是线性的，因为input不同，输出为0的ReLU神经元也不同。</p><blockquote><p><strong>Q : 为什么ReLU是非线性？明明两端都是线性直线啊？</strong><br>因为ReLU在0点没有导数定义，线性讲究的是在整个定义域内。而在0点处，不可微，所以整体是非线性的。</p></blockquote><p> ReLU是<strong>Maxout</strong>的特例，Maxout可以学出激活函数。<br><img src="https://img-blog.csdnimg.cn/436e424d5bf8460a9eaecb263ffdc73d.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h5 id="MaxOut-的训练是怎样的？好在哪？"><a href="#MaxOut-的训练是怎样的？好在哪？" class="headerlink" title="MaxOut 的训练是怎样的？好在哪？"></a>MaxOut 的训练是怎样的？好在哪？</h5><p>下面是一个神经网络的栗子，我们将激活函数换成 <em>MaxOut</em>激活函数。<br><img src="https://img-blog.csdnimg.cn/6b39a8e35978476e96d1eb7d9580c49d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上面的图 可以删除不需要的边和点，变为如下的神经网络：<br><img src="https://img-blog.csdnimg.cn/92fa0460f9a84b0689ef391fc257ade0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>Q ：如果和上图所示，那我去掉的那些边不是啥用没有？也就是这个权值压根和神经网络没关系呢，不是吗？</strong><br>答 ：当然有关系，因为我们的任务是调参，每个权值和偏值得变化，都会导致 <em>MaxOut激活函数</em> 选择的不同。比如说改变一下 <em>w</em>，他可能会选择 <em>z2</em>呢。</p></blockquote><h3 id="方法二：优化器-——-优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）"><a href="#方法二：优化器-——-优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）" class="headerlink" title="方法二：优化器 —— 优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）"></a>方法二：优化器 —— 优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）</h3><p>寻找最佳参数的方法，调节学习率的方法有（<strong>SGD、Adagrad、RMSProp、Momentum、Adam</strong>）<br>SGD、Adagrad 可以看之前关于 梯度算法进阶的部分 <a href="#">Post not found: 梯度下降算法 进阶</a><br>这边主要讲解一下 RMSProp 和 Momentum</p><h4 id="关于-RMSProp"><a href="#关于-RMSProp" class="headerlink" title="关于 RMSProp"></a>关于 RMSProp</h4><h5 id="为什么要使用RMSProp"><a href="#为什么要使用RMSProp" class="headerlink" title="为什么要使用RMSProp"></a>为什么要使用RMSProp</h5><p>　在 <em>Adagrad</em> 中，<font color="red">学习率是跟损失函数对 <em>w</em> 的<strong>二次微分</strong>有关。</font>那么对于图中蓝绿相交的一点来说，因为 <em>w1</em> 所在的曲率相对于 <em>w2</em> 要小，所以 <em>w1</em> 的学习率会比 <em>w2</em> 大。现在单考虑 <em>w1</em>（只看横向），那么二次微分是固定的（碗状），也就是说 <em>w1</em>是根据固定的规则去自动调整 <em>η</em> 的。但是现实中同一方向的二次微分是不固定的，因此对于同一方向去 <em>w1</em>，需要不同的规则去调 <em>η</em>。</p><font color=" turquoise">对于一个参数来说，*Adagrad* 是用固定的规则去调 *η*，*RMSProp* 是用变化的规则去调 *η*</font>![在这里插入图片描述](https://img-blog.csdnimg.cn/cc6becd55c104daaa6f8eddadc55eb9d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)##### 如何实现RMSProp在原来分母这一项中，在过去梯度平方和前面加上权值 *a*，现有的梯度平方加上 *1-a*。<font color=" turquoise">其在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得**陡峭的方向变得平缓**，从而加快训练速度）</font>![在这里插入图片描述](https://img-blog.csdnimg.cn/1377f4da7b564c9a89ede8f17577f241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)#### 关于 Momentum##### Momentum（动量）是用来解决什么问题的？<font color="red">**这个是用来解决局部最优解的问题的**</font><p>说白了就是要延续他的惯性<br><img src="https://img-blog.csdnimg.cn/85f0e0d5d9a540eaa4f68d7c2e13c4ab.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h5 id="如何实现Momentum"><a href="#如何实现Momentum" class="headerlink" title="如何实现Momentum"></a>如何实现Momentum</h5><p>考虑他此时的方向，并保留他的惯性。来调整下一步，最大可能的避免局部最优解。<br><img src="https://img-blog.csdnimg.cn/0201be68528b443bb4b5596d5f27b58d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="关于-Adam"><a href="#关于-Adam" class="headerlink" title="关于 Adam"></a>关于 Adam</h4><p>他其实是一种 <em>Momentum + RMSProp</em> 结合的方法。其具体算法可以看如下图所示：<img src="https://img-blog.csdnimg.cn/c3aff899691d4155b8e47e51420c868e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="模型Train阶段OK-Test阶段Error，即过拟合"><a href="#模型Train阶段OK-Test阶段Error，即过拟合" class="headerlink" title="模型Train阶段OK Test阶段Error，即过拟合"></a>模型Train阶段OK Test阶段Error，即过拟合</h2><h3 id="方法一-：参数在过拟合之前就停止更新（Early-Stopping）"><a href="#方法一-：参数在过拟合之前就停止更新（Early-Stopping）" class="headerlink" title="方法一 ：参数在过拟合之前就停止更新（Early Stopping）"></a>方法一 ：参数在过拟合之前就停止更新（Early Stopping）</h3><font color="red">这里的testing set指的是有label的testing set（即validation set ）。</font>如果learning rate设得对的话，training set的loss会逐渐降低，而testing set与training set可能分布不同，所以testing set的loss可能先降后升，这时就不要一直train下去，而是要在testing loss最小的地方停止train。这里的testing set 实际指的是**validation set**。### 方法二 ：正则化Regularization> **Q ： 首先理解什么是范数，L1（范数为1）和L2（范数为2）是什么？**> 范数：向量在不同空间中“长度”的计算公式> L1：绝对值之和> L2：平方和#### L2正则化（权值衰减）![在这里插入图片描述](https://img-blog.csdnimg.cn/8a8facf4f6bf42ebb56f4f657102b5ed.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)> **Q ： 为什么通常 不考虑 bias？**> L2正则化让function更平滑，而bias与函数平滑程度没有关系。![在这里插入图片描述](https://img-blog.csdnimg.cn/35cbdf14ab7049bd9967fcabc1223ae2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)参数更新：因为η、λ、n都是正的，**所以 *1−ηλ*小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来**。**当然考虑到后面的导数项，w最终的值可能增大也可能减小**。<font color="red">正则化在NN中虽有用但不明显。</font>NN参数初始值一般接近0，update参数即是要参数原理0。L2正则化（让参数接近0）的作用可能与early stopping类似。> **Q : 为什么参数w衰减能防止过拟合?**> 答 ：模型过于复杂会导致过拟合。那么越小的w（可以想象成0理解），表示网络复杂度低，越简单的网络结构，就越不会过拟合。比如模型  *y=w1×w1 + w2×w2* 的平方  中把 *w2=0* 代入，模型就会简化，就不会引起过拟合。![在这里插入图片描述](https://img-blog.csdnimg.cn/c8fc90454614466ebee4a2d5a549a25e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)> **Q : 对正则化解决过拟合问题的一些本质理解？**> 正则化防止过拟合的本质：减少“没用参数”的权值（防止过拟合），同时也减少“有用参数”的权值（会增加bias误差）> **Q : 什么是有用参数，什么是没用参数？如上图中，怎么就把 *x2* 删去，不把 *x1* 删去呢？**> 我们姑且假设 *w1* 是有用参数， *w2* 是无用参数，由公式知参数更新值跟权值、梯度值两个因素有关，实际上，无论是 *x1* 还是 *x2* ，权值都会衰减，每update一次参数，权值 *w* 就会衰减一次，但如果是下图的情况，*损失函数Loss* 的减少跟 *w2* 没关系的，所以对其偏导为0，那么 *w2* 的参数更新只跟权值有关了，随着更新次数叠加，权值就会逐渐衰减接近0；对于***有用参数 w1*** ，虽然它权值衰减，但是它其作用的是后面的偏导值，所以它还是不会变成0的。![在这里插入图片描述](https://img-blog.csdnimg.cn/50e060cb7bc24fbe896ef111e287092b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)#### L1正则化L1是在Loss函数加上绝对值之和，求偏导后比原始的更新规则多出了η ×λ ×sgn(w)这一项。<font color="turquoise">**当w为正时，更新后的w变小。当w为负时，更新后的w变大（一加一减）——因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合**</font>![在这里插入图片描述](https://img-blog.csdnimg.cn/113c9ac61f5946c783a434a07f8d52e9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)#### L1和L2相比L1 update的速度是*|ηλ||ηλ|* （划横线那项每次改变量是一定的）。而 L2 update 的速度与w的t有关（如果wt  很大，那么改变量也很大）。用L1做training，结果比较sparse(稀疏的)，参数中有很多接近0的值，也有很多很大的值。L2 learn的结果：参数值平均来讲比较小。### Dropout **Dropout也是为了简化神经网络结构的目的**，但L1、L2正则化是通过修改代价函数来实现的，而Dropout则是通过修改神经网络本身来实现的。#### dropout是如何实现的在training的时候，每次update参数之前，对每一个Neuron（包括input_layer）做sampling，决定这个Neuron按一定几率p丢掉，跟它相连的weight也被丢掉，结果得到一个细长的Network。（每一次update一个mini-batch之前，拿来traing的Network structure是不一样的）。换句话说input layer中每个element也算是一个neuron.每次更新参数之前都要resample.**用dropout，在training上的结果会变差，但在testing上的结果会变好。**![在这里插入图片描述](https://img-blog.csdnimg.cn/196bfc65730b4cb6b5877bcbc0a40729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)<font color="red">在**testing的时候不做dropout，所有neuron都要用**。如果training时的删除神经元的概率为p%，则在testing时，所有的weight都要乘以（1-p）%</font>![在这里插入图片描述](https://img-blog.csdnimg.cn/2c2491b39cef4452b88c9c465a8a1cbe.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)#### Dropout的原理 ![在这里插入图片描述](https://img-blog.csdnimg.cn/27c81c02d95847c2a5776da769696c54.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)> **Q : 为什么当在做 *testing* 的时候，weights需要都乘以 *(1-p)%*  (p是Dropout的概率)？**>> 答：如下图所示，左侧是在 *Traing* 阶段，*w2* 与 *w4* 由于丢失几率和丢掉(几率是这里是0.5)，假设神经元此时都是1，这时输出的应该是 *w1+w3*；右侧是在 *Testing* 阶段，在这个阶段要保证所有的神经元都不做 *Dropout* 。所以变成了w1+w2+w3+w4，约等于变成了左侧的两倍；所有取所有weight都要乘以（1-0.5）。实际上，dropout是利用ensemble思想，把一个复杂神经网络的训练转化为，训练很多个简单的神经网络，然后再把多个简单神经网络训练出来的参数做平均。![在这里插入图片描述](https://img-blog.csdnimg.cn/96b24fb760ee4d65b18862bc7bcf4ee0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)一个复杂model, bias准，但variance大，把多个复杂model ensemble起来，variance变小。每个dropout后的结构由一个batch来train，但是权重是共享的，每个权重是由多个batch 来train的。![在这里插入图片描述](https://img-blog.csdnimg.cn/32d269be253c4f1790664d3b5c88caca.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)<font color="red">**在testing的时候，把多个结构的结果取平均，与把所有参数乘以(1 - p%)，效果是近似的。****Dropout用在ReLU、Maxout上效果较好。**</font><blockquote><p><strong>Q : 为什么 Dropout 在testing的时候，把多个结构的结果取平均，与把所有参数乘以(1 - p%)，效果是近似的？ 而不是相同</strong><br>答：因为神经元之间的层次，使用的激活函数不一定都是线性的，只有线性的情况下才会是相同的。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;深度学习 Deep Learning 后续优化&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;深度学习  怎么评价效果与改进？&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先检查 训练阶段 是否有比较好的结果&lt;br&gt;training优化方法：
　&lt;ol</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="模型优化" scheme="http://example.com/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 Deep Learning 基础</title>
    <link href="http://example.com/2021/08/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2021/08/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E5%9F%BA%E7%A1%80/</id>
    <published>2021-08-07T09:03:01.000Z</published>
    <updated>2021-08-07T09:03:41.534Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="深度学习 Deep Learning 基础">TOC</a></p><p><strong>深度学习需要明白的几个问题？</strong><br>思路：</p><ol><li>什么是深度学习？为什么需要深度学习？深度学习和机器学习的关系？</li><li>深度学习的步骤</li><li>确定神经网络模型的损失函数，如何优化模型，即调参问题</li><li>如何使用Back Propagation（反向传播） 方法 update DNN（深度神经网络）参数</li></ol><h1 id="深度学习的概念"><a href="#深度学习的概念" class="headerlink" title="深度学习的概念"></a>深度学习的概念</h1><h2 id="什么是深度学习？"><a href="#什么是深度学习？" class="headerlink" title="什么是深度学习？"></a>什么是深度学习？</h2><p>深度学习（Deep Learning，DL）是指多层的人工神经网络和训练它的方法。一层神经网络会把大量矩阵数字作为输入，通过<strong>非线性激活方法</strong>取权重，再产生另一个数据集合作为输出。这就像生物神经大脑的工作机理一样，通过合适的矩阵数量，多层组织链接一起，形成神经网络“大脑”进行精准复杂的处理，就像人们识别物体标注图片一样。</p><ul><li>深度学习的model是一个深度神经网络结构（neural structure）</li><li>深度学习的“深度”是指神经网络的隐层（hidden layer）数量足够多</li><li>深度学习是<strong>自动提取特征</strong>（Feature extractor），<strong>不需要像逻辑回归那样特征转换</strong>（Feature engineering）</li></ul><h2 id="为什么需要深度学习？-深度学习和机器学习的关系？"><a href="#为什么需要深度学习？-深度学习和机器学习的关系？" class="headerlink" title="为什么需要深度学习？ 深度学习和机器学习的关系？"></a>为什么需要深度学习？ 深度学习和机器学习的关系？</h2><ul><li>传统机器学习的模型结构较简单，很依赖算法工程师做特征工程甚至子模型来提升模型效果。就像我们之前上节那个栗子一样，做多分类的问题，四个角对角是一个类的情况，没办法进行分类，所以只能使用特征工程来进行特征的变换。</li><li>深度学习由于其层次化的结构，理论上可以拟合任意函数，整个复杂结构即可以用来对特征进行自动组合（如图像），也可以用来构建复杂的模型（如nlp领域里的LSTM，能够考虑上下文）。</li></ul><p><img src="https://img-blog.csdnimg.cn/b8b8df57f814494d8199e0e7b708e597.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="深度学习的步骤"><a href="#深度学习的步骤" class="headerlink" title="深度学习的步骤"></a>深度学习的步骤</h1><p>其实和机器学习一样分为三步。<img src="https://img-blog.csdnimg.cn/514f075664384cdcb62f6062fd5bd722.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="Step-1：定义一个神经网络结构（neural-structure）"><a href="#Step-1：定义一个神经网络结构（neural-structure）" class="headerlink" title="Step 1：定义一个神经网络结构（neural structure）"></a>Step 1：定义一个神经网络结构（neural structure）</h2><p>神经网络的创建包括3部分：</p><ol><li>神经网络有多少<strong>隐层（layer）</strong></li><li>每一层有多少<strong>神经元（neuron）</strong></li><li>每个神经元之间如何连接</li></ol><p>常见的出名神经网络<img src="https://img-blog.csdnimg.cn/5394cd49b44545c8ab4b8c1191d69239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="神经元怎么定义？"><a href="#神经元怎么定义？" class="headerlink" title="神经元怎么定义？"></a>神经元怎么定义？</h3><p>每个神经元都有一个 bias 和一个 function ，每条输入的边都有一个 weight<img src="https://img-blog.csdnimg.cn/52bc001e984c4bdf980921c7f2458fdf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="一个神经网络的栗子"><a href="#一个神经网络的栗子" class="headerlink" title="一个神经网络的栗子"></a>一个神经网络的栗子</h3><p>下面是一个 3个隐层（layer）、六个神经元（neuron）（每个球都是一个神经元）、全连接<strong>前馈</strong>网络（Fully Connection Feedforward Network）</p><blockquote><p>“<strong>前馈</strong>”是指整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示<br>其实我们常用的网络，都是前馈神经网络，从输入到输出是一个有向图，中间不会有环或者反向传播。<br>当然，我们在训练前馈神经网络的时候，会用到反向传播进行参数调整。但仍不影响整个网络的有向和前馈性质。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/748979f3a3c74e0ab0c19ea60bd13142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="神经网络如何工作？"><a href="#神经网络如何工作？" class="headerlink" title="神经网络如何工作？"></a>神经网络如何工作？</h3><p><img src="https://img-blog.csdnimg.cn/d685eb4da45646df93096525226bb59f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其实 这就是矩阵运算（可以使用GPU加速计算）<img src="https://img-blog.csdnimg.cn/34294cf6a4254049b0f8bbc0aaf631a1.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>总体可以归纳为：<br><img src="https://img-blog.csdnimg.cn/9baa5b003ea04ff7ad3bc25076e2bb58.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="栗子：识别手写数字图像"><a href="#栗子：识别手写数字图像" class="headerlink" title="栗子：识别手写数字图像"></a>栗子：识别手写数字图像</h3><p>从图像中识别是数字几？<br><img src="https://img-blog.csdnimg.cn/5fb1bc809a444ff5bc19325453eb703d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="制作神经网络模型-的FAQ（最容易被问得问题）"><a href="#制作神经网络模型-的FAQ（最容易被问得问题）" class="headerlink" title="制作神经网络模型 的FAQ（最容易被问得问题）"></a>制作神经网络模型 的FAQ（最容易被问得问题）</h3><ul><li>Q1： 神经网络需要几个隐层（layers）？每个层需要多少个神经元呢？<ul><li>反复试验+ 直觉 （说白了就是要慢慢试= = 看经验呗）</li></ul></li><li>Q2：神经网络可以自动确定结构吗？<ul><li>理论上其实是可以的，不过这些方法我们还没学到。Evoluntionary Artifical Neural Networks</li></ul></li><li>Q3：我们可以设计层次之间的结构么？<ul><li>意思就是说例如Layer1 链接 Layer3 这样跳着的 等等。当然可以 CNN就是不按顺序来的，具体看下一节。<br><img src="https://img-blog.csdnimg.cn/6c74000cf96a4af0b33146e9dca6efc3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul></li></ul><h2 id="Step-2：确定神经网络模型的损失函数"><a href="#Step-2：确定神经网络模型的损失函数" class="headerlink" title="Step 2：确定神经网络模型的损失函数"></a>Step 2：确定神经网络模型的损失函数</h2><p>还是和逻辑回归一样，用<strong>交叉熵损失函数</strong>，调参使得交叉熵损失函数最小，如下图所示：<br><img src="https://img-blog.csdnimg.cn/29c36a171c584a36bead0693b41b9027.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>当然上面只是一个点的，把每个样本结合起来看。 <img src="https://img-blog.csdnimg.cn/ef62afa46f1248a09b17cea823c91513.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>不过这个地方有几个疑问？</p><blockquote><ul><li>Q1：神经网络里面有很多层，所以有很多 <em>w</em> 和 <em>b</em> 调整。那是全体都一次性调整么？</li><li>答：  <strong>现在不知道啊！</strong></li><li>Q2：上面我是把所有的 交叉熵 都加在一起 然后对 整个大的损失函数进行调参是这样嘛？ 那我的训练样本应该是 各个数字都有是吗？</li><li>答： <strong>现在不知道啊！</strong></li></ul></blockquote><h2 id="Step-3：如何找到一个最好的函数（最佳参数），即调参"><a href="#Step-3：如何找到一个最好的函数（最佳参数），即调参" class="headerlink" title="Step 3：如何找到一个最好的函数（最佳参数），即调参"></a>Step 3：如何找到一个最好的函数（最佳参数），即调参</h2><p>用的还是 <strong>梯度下降法</strong>  </p><ol><li>随机选取一组参数</li><li>输入所有的训练样本</li><li>然后样本数据不变，参数不断变，用梯度下降法更新参数</li></ol><p><img src="https://img-blog.csdnimg.cn/72b790c9e1764d88bfef2b9f447c36c3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/24ebb0869baf46e899e8f509d26ba720.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="Backpropagation（反向传播）-来优化调参速度"><a href="#Backpropagation（反向传播）-来优化调参速度" class="headerlink" title="Backpropagation（反向传播） 来优化调参速度"></a>Backpropagation（反向传播） 来优化调参速度</h1><p>神经网络可能有很多个隐层，因此可能有百万数量级的参数，为了在<strong>梯度下降时有效地快速计算梯度</strong>，使用<strong>反向传播</strong>。<br>下图是 使用的损失函数为：交叉熵损失函数 ， 梯度下降法就是对每个参数求偏导，然后根据偏导大小进行左右移动，找到偏导为0的点，就是最佳参数值。</p><h2 id="计算对参数偏导的表达式"><a href="#计算对参数偏导的表达式" class="headerlink" title="计算对参数偏导的表达式"></a>计算对参数偏导的表达式</h2><p><img src="https://img-blog.csdnimg.cn/52fb4c86e71843b3b44aec5e596d4324.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>交叉熵损失函数，为 正确值乘以 ln的带入x样本的线性模型 +<br>（1-正确值）乘以ln（1-带入x样本线性模型）<img src="https://img-blog.csdnimg.cn/cf25205ad5354582b0355f3aba098387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>只考虑刚输入阶段的神经元，可以得到用 <em>sigmoid</em>激活函数之前的量 <em>z</em>，而 <em>z</em> 是由线性模型（按权重分配向量 + 偏值 <em>b</em> 得到）。如下图所示，可以看到对参数 w 的偏导可以按照链式法则为  <script type="math/tex">∂C/∂w=∂z/∂w × ∂C/∂z</script>  前项为前项传递，后项为后向传递<br><img src="https://img-blog.csdnimg.cn/0438d1f3d1384270bf8a8abc93a6fe81.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><ul><li>前项传递非常好看出来，就是对应的输入值，比如 <code>∂C/∂w1=∂z/∂w1 × ∂C/∂z</code> 。其中<code>∂z/∂w1</code> 看图上所示，不就是对 <em>w1</em> 进行偏导么，那就是 <em>x1</em>。</li><li>后项过程比较复杂，根据链式法则，<code>∂C/∂z=∂a/∂z × ∂C/∂a</code>，其中<code>∂a/∂z = σ&#39;(z)</code> 如下图所示<img src="https://img-blog.csdnimg.cn/636c2b53744d478ab268bf80073be7fa.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>归纳一下得到如下：<img src="https://img-blog.csdnimg.cn/c03a971beb4940cd93cb185b340bf470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">这个时候我们从另一观点看待上面的式子：有另外一个神经元（下图中的三角形，表示乘法/放大器），input是<code>∂C/∂z</code>‘与<code>∂C/∂z′&#39;</code>  ，权重分别是 <em>w3,w4</em>，求和经过神经元（乘以σ′(z)），得到 <code>∂C/∂z</code>。（相当于反向传播，先线性加权再乘以一个σ′(z) 和正向非常类似）<img src="https://img-blog.csdnimg.cn/6ec05d31b6ed4eb89ab2c5bcc39b6282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul><h2 id="后项传递-的两种情况"><a href="#后项传递-的两种情况" class="headerlink" title="后项传递 的两种情况"></a>后项传递 的两种情况</h2><p>如上图所示是我们最后得到的后项传递的式子，其中我们还有两个项不知道。那我们现在需要计算这两个部分，但分为两种情况</p><h3 id="case-1：下一层是最终输出层"><a href="#case-1：下一层是最终输出层" class="headerlink" title="case 1：下一层是最终输出层"></a>case 1：下一层是最终输出层</h3><p>第一种情况，z′,z′′ 所接的neuron是output layer的neuron。<br>这个就比较简单，直接根据最后一层的输出反向写出即可。<img src="https://img-blog.csdnimg.cn/5683f53fae4b4b619a617d5513159510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="case-2：下一层不是最终输出层"><a href="#case-2：下一层不是最终输出层" class="headerlink" title="case 2：下一层不是最终输出层"></a>case 2：下一层不是最终输出层</h3><p>第二种情况，z′,z′′ 所接的neuron不是output layer的neuron。<img src="https://img-blog.csdnimg.cn/935435dd3b0b435c9ab31388786acd78.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">就上面那个图，我们可以推得类似后项传递的式子<img src="https://img-blog.csdnimg.cn/783ca76c842846efa9b6d89de1a874d9.png#pic_center" alt="在这里插入图片描述">就这样反复迭代(递归)，直到遇到case1的情况，就可以算出整个后项传递。然后结合之前的前项传递，就是我们要得到的对这个参数的偏微分值。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://img-blog.csdnimg.cn/9796822a131546fabd92e1a1bac1a2d1.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>梯度下降时需要计算损失函数对每个参数偏微分<code>∂C/∂w</code>，<em>w</em> 代表相邻隐层之间的一条连线（即权值），每个 <em>w</em> 只有一个所指的神经元。</p><ul><li>链式法则将计算 <code>∂C/∂w</code> 拆成前向过程与后向过程。</li><li>前向过程计算的是<code>∂z/∂w</code> ，这里 <em>z</em> 是 <em>w</em> 所指neuron的input，<strong>计算结果是与 <em>w</em> 相连的值。</strong><br>后向过程计算的是<code>∂C/∂z</code>，这里 <em>z</em> 仍是 <em>w</em> 所指neuron的input，计算结果通过从后至前递归得到。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;深度学习 Deep Learning 基础&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;深度学习需要明白的几个问题？&lt;/strong&gt;&lt;br&gt;思路：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;什么是深度学习？为什么需要深度学习？深度学习和机器学习的关系？&lt;/li&gt;
</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="反向传播" scheme="http://example.com/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归 Logistic Regression</title>
    <link href="http://example.com/2021/08/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%20Logistic%20Regression/"/>
    <id>http://example.com/2021/08/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%20Logistic%20Regression/</id>
    <published>2021-08-05T15:08:06.000Z</published>
    <updated>2021-08-05T15:08:47.033Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="逻辑回归问题 Logistic Regression">TOC</a></p><p><strong>逻辑回归需要明白的几个问题？</strong></p><p>1、逻辑回归(Logistics Regression) 与 线性回归(Linear Regression)的区别在哪<br><img src="https://img-blog.csdnimg.cn/d50c9e51dfae4dadba381f980f05752a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>2、生成模型(Generative Model) 与 判别模型(Discriminative Model）的区别在哪</p><ul><li>生成模型就是要学习 x 和 y 的联合概率分布 <em>P(x,y)</em>，然后根据贝叶斯公式来求得条件概率 <em>P(y|x)</em>，预测条件概率最大的 y。</li><li>判别模型就是直接学习条件概率分布 <em>P(y|x)</em>。</li></ul><p>3、逻辑回归(Logistics Regression) 与  深度学习（ VS Deep Learning）<br>逻辑回归具有缺陷，需要做特征工程来转变特征，但是这个人为步骤非常麻烦，所以引入了深度学习。</p><h1 id="逻辑回归与线性回归"><a href="#逻辑回归与线性回归" class="headerlink" title="逻辑回归与线性回归"></a>逻辑回归与线性回归</h1><h2 id="什么是逻辑回归？"><a href="#什么是逻辑回归？" class="headerlink" title="什么是逻辑回归？"></a>什么是逻辑回归？</h2><ul><li>逻辑回归是<strong>解决分类问题的一种算法</strong></li><li>它与 <strong>线性模型</strong> 形式上有点像（本质上是在线性模型外面“裹”一个<strong>sigmoid激活函数</strong>，来表示概率的函数）</li><li>它是一种<strong>判别模型</strong>，与前面说的生成模型不同</li><li>它是深度学习的基础</li></ul><h2 id="对比逻辑回归与线性回归"><a href="#对比逻辑回归与线性回归" class="headerlink" title="对比逻辑回归与线性回归"></a>对比逻辑回归与线性回归</h2><h3 id="区别一：模型不同"><a href="#区别一：模型不同" class="headerlink" title="区别一：模型不同"></a>区别一：模型不同</h3><p>本质上是在线性模型外面“裹”一个<strong>sigmoid激活函数</strong>，来表示概率的函数<br><img src="https://img-blog.csdnimg.cn/559dca41ef354a65b9e73ad4d8bb8222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2c3580835cbe4497a28537f12f972bf9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>总结的来看<img src="https://img-blog.csdnimg.cn/e591d052433d421da59cc20086e67959.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="区别二：损失函数Loss不同"><a href="#区别二：损失函数Loss不同" class="headerlink" title="区别二：损失函数Loss不同"></a>区别二：损失函数Loss不同</h3><p><strong>逻辑回归的</strong><br><img src="https://img-blog.csdnimg.cn/81b9e6bbaf8743c8811b04dab97ae6cd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>为什么似然函数最大，参数就越有可能，越合理？</strong><br>最大似然估计：现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/d853c71170e743b58d68534399dbcd5a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/09f5eac31e1e4c90a1ca26b713423f2c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>总结一下：</strong><br><img src="https://img-blog.csdnimg.cn/c1c93acf9e1a46f9b6b3d821a168ebdf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>问题：为什么逻辑回归不采用线性回归的差平方来做？<br>例如，其实目前离目标点还很远，但梯度已经为0了，这显然不合理。<img src="https://img-blog.csdnimg.cn/2dee2450c8d34bf1b56a4e5c8d305db9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="区别三：如何调参的方式是一致的"><a href="#区别三：如何调参的方式是一致的" class="headerlink" title="区别三：如何调参的方式是一致的"></a>区别三：如何调参的方式是一致的</h3></blockquote><ul><li><strong>化简逻辑回归损失函数左侧部分</strong><br><img src="https://img-blog.csdnimg.cn/5a911af3aeb7475b832369858f503b7f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li><strong>化简逻辑回归右侧部分</strong><img src="https://img-blog.csdnimg.cn/c627e39a6c9348d0a3e81c0099430536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>化简，调参如下<img src="https://img-blog.csdnimg.cn/ccfda49ca7004f8f87fc76f238dd5337.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><img src="https://img-blog.csdnimg.cn/771e437cc99945d8b884f331f09132eb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul><h1 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h1><h2 id="什么是生成模型和判别模型？"><a href="#什么是生成模型和判别模型？" class="headerlink" title="什么是生成模型和判别模型？"></a>什么是生成模型和判别模型？</h2><p><strong>从本质上讲，生成模型和判别模型是解决分类问题的两类基本思路 。</strong><br><em>分类问题，就是给定一个数据 x，要判断它对应的所属标签 y</em></p><ul><li>生成模型就是要学习 x 和 y 的联合概率分布 <em>P(x,y)</em>，然后根据贝叶斯公式来求得条件概率 <em>P(y|x)</em>，预测条件概率最大的 y。</li><li>判别模型就是直接学习条件概率分布 <em>P(y|x)</em>。</li></ul><h2 id="两种模型案例-举个栗子？"><a href="#两种模型案例-举个栗子？" class="headerlink" title="两种模型案例 举个栗子？"></a>两种模型案例 举个栗子？</h2><blockquote><p>栗子1：<br>假设你从来没有见过大象和猫，连听都没有听过，这时，给你看了一张大象的照片和一张猫的照片。<br><strong>Q : 你看过之后，有人牵了一头真的大象过来，问你只是大象还是猫？</strong></p><ul><li>用<strong>判别模型</strong>的思路回答：你回想刚才看过的照片，大象比猫很明显有个长鼻子，所以眼前这个有着长鼻子的动物就是大象。</li><li><p>用<strong>生成模型</strong>的思路回答：你回想刚才看过的照片，然后用笔把它们画在了纸上，拿着纸和我家的大象做比较，你发现，眼前的动物更像是大象。</p><p>第一个解决问题的思路就是判别模型，<strong>因为你只记住了大象和猫之间的不同之处</strong>。第二个解决问题的思路就是生成模型，因为你<strong>实际上学习了什么是大象，什么是猫</strong>。</p></li></ul><p>栗子2：<br>有四个形式为(x,y)的样本。(1,0), (1,0), (2,0), (2,1）。假设，我们想从这四个样本中，<strong>学习到如何通过x判断y的模型</strong>。</p><ul><li>用生成模型，我们要学习 <em>P(x,y)</em>。如下所示：<img src="https://img-blog.csdnimg.cn/b435c974485141b1a36542a5c933ac01.png#pic_center" alt="在这里插入图片描述">我们学习到了四个概率值，它们的总和是1，这就是联合分布律P(x,y)。（因为这是离散的，连续的话叫联合概率密度）<ul><li>用判别模型，我们要学习 <em>P(y|x)</em>，如下所示：<img src="https://img-blog.csdnimg.cn/4bde84a64231412d951a1c7c31d180f1.png#pic_center" alt="在这里插入图片描述">因为这是条件分布律，每一行概率值相加都为1。</li></ul></li></ul><p><strong>Q : 当 <em>x=1</em> 时，请问 <em>y</em> 是 <em>0</em> 还是 <em>1</em> 呢？</strong></p><ul><li>用<strong>生成模型</strong>，我们会比较<br>P(x=1,y=0) = 1/2<br>P(x=1,y=1) = 0<br>我们发现 <em>P(x=1,y=0)</em>的概率要比 <em>P(x=1,y=1)</em>的概率大，所以，我们判断：<em>x=1时，y=0</em>。</li><li><p>用<strong>判别模型</strong>，我们会比较：<br>P(y=0|x=1) = 1<br>P(y=1|x=1) = 0<br>同样，<em>P(y=0|x=1)</em> 要比 <em>P(y=1|x=1)</em>大，所以，我们判断：<em>x=1时，y=0</em>。</p><p>我们看到，虽然最后预测的结果一样，但是得出结果的逻辑却是完全不同的。</p></li></ul></blockquote><h2 id="生成模型为啥叫生成模型？"><a href="#生成模型为啥叫生成模型？" class="headerlink" title="生成模型为啥叫生成模型？"></a>生成模型为啥叫生成模型？</h2><p>生成模型之所以叫生成模型，是因为：<br>它背后的思想是，x是特征，y是标签，什么样的标签就会生成什么样的特征。好比说，标签是大象，那么可能生成的特征就有大耳朵，长鼻子等等。<br>当我们来根据x来判断y时，我们实际上是在比较，<strong>什么样的y标签更可能生成特征x，我们预测的结果就是更可能生成x特征的y标签</strong>。</p><h2 id="为什么一般来说，判别模型表现得会比生成模型好？"><a href="#为什么一般来说，判别模型表现得会比生成模型好？" class="headerlink" title="为什么一般来说，判别模型表现得会比生成模型好？"></a>为什么一般来说，判别模型表现得会比生成模型好？</h2><p>我们举一个栗子，现在有Class1 和 Class2 两类数据。现在训练集数据如图所示：<img src="https://img-blog.csdnimg.cn/bd00dca74d524c1a83b9b90bb3745fcf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>Q : 问请问如下的测试集，他是应该属于Class1 还是 Class2 呢？</strong><img src="https://img-blog.csdnimg.cn/a1ee9138baba426883e87dcd9b3f2358.png#pic_center" alt="在这里插入图片描述"><br>我们这边用判别模型来计算，算出如下图所示的数据：<img src="https://img-blog.csdnimg.cn/3f0b16520085492da2e7639d2646130c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>然后用贝叶斯公式算出 <em>P(C1 | x)</em> ，就是当x1和x2全为1 的情况下的概率是多少。<img src="https://img-blog.csdnimg.cn/a6f8b3138add4e2bbb548322591e0584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>算出的结果是 <em>＜ 0.5</em> 的，但从我们人的角度看，其实应该是属于Class1的，因为Class2里面压根就没有x1，x2同时为1的存在。<strong>这是为什么呢？因为我们的样本太少了，用判别模型是可以帮助我们在有限的数据样本中假象数据。</strong></p><blockquote><p><strong>那用生成模型怎么做呢？</strong></p></blockquote><p>所以判别模型的优势在于</p><ol><li>样本量少的时候表现比判别模型好，因为它能自己脑补出一个假想模型</li><li>噪声对它影响较小，因为它没有过分依赖数据，它是按照自己假想模型走的</li></ol><h2 id="常见的生成模型和判别模型有哪些呢？"><a href="#常见的生成模型和判别模型有哪些呢？" class="headerlink" title="常见的生成模型和判别模型有哪些呢？"></a>常见的生成模型和判别模型有哪些呢？</h2><p>生成模型</p><ul><li>HMM（隐马尔可夫模型）</li><li>朴素贝叶斯</li></ul><p>判别模型</p><ul><li>逻辑回归</li><li>SVM（支持向量机）</li><li>CRF（条件随机场）</li><li>最近邻</li><li>一般的神经网络</li></ul><h1 id="逻辑回归与深度学习"><a href="#逻辑回归与深度学习" class="headerlink" title="逻辑回归与深度学习"></a>逻辑回归与深度学习</h1><h2 id="逻辑回归解决多分类问题"><a href="#逻辑回归解决多分类问题" class="headerlink" title="逻辑回归解决多分类问题"></a>逻辑回归解决多分类问题</h2><p>逻辑回归是解决分类问题的，实际中的问题大多是多分类的问题，多分类问题会用到softmax。 <em>逻辑回归其实就是线性回归在外面加了个sigmoid激活函数（二分类）或者softmax激活函数（多分类）。</em></p><blockquote><p><strong>sigmoid激活函数 和 softmax函数的区别：</strong><br>通常在<strong>二分类</strong>中使用<strong>sigmoid作为最后的激活层</strong>。在<strong>多分类</strong>单标签中使用<strong>softmax作为激活层</strong>，取概率最高即可。多标签问题中使用sigmoid作为激活层，相当于把每一个类别都当成了二分类来处理。<img src="https://img-blog.csdnimg.cn/52837028507c423c97e4ba53a831270d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>多分类问题解决<br><img src="https://img-blog.csdnimg.cn/f92fa56ec4154bf687f88bc0192dcb08.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b674d12040f14c7dacc30b4a19f3af0b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="逻辑回归的局限性"><a href="#逻辑回归的局限性" class="headerlink" title="逻辑回归的局限性"></a>逻辑回归的局限性</h2><p><img src="https://img-blog.csdnimg.cn/d1fd15ed70b44e689619a1dfecaed0f4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h2 id="用深度学习去解决这个问题"><a href="#用深度学习去解决这个问题" class="headerlink" title="用深度学习去解决这个问题"></a>用深度学习去解决这个问题</h2><p><img src="https://img-blog.csdnimg.cn/23f73c28c3f54fa19624a03839a18958.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>这个怎么变换过来的啊？</strong><br>是需要用特征工程的方法的，而特征工程是需要我们人为地去建立一个特征函数去把这些点转化，<strong>实际上是比较难的</strong>，或者说比较费工夫的<br>这个时候我们需要引入 <strong>深度学习</strong><br><img src="https://img-blog.csdnimg.cn/39cb9b68112d48e384f6958820a041c9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;逻辑回归问题 Logistic Regression&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;逻辑回归需要明白的几个问题？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1、逻辑回归(Logistics Regression) 与 线性回归(Linear Re</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="分类问题" scheme="http://example.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    
    <category term="逻辑回归" scheme="http://example.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>分类问题 Classification 案例一：神奇宝贝是水系还是普通系？</title>
    <link href="http://example.com/2021/08/04/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%20Classification%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E7%A5%9E%E5%A5%87%E5%AE%9D%E8%B4%9D%E6%98%AF%E6%B0%B4%E7%B3%BB%E8%BF%98%E6%98%AF%E6%99%AE%E9%80%9A%E7%B3%BB%EF%BC%9F/"/>
    <id>http://example.com/2021/08/04/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%20Classification%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E7%A5%9E%E5%A5%87%E5%AE%9D%E8%B4%9D%E6%98%AF%E6%B0%B4%E7%B3%BB%E8%BF%98%E6%98%AF%E6%99%AE%E9%80%9A%E7%B3%BB%EF%BC%9F/</id>
    <published>2021-08-04T06:21:06.000Z</published>
    <updated>2021-08-06T14:47:11.946Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="分类问题 Classification">TOC</a></p><blockquote><p>概念：（从概率生成模型到判别模型）<br>概率生成模型：由数据学习联合概率密度分布 <em>P(X,Y)</em> ，然后求出条件概率分布<em>P(Y|X)</em> 作为预测的模型。例如：朴素贝叶斯、隐马尔可夫（em算法）<br>判别模型：由数据直接学习决策函数 <em>Y=f(X)</em> 或者条件概率分布 <em>P(Y|X)</em>  作为预测的模型。例如：k近邻法、感知机、决策树、逻辑回归、线性回归、最大熵模型、支持向量机(SVM)、提升方法、条件随机场（CRF）</p></blockquote><p><strong>分类问题的思路</strong></p><ol><li>分类问题及其解决方法的讨论<pre><code> 1. 首先，什么是分类问题？ 2. 接着，分类问题该如何解决呢？</code></pre></li><li>建立概率生成模型的步骤（以朴素贝叶斯分类器为例）<br>step1：求先验概率<br>step2：确定<strong>数据属于哪一个分布，用最大似然估计出分布函数的参数</strong><br>step3：求出后验概率</li><li>生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出</li></ol><h1 id="分类问题及其解决方法的讨论"><a href="#分类问题及其解决方法的讨论" class="headerlink" title="分类问题及其解决方法的讨论"></a>分类问题及其解决方法的讨论</h1><h2 id="什么是分类问题？"><a href="#什么是分类问题？" class="headerlink" title="什么是分类问题？"></a>什么是分类问题？</h2><p><img src="https://img-blog.csdnimg.cn/06b2e30dc04b4fa1a871ee3acee382c3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>说白了 就是输入一个事务的一些参数特征，通过数学模型，可以得到这个东西是什么。 这也包括二分问题（结果是由两面）与多分问题。</p><p>案例：<br>举个栗子，输入一个神奇宝贝，输出：他是属于什么属性的？ <strong>这是一个典型的多分问题，因为属性有好几种啊。</strong><br><img src="https://img-blog.csdnimg.cn/d6dd520c2c1b4c8d9e6396369b5b1f56.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>举个栗子，输入一个神奇宝贝图片，输出：他是可达鸭么？ <strong>这是一个典型的二分问题，因为结果只有两种 是还是不是。</strong><br>举个栗子，假设有两个类别（水系和普通系），每个类别有不同的精灵，现在我抓到一个精灵，那么它是属于水系和普通系的概率分别是多少。这也是个<strong>典型的二分问题</strong>。</p><h2 id="如何解决分类问题？"><a href="#如何解决分类问题？" class="headerlink" title="如何解决分类问题？"></a>如何解决分类问题？</h2><h3 id="如何解决二分问题？"><a href="#如何解决二分问题？" class="headerlink" title="如何解决二分问题？"></a>如何解决二分问题？</h3><h4 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h4><p>对于二分类问题，定义一个function也就是数学model，<strong>当输出函数值大于0就划分为<em>类别1</em>，否则就为<em>类别2</em>.</strong> <strong>而损失函数定义为在测试数据上误分类的次数</strong>。<img src="https://img-blog.csdnimg.cn/3ee112c779004e5da28ca9878930b369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="那这个function我到底怎么定义呢？"><a href="#那这个function我到底怎么定义呢？" class="headerlink" title="那这个function我到底怎么定义呢？"></a>那这个function我到底怎么定义呢？</h4><p>实际上，可以将其定义为一个<strong>概率模型</strong>。<br>它可以是一个<strong>条件概率模型 <em>P(C1 | X)</em></strong>,当  <strong><em>P(C1 | X) &gt; 0.5</em></strong> ,比如在神奇宝贝二分问题中，我们定义X是这张图片的参数，而C1表示是可达鸭，整个的意思就变为了在这些图片参数的条件下这张图片是可达鸭的概率是多少，概率大于一半，说明确实很有可能就是可达鸭。或者对于第二个栗子，同样的可以规定<strong>条件概率模型 <em>P(C2 | X)</em></strong>，表示在捕捉了一个精灵后，他的参数条件下，是水系的概率是多少？（这边C2表示，捕捉的是水系）</p><p>这边我们以栗子2为例，假如我们捕捉了一只神奇宝贝(其实他就是可达鸭)，问他是水系的概率是多少？（理论上其实，他就是水系的，但机器需要通过概率论去推，需要包括以下的概率推导）<br><img src="https://img-blog.csdnimg.cn/dfa3d538674d4dcd8cb32ccbd52ea7f2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>如上图所示： 　 x就表示是可达鸭<br>先验概率：这里是P(C1)——训练样本中的精灵是水系的概率;同理P(C2)——训练样本中的精灵是普通系的概率<br>——————————————————————————————————————————————————————————————————<br><em>P(x | C1)</em>：这里是指在水系中是可达鸭的概率<br><em>P(x | C2)</em>：这里是指在普通系中是可达鸭的概率<br>两者可以用<strong>最大似然估计法</strong>求出——————————————————————————————————————————————————————————————————<br>后验概率：这里指抓到的神奇宝贝可达鸭是水系的概率。其用贝叶斯公式算出，公式如上图所示</p></blockquote><h5 id="求先验概率-P-C1-：训练样本中的精灵是水系的概率"><a href="#求先验概率-P-C1-：训练样本中的精灵是水系的概率" class="headerlink" title="求先验概率 P(C1)：训练样本中的精灵是水系的概率"></a>求先验概率 P(C1)：训练样本中的精灵是水系的概率</h5><p>根据训练样本，分别算出水系和普通系的概率<br><img src="https://img-blog.csdnimg.cn/67108f6b24044efbb896cb62ac2bd2c0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h5 id="选择概率分布函数，用最大似然估计出分布函数的参数（就是调参）"><a href="#选择概率分布函数，用最大似然估计出分布函数的参数（就是调参）" class="headerlink" title="选择概率分布函数，用最大似然估计出分布函数的参数（就是调参）"></a>选择概率分布函数，用最大似然估计出分布函数的参数（就是调参）</h5><blockquote><p>注：</p><ul><li>当样本数据x取实数值时，采用正太分布(高斯分布)</li><li>当每种特征的数值都在0-1内时，采用伯努利分布</li><li>当每种特征取值在{1, 2 , 3 , …，K}，采用多项式分布（Multinomial Distribution）</li></ul></blockquote><ol><li>　首先，我们目标是求水系样本中的79个精灵中，抓到其中一种神奇宝贝的概率 <strong>P(x | C1)</strong> ，那么这个概率应该是跟精灵的属性有关的。<br>这里我们选择<strong>两种属性（物防和法防）讨论</strong>，此时数据中（x,水系）中的x应该是一个神奇宝贝<strong>向量（[x1物防，x2法防] , 水系）。</strong><br><img src="https://img-blog.csdnimg.cn/4eaa50360e3e4c8492b900326980f697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>2、然后，这里选择正太分布（二维）：也就是说在水系样本中的79个精灵中，抓到其中一种精灵的概率 <em>P( x | C1)</em> 呈正太分布。<br><img src="https://img-blog.csdnimg.cn/86f5ab53fd124a47918b1da54cef4c3a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>接着，用最大似然估计法算出分布函数的参数<br>似然函数<em>L</em>：x1,x2…x79同时出现的概率函数。<br>最大似然估计：使似然函数最大时的参数估计。<img src="https://img-blog.csdnimg.cn/8ea6f6da88ba4657999c75674b70170b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>调整的参数<br><img src="https://img-blog.csdnimg.cn/528ff3d528f04fcb8a799f08f3712f4a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><h5 id="最后求出后验概率-即P-C1-x-抓到的可达鸭是水系的概率"><a href="#最后求出后验概率-即P-C1-x-抓到的可达鸭是水系的概率" class="headerlink" title="最后求出后验概率 即P(C1 | x): 抓到的可达鸭是水系的概率"></a>最后求出后验概率 即P(C1 | x): 抓到的可达鸭是水系的概率</h5>利用的就是贝叶斯公式。<br>整体每个部分的逻辑可以看下图所示：<img src="https://img-blog.csdnimg.cn/e1d5794c439749dc8e2286bfe3af4ea3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><h4 id="效果不好的解决方法"><a href="#效果不好的解决方法" class="headerlink" title="效果不好的解决方法"></a>效果不好的解决方法</h4><strong>实验结果</strong><br><img src="https://img-blog.csdnimg.cn/67640ca1732f41189f0bc43688a71a0e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>分类后，准确率并不高。理论上考虑其他因素，即增加维度可以增大准确率。但效果还是不佳，这该怎么办。<br><img src="https://img-blog.csdnimg.cn/841b72c2ccb243df9a91141dc200370f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>实验结果<br><img src="https://img-blog.csdnimg.cn/0b0ee16c14424df59cf27c80a49a0e1b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><h2 id="生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出"><a href="#生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出" class="headerlink" title="生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出"></a>生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出</h2><h3 id="回到如何用机器学习的三大步骤解决分类问题："><a href="#回到如何用机器学习的三大步骤解决分类问题：" class="headerlink" title="回到如何用机器学习的三大步骤解决分类问题："></a>回到如何用机器学习的三大步骤解决分类问题：</h3><img src="https://img-blog.csdnimg.cn/b62ddf0753f84cce856bf1991f2d1f81.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ol><h3 id="逻辑回归方法（判别模型）的引出"><a href="#逻辑回归方法（判别模型）的引出" class="headerlink" title="逻辑回归方法（判别模型）的引出　"></a>逻辑回归方法（判别模型）的引出　</h3><p><img src="https://img-blog.csdnimg.cn/5b57d3ebf742485eb514f106dba3de5a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="化简推到一下（纯数学）"><a href="#化简推到一下（纯数学）" class="headerlink" title="化简推到一下（纯数学）"></a>化简推到一下（纯数学）</h3><p><img src="https://img-blog.csdnimg.cn/f2323d6bd8474f5891cf63af3f18c47e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/e8faa78abb0641adae81388745601c13.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>所以其实不用考虑，N1，N2，μ1，μ2，∑的值。 直接就是w和b两个参数，这也是为什么之前，我们将∑按权值分配后，图像由线性分类的原因。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;分类问题 Classification&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;概念：（从概率生成模型到判别模型）&lt;br&gt;概率生成模型：由数据学习联合概率密度分布 &lt;em&gt;P(X,Y)&lt;/em&gt; ，然后求出条件概率分布&lt;em&gt;P(Y|X</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="分类问题" scheme="http://example.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降算法 进阶</title>
    <link href="http://example.com/2021/08/04/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%20%E8%BF%9B%E9%98%B6/"/>
    <id>http://example.com/2021/08/04/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%20%E8%BF%9B%E9%98%B6/</id>
    <published>2021-08-04T05:16:06.000Z</published>
    <updated>2021-08-04T12:49:04.332Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="梯度算法 进阶">TOC</a></p><h1 id="回顾梯度算法"><a href="#回顾梯度算法" class="headerlink" title="回顾梯度算法"></a>回顾梯度算法</h1><p><strong>是一种迭代的算法，每看一个参数都会更新。</strong><br><img src="https://img-blog.csdnimg.cn/eee493cfe8f24755a2aa3cfd41f486f4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/62ccf5ddd84348308c79a70233b0da19.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="学习率η（Learning-Rate）"><a href="#学习率η（Learning-Rate）" class="headerlink" title="学习率η（Learning Rate）"></a>学习率η（Learning Rate）</h1><h2 id="自定义的学习率对参数选择的影响"><a href="#自定义的学习率对参数选择的影响" class="headerlink" title="自定义的学习率对参数选择的影响"></a>自定义的学习率对参数选择的影响</h2><p><img src="https://img-blog.csdnimg.cn/6c39ce49d85048098ac7303a552bc184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>学习率需要选取合适的值，过小会导致模型调参的速度太慢，但过大会导致错失掉了最佳参数点</strong></p><h2 id="如何调整学习率η？"><a href="#如何调整学习率η？" class="headerlink" title="如何调整学习率η？"></a>如何调整学习率η？</h2><ol><li><p>在最开始的时候，随机点离目标点很远，我们一般会选取一个比较大的学习率；当做了几期后，我们离目标点很近了，所以我们会减小学习率 缩减为 如下图所示的公式<img src="https://img-blog.csdnimg.cn/b9bc45580c004a6cbb708595125b1a1a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></li><li><p><strong>不同的参数应该设置不同的学习率</strong></p></li></ol><p>下面是常用的 调整学习率的方法：</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Adagrad是解决不同参数应该使用不同的更新速率的问题。<strong>Adagrad自适应地为各个参数分配不同学习率的算法。</strong></p><blockquote><p><strong>其原理为：</strong><img src="https://img-blog.csdnimg.cn/4423c0afa60f462cb444f37cd1307576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>里面比较核心的部分在 每次σ的取值。<strong>每次σ规定为之前所有<em>g</em>平方对应的之和的均方根</strong>。例如下图所示：其中 <em>g</em> 是每次的偏导值<img src="https://img-blog.csdnimg.cn/82780afd2b0347ad8505db41024b9f7d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>结合之前说的 <em>η</em> 值的变化 可以得到如下的公式推导：<img src="https://img-blog.csdnimg.cn/cf79f34d94b44c72abdd283ebb0f3431.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><p><strong>提问:</strong>    发现一个现象，本来应该是随着gradient的增大，我们的学习率是希望增大的，也就是图中的g上标t；但是与此同时随着gradient的增大，我们的分母是在逐渐增大，也就对整体学习率是减少的，这是为什么呢？<img src="https://img-blog.csdnimg.cn/64154f52e91f4486be92171800f09815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这是因为随着我们更新次数的增大，我们是希望我们的学习率越来越慢。因为我们认为在学习率的最初阶段，我们是距离损失函数最优解很远的，随着更新的次数的增多，<strong>我们认为越来越接近最优解，于是学习速率也随之变慢。</strong>  为什么化简之后是如上这个式子呢，其在图像上的意义是 一阶导数比上二阶数的值。如下图所示：<img src="https://img-blog.csdnimg.cn/90fda3afedce4958a877451546c006e2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="Stochastic-Gradient-Descent（SGD随机梯度下降法）-？？？"><a href="#Stochastic-Gradient-Descent（SGD随机梯度下降法）-？？？" class="headerlink" title="Stochastic Gradient Descent（SGD随机梯度下降法） ？？？"></a>Stochastic Gradient Descent（SGD随机梯度下降法） ？？？</h3><p>其和 普通的梯度下降算法区别在。普通遍历在求和的时候需要浪费大量时间，进而去掉求和产生了随机梯度下降算法。<img src="https://img-blog.csdnimg.cn/e92e87942f4443cbac50e52369c68151.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>和下图看到的一样：<br><img src="https://img-blog.csdnimg.cn/76673f0f26614df6a9ac37311fe015b1.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p> 只是要注意一下标准的梯度下降和随机梯度下降的区别：</p><ol><li><p>标准下降时在权值更新前汇总所有样例得到的标准梯度，随机下降则是通过考察每次训练实例来更新（<strong>就是随机选择一些按顺序的后续样本点来，而不是全体数据</strong>）。<img src="https://img-blog.csdnimg.cn/32da14cad9bf4a2ba439745929b7ec11.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></li><li><p>对于步长 <em>η</em> 的取值，标准梯度下降的 <em>η</em> 比随机梯度下降的大。因为标准梯度下降的是使用准确的梯度，理直气壮地走，随机梯度下降<strong>使用的是近似的梯度</strong>，就得小心翼翼地走，怕一不小心误入歧途南辕北辙了。</p></li><li>当损失函数有多个局部极小值时，随机梯度反而更可能避免进入局部极小值中。</li></ol></blockquote><h4 id="小批量随机梯度下降（batch-gradient-descent）"><a href="#小批量随机梯度下降（batch-gradient-descent）" class="headerlink" title="小批量随机梯度下降（batch gradient descent）"></a>小批量随机梯度下降（batch gradient descent）</h4><p>如果在每次迭代中，梯度下降是用整个训练数据集来计算梯度的话，则会带来大量的计算量。因此提出批量梯度下降来进行优化。<strong>每次优化不再是对整体数据集来计算损失，取而代之使用随机采样小批量的样本来计算梯度。</strong><img src="https://img-blog.csdnimg.cn/0d8e8d1e6c234e36b0972e5b95c9f46a.png#pic_center" alt="在这里插入图片描述"></p><h3 id="Feature-Scaling-（特征缩放）"><a href="#Feature-Scaling-（特征缩放）" class="headerlink" title="Feature Scaling （特征缩放）"></a>Feature Scaling （特征缩放）</h3><p>其意思就是说要<strong>将所有特征有相同的规模</strong>。例如下图所示，<em>X2</em> 的范围明显大宇 <em>X1</em>，所以要将 <em>X2</em> 进行缩放。<br><img src="https://img-blog.csdnimg.cn/2b4681ef27884c0bbf400db0624e8740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="为什么要做Feature-Scaling？"><a href="#为什么要做Feature-Scaling？" class="headerlink" title="为什么要做Feature Scaling？"></a>为什么要做Feature Scaling？</h4><blockquote><p>为什么要做Feature Scaling？<img src="https://img-blog.csdnimg.cn/456a5666c6bf40b7a1e5f7a8727b529f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如左图，<em>X1</em> 和 <em>X2</em> 数值规模大小相差很大，那 <em>W2</em> 这参数的变动会极大的影响损失函数，而 <em>W1</em> 影响度就很小。所以我们<strong>需要对 <em>X2</em> 进行特征缩放，使其与 <em>X1</em> 保持一个规模</strong>。并且其实从图像可以看出，如果按左图来做的话，我们很难找到一组合适的参数集合，因为他梯度下降的方法不是直线的而是曲线；而右图是近乎直线。（<strong>最里面圈的损失函数最小</strong>）</p></blockquote><h4 id="如何实现Feature-Scaling？"><a href="#如何实现Feature-Scaling？" class="headerlink" title="如何实现Feature Scaling？"></a>如何实现Feature Scaling？</h4><p><img src="https://img-blog.csdnimg.cn/f129b6a000c841ec926df684d2c77a15.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>如上图所示，假如有R个数据样本，每个样本有<em>X1 - Xi</em> 个特征。我们用上面的公式计算出其应该的scale（其实说实在的这不就是化成标准正太分布么）</p><h1 id="梯度下降算法数学总结"><a href="#梯度下降算法数学总结" class="headerlink" title="梯度下降算法数学总结"></a>梯度下降算法数学总结</h1><p>提出问题：如下图所示<strong>（我怎么样才能在红圈里找到最小损失的那个点呢）</strong><img src="https://img-blog.csdnimg.cn/53c25201a11349b6bc3af9e1dfc9fcc6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="首先要回顾泰勒公式"><a href="#首先要回顾泰勒公式" class="headerlink" title="首先要回顾泰勒公式"></a>首先要回顾泰勒公式</h2><p><img src="https://img-blog.csdnimg.cn/c9b154d4d14c43c8bf5e2609789bb094.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/205bb83b97d849fc9b8c7451e044fdab.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>二元泰勒：<br><img src="https://img-blog.csdnimg.cn/2efa8bf67ffa419e90221154515a3712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="用泰勒展开损失函数"><a href="#用泰勒展开损失函数" class="headerlink" title="用泰勒展开损失函数"></a>用泰勒展开损失函数</h2><p><img src="https://img-blog.csdnimg.cn/3ce8a2749e644d1f978c43d485798dde.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>理解为点乘，反向180度的时候，损失函数才是最小的：<img src="https://img-blog.csdnimg.cn/235fc97c36b74012b0663975ba0dc8a9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>最终可以表达成：<img src="https://img-blog.csdnimg.cn/83f0037797f64c048376868ee2c21fae.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="梯度下降算法缺点在哪"><a href="#梯度下降算法缺点在哪" class="headerlink" title="梯度下降算法缺点在哪"></a>梯度下降算法缺点在哪</h1><p>其不仅仅包括可能有找到是局部最优解问题，有可能在中间的时候就有偏微分为0的时候，而这时这个点可能离全局最优解点 很远。<br><img src="https://img-blog.csdnimg.cn/cd31675e6d8e4c83925852218d4f2acc.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;梯度算法 进阶&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;回顾梯度算法&quot;&gt;&lt;a href=&quot;#回顾梯度算法&quot; class=&quot;headerlink&quot; title=&quot;回顾梯度算法&quot;&gt;&lt;/a&gt;回顾梯度算法&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;是一种迭代的算法，每看一</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>模型误差的来源分析</title>
    <link href="http://example.com/2021/08/02/%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/2021/08/02/%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90/</id>
    <published>2021-08-02T05:16:06.000Z</published>
    <updated>2021-08-04T12:48:42.267Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="模型误差的来源分析">TOC</a></p><blockquote><p>思路：<br>1、首先，误差的两大来源 <em>bias</em>（偏差） 和 <em>variance</em>（方差） 是指什么<br>2、然后，<em>bias</em>（偏差）和 <em>variance</em>（方差）是怎么产生的<br>3、进一步，如何判断你的模型是 <em>bias大</em>（欠拟合）还是<em>variance大</em>（过拟合），如何解决 ？</p></blockquote><h1 id="模型的两大来源bias（偏差）和variance（方差）"><a href="#模型的两大来源bias（偏差）和variance（方差）" class="headerlink" title="模型的两大来源bias（偏差）和variance（方差）"></a>模型的两大来源bias（偏差）和variance（方差）</h1><blockquote><p>什么是bias和variance呢？<br>思路：</p><ol><li>首先要知道什么是误差</li><li>在了解什么是bias和variance</li></ol></blockquote><h2 id="什么是误差？"><a href="#什么是误差？" class="headerlink" title="什么是误差？"></a>什么是误差？</h2><p><strong>机器学习就是寻找一个函数，然后给它一个输入，就能得到一个理想的输出。</strong><br>f head是理论上找到的最佳函数，f star 是我们用模型预测出来的函数，<strong>两者的差值就是误差</strong>。<img src="https://img-blog.csdnimg.cn/39bb552a5ba348b88a8882c744f74566.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="什么是bias和variance（偏差和方差）"><a href="#什么是bias和variance（偏差和方差）" class="headerlink" title="什么是bias和variance（偏差和方差）"></a>什么是bias和variance（偏差和方差）</h2><h3 id="什么是bias（偏差）？"><a href="#什么是bias（偏差）？" class="headerlink" title="什么是bias（偏差）？"></a>什么是bias（偏差）？</h3><p>举个栗子说明，下图是用一定样本数的均值m来估计假设的随机变量的<strong>平均值u</strong>，这是一种<strong>无偏估计（unbiased）</strong>。<img src="https://img-blog.csdnimg.cn/eaeb2438d6a645d7b1236e6a9df59da8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>也就是说，<strong>估计值的期望等于假设值</strong>（如上文的E(m)=u），<strong>即为无偏差，反之有偏差（bias）</strong>。<br><strong>当样本数越来越大时，m就越靠近u。</strong></p><h3 id="什么是variance（方差）"><a href="#什么是variance（方差）" class="headerlink" title="什么是variance（方差）"></a>什么是variance（方差）</h3><p><strong>方差表达的是数据的离散程度</strong><img src="https://img-blog.csdnimg.cn/28ad8de1e99b4c91a3f16a1088968ca4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里对于方差的估计是有差估计：<img src="https://img-blog.csdnimg.cn/c9ee7e91fff24cd498b5a21420ad1ed5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="总结直观理解"><a href="#总结直观理解" class="headerlink" title="总结直观理解"></a>总结直观理解</h3><p>最后，bias和variance的直观理解：<img src="https://img-blog.csdnimg.cn/357d51b0a7f34b06933c5a86c042c8dc.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ol><li>bias表示的是预测的f bar（f star预测值的期望）与f head（实际正确值）的距离</li><li>variance表示的是每次预测的f star（预测值）与f bar（预测值的期望）的距离（看图）</li></ol><h2 id="bias和variance是怎么产生的"><a href="#bias和variance是怎么产生的" class="headerlink" title="bias和variance是怎么产生的"></a>bias和variance是怎么产生的</h2><blockquote><p>下面结合实际的实验说明，bias和variance是如何产生的。<br>————————————————————————————————————————————————————<br>bias如何知晓？，就要做多次实验，确定多个f star（一次实验一个预测值），然后求出f star 样本集的期望（E(f star)）<br>那么首先，我们虚拟出100个神奇宝贝平行宇宙（相当于设置了100组实验），每个预祝<strong>一个神奇宝贝训练家捕捉10只神奇宝贝</strong>（相当于每组实验10个数据），如下图：<img src="https://img-blog.csdnimg.cn/ef3d6cfdd1ca4c179d091c2f5130a42c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>然后思考，对于这样的数据，<strong>我们选用什么model比较好</strong> ?<strong>哪一个model最后的bias比较小</strong>？<img src="https://img-blog.csdnimg.cn/c05149fd285d4c96bd182567c2d891fd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如上图，不同宇宙的神奇宝贝数据非常随机，项次越高，模型越复杂</p></blockquote><h3 id="方差-variance"><a href="#方差-variance" class="headerlink" title="方差 variance"></a>方差 variance</h3><p><img src="https://img-blog.csdnimg.cn/fea8cce3d3b64a768fbf007ddefe0021.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对于方差而言，其表示结果的离散程度，项次越高，模型越复杂，则其离散程度肯定是越大的。<strong>因为模型越简单，收到数据的影响也就越低</strong>，比如：我极端一点，f(x)=c，数据根本不会影响model最后的预测值</p><h3 id="偏差-bias"><a href="#偏差-bias" class="headerlink" title="偏差 bias"></a>偏差 bias</h3><p><img src="https://img-blog.csdnimg.cn/ade6e8be3e52487bafd38dd0f8e750a9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/759ad31af9b04b6a8cdeb92364abe4f7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>图中看出，简单model可能并不包含目标，因此会造成较大的bias，而复杂的model是涵盖目标的，所以bias小。</p><h3 id="偏差和方差的总结"><a href="#偏差和方差的总结" class="headerlink" title="偏差和方差的总结"></a>偏差和方差的总结</h3><blockquote><p>简单的模型（次数小），bias会比较大，但variance会比较小，预测值更加集中。<br>复杂的模型（次数大），bias会比较小，但variance会比较大，预测值更加的离散。<br>因此，我们理想中的目标是找到一个平衡点，使bias和variance尽可能小。<img src="https://img-blog.csdnimg.cn/c5a88735d7d94da79046122ca8310740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="判断你的模型是bias大（欠拟合）还是variance大（过拟合）并解决"><a href="#判断你的模型是bias大（欠拟合）还是variance大（过拟合）并解决" class="headerlink" title="判断你的模型是bias大（欠拟合）还是variance大（过拟合）并解决"></a>判断你的模型是bias大（欠拟合）还是variance大（过拟合）并解决</h2><h3 id="如何判断"><a href="#如何判断" class="headerlink" title="如何判断"></a>如何判断</h3><p><img src="https://img-blog.csdnimg.cn/c8150c9d246e4b10841294be51ab0f4a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h3 id="bias过大（欠拟合）"><a href="#bias过大（欠拟合）" class="headerlink" title="bias过大（欠拟合）"></a>bias过大（欠拟合）</h3><p>需要重新设计模型</p><ol><li>模型考虑的特征没有全，也就是很多其实没有作用。关键特征可能还没考虑到，需要加入进去</li><li>模型应该需要更加的复杂，增加更高的次项。</li></ol><h3 id="variance过大（过拟合）"><a href="#variance过大（过拟合）" class="headerlink" title="variance过大（过拟合）"></a>variance过大（过拟合）</h3><ol><li>需要更多的数据，有些数据不够有特点</li><li>可以增加一个 正则项，加强模型的平滑度，使其预测值分布不要太离散</li></ol><h3 id="解决实际测试比共有数据集误差更大的问题"><a href="#解决实际测试比共有数据集误差更大的问题" class="headerlink" title="解决实际测试比共有数据集误差更大的问题"></a>解决实际测试比共有数据集误差更大的问题</h3><p><img src="https://img-blog.csdnimg.cn/d742c18f9bc04a8ea7981b28a9c2970b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>将训练集，分为两部分。这叫做2-折交叉验证。一部分还是当做训练样本，帮助我们进行调参；另一部分用作验证，验证我的模型损失如何，是否合理（充当原来共有训练集的作用）。而现在原有训练集的部分用来充当实际样本数据，算出误差值，以便于真正在实际中误差太大。</p><p><img src="https://img-blog.csdnimg.cn/1c0fa7f4983a4dc585448876cab974d4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;模型误差的来源分析&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;思路：&lt;br&gt;1、首先，误差的两大来源 &lt;em&gt;bias&lt;/em&gt;（偏差） 和 &lt;em&gt;variance&lt;/em&gt;（方差） 是指什么&lt;br&gt;2、然后，&lt;em&gt;bias&lt;/em&gt;</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性回归Regression 案例一：宝可梦进化CP值</title>
    <link href="http://example.com/2021/07/31/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Regression%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E5%AE%9D%E5%8F%AF%E6%A2%A6%E8%BF%9B%E5%8C%96CP%E5%80%BC/"/>
    <id>http://example.com/2021/07/31/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Regression%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E5%AE%9D%E5%8F%AF%E6%A2%A6%E8%BF%9B%E5%8C%96CP%E5%80%BC/</id>
    <published>2021-07-31T03:16:06.000Z</published>
    <updated>2021-08-04T12:49:26.964Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="线性回归Regression 案例一：宝可梦进化CP值">TOC</a></p><blockquote><p>本栗子：预测Pokemon精灵攻击力。<br>输入：进化前的CP值、物种（Bulbasaur）、血量（HP）、重量（Weight）、高度（Height）<br>输出：进化后的CP值</p></blockquote><p><img src="https://img-blog.csdnimg.cn/img_convert/d53b2b95d30c7af9574c7a0bbaba2c82.png#pic_center" alt="在这里插入图片描述"></p><h1 id="实现回归的步骤（机器学习的步骤）"><a href="#实现回归的步骤（机器学习的步骤）" class="headerlink" title="实现回归的步骤（机器学习的步骤）"></a>实现回归的步骤（机器学习的步骤）</h1><h2 id="Step1-确定一个model-线性模型"><a href="#Step1-确定一个model-线性模型" class="headerlink" title="Step1 确定一个model - 线性模型"></a>Step1 确定一个model - 线性模型</h2><blockquote><p>输入的特征包括：进化前的CP值（<em>Xcp</em>）、物种（Bulbasaur）（<em>Xs</em>）、血量（HP）（<em>Xhp</em>）、重量（Weight）（<em>Xw</em>）、高度（Height）（<em>Xh</em>）</p></blockquote><p>先从简单的<strong>单个特征</strong> 进化前的CP值（<em>Xcp</em>）开始（<strong>后面改进再考虑多个特征</strong>）。<br><img src="https://img-blog.csdnimg.cn/img_convert/5b4df493600379c7aa0372e103eb7d79.png#pic_center" alt="在这里插入图片描述"></p><h2 id="Step2-goodness-of-function（函数优化）——损失函数"><a href="#Step2-goodness-of-function（函数优化）——损失函数" class="headerlink" title="Step2 goodness of function（函数优化）——损失函数"></a>Step2 goodness of function（函数优化）——损失函数</h2><ol><li>确定model后（案例为线性模型），开始训练数据（使用的为训练样本集）<img src="https://img-blog.csdnimg.cn/img_convert/ebd00bf7bee79841d6c35eac2865fd61.png#pic_center" alt="在这里插入图片描述"></li><li>训练10个训练样本后得到10个的预测进化CP值（<em>y</em>）如下图所示。左侧为训练样本本身真实的进化CP值，右侧为横轴为进化前CP值（<em>Xcp</em>）<img src="https://img-blog.csdnimg.cn/img_convert/fbba4351217b730cd11ca92a7427cd51.png#pic_center" alt="在这里插入图片描述"></li><li><strong>确定损失函数</strong>。损失函数用于评价一个模型的好坏。损失函数的值越小，那么模型越好。<strong>对于本案例，损失函数采用最简单的距离表示</strong>。即求<strong>实际进化后的CP值与模型预测的CP值差</strong>，来判定模型的好坏。<img src="https://img-blog.csdnimg.cn/img_convert/3abae7108681dbad20cc15a952d20bba.png#pic_center" alt="在这里插入图片描述"><ol><li>List item</li></ol></li></ol><h2 id="Step3-best-function（找出最好的一个函数-即调参）——使用梯度下降法"><a href="#Step3-best-function（找出最好的一个函数-即调参）——使用梯度下降法" class="headerlink" title="Step3 best function（找出最好的一个函数 即调参）——使用梯度下降法"></a>Step3 best function（找出最好的一个函数 即调参）——使用梯度下降法</h2><ol><li>案例采用<strong>梯度下降法</strong>来帮助选择 <em>w</em> 和 <em>b</em> 两个参数取何值时损失函数最小，也就意味着构建的模型越准确。 <img src="https://img-blog.csdnimg.cn/img_convert/7d75962cbe0b2dc3c54f370383912138.png#pic_center" alt="在这里插入图片描述"><blockquote><p>什么是梯度下降法？梯度指的是？<br>………………………………………………………………………………………………………<br>梯度？<br>在<strong>单变量的函数</strong>中，梯度其实就是对应点<strong>函数的微分</strong>，代表着这个函数在<strong>某个给定点的切线的斜率</strong><br>在<strong>多变量函数</strong>中，梯度是一个<strong>向量</strong>，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向  <strong>即例如二维就是 偏导 i + 偏导 j</strong><br>………………………………………………………………………………………………………<br>梯度下降法(SGD  Stochastic gradient descent）？答： “下山最快路径”的一种算法<br>我们是尝试使用偏导来衡量函数随自变量的值变化关系，选择变化更为平缓的那一处  对应的 <em>w</em> 和 <em>b</em> 作为调整后的参数</p></blockquote></li></ol><h3 id="一维视角：只考虑一个参数-w"><a href="#一维视角：只考虑一个参数-w" class="headerlink" title="一维视角：只考虑一个参数 w"></a>一维视角：只考虑一个参数 <em>w</em></h3><p><img src="https://img-blog.csdnimg.cn/3c72961665a942ac87f066a129a30183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>如上图  learning rate（学习率）：移动的步长 一般用η来表示<br>………………………………………………………………………………………………………<br>如何寻找一个最合适的 <em>w</em> ？<br>步骤1：随机在横轴上选取一个 <em>w0</em> 。<br>步骤2：计算微分，也就是当前的斜率，根据斜率来判定移动的方向。微分大于0应向右移动（增加 <em>w</em> ）；微分小于0向左移动（减少 <em>w</em> ）<br>步骤3：根据学习率，按步骤2得到的方向移动<br>重复步骤2和步骤3，直到找到<strong>最低点</strong>。横轴即对于的最佳 <em>w</em>  参数<br>………………………………………………………………………………………………………<br>如下图，是经过迭代多次后找到的最佳点（得是全局最优解） 。注：大部分损失函数都为正<br><img src="https://img-blog.csdnimg.cn/588186b690dd43c38a10a80600802ab0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="二维视角：考虑两个参数-w-和-b"><a href="#二维视角：考虑两个参数-w-和-b" class="headerlink" title="二维视角：考虑两个参数 w 和 b"></a>二维视角：考虑两个参数 <em>w</em> 和 <em>b</em></h3><p><img src="https://img-blog.csdnimg.cn/55f2f295750242bfa35f9ac576602194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如何寻找一个最合适的 <em>w</em> ？<br>步骤1：随机在横轴上选取一个 <em>w0</em> ，<em>b0</em> 。<br>步骤2：计算各偏导，也就是当前偏导的斜率，根据斜率来判定移动的方向。微分大于0应向右移动（增加 <em>w</em> 或 <em>b</em>）；微分小于0向左移动（减少 <em>w</em> 或 <em>b</em>）<img src="https://img-blog.csdnimg.cn/6a082b40869947c2a0723b298cd36027.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>步骤3：根据学习率，按步骤2得到的方向移动<br>重复步骤2和步骤3，直到找到<strong>最低点</strong>。横轴即对于的最佳 <em>w</em> 和 <em>b</em>  参数</p></blockquote><p>二维情况：梯度下降法的效果<br>颜色约深的区域代表的损失函数越小？ <strong>为什么呢？</strong><br><img src="https://img-blog.csdnimg.cn/299d0d615f6640a6ae092e51579d09ea.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="梯度算法的优缺点"><a href="#梯度算法的优缺点" class="headerlink" title="梯度算法的优缺点"></a>梯度算法的优缺点</h3><blockquote><p>总结一下梯度下降法：<br>　　　我们要解决使<strong>损失函数<em>L(w,b)</em></strong> 最小时参数的最佳值，梯度下降法是每次update参数值，直到损失函数最小时找到对应最佳的参数<em>w，b</em></p></blockquote><p><strong>缺点1</strong>：求解的未必是全局最优解，有可能是局部最优解。</p><blockquote><p><strong>用下山的例子讲</strong>：<br>　　　比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在<strong>每走到一个位置的时候</strong>，<strong>求解当前位置的梯度</strong>，沿着<strong>梯度的负方向</strong>，也就是<strong>当前最陡峭的位置向下走一步</strong>，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，<strong>有可能我们不能走到山脚，而是到了某一个局部的山峰低处。</strong><br>………………………………………………………………………………………………………<br>　　　从上面的解释可以看出，<strong>梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解</strong>。当然，如果损失函数是<strong>凸函数</strong>，梯度下降法得到的解就<strong>一定是</strong>全局最优解。</p></blockquote><p><strong>优点1</strong>：无论随机从哪个点出发，得到的最佳参数一定是同一组</p><blockquote><p><strong>用下山的例子讲</strong>：<br>　　　因为山只有一处是最低点，在确保能找到全局最优解的情况下，无论人从山上哪一点出发，一定能找到这特定一处的最低洼处。</p></blockquote><h2 id="Step4-回归结果分析"><a href="#Step4-回归结果分析" class="headerlink" title="Step4 回归结果分析"></a>Step4 回归结果分析</h2><blockquote><p>经过上述三个步骤后，得到了<strong>训练后</strong>的“最佳参数<em>w</em>，<em>b</em>”，那么现在这个模型在<strong>测试集</strong>上是什么表现呢？<br>得到的结果是：<br><strong>测试集的误差比在训练集上得到的损失值大</strong> 这个事非常正常。因为你训练集里面可能有些数据是不典型的，同样测试集中很多也包含了训练集中没有的因素。所以一个函数模型在实际应用中，效果基本上时打折扣的。<strong>一般会采用两种方式来加强这个函数模型</strong>：<br>　　　select another model（选择另一个模型）即增加维度，增加高此项多项式<br>　　　consider the hidden factors（考虑其他隐藏因素）</p></blockquote><h3 id="优化模型方法一：增加高次项（一般用于拟合度不够的情况）"><a href="#优化模型方法一：增加高次项（一般用于拟合度不够的情况）" class="headerlink" title="优化模型方法一：增加高次项（一般用于拟合度不够的情况）"></a>优化模型方法一：增加高次项（一般用于拟合度不够的情况）</h3><p><strong>回到Step1</strong></p><ol><li>尝试二次项、三次项、四次项、五次项…… 右上图为训练集损失，下图是测试集的损失。第五次<strong>过拟合</strong>导致了，测试集损失度很高。<img src="https://img-blog.csdnimg.cn/c707638cef5c4d698813e6acfe4ec172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/ce26490bbfc64ea6967cd7b8e1e8184d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/166aef516df14c339c45fee48a78e16a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/87e7491ce6a1464dad4c023dc9004274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li><strong>选择合适的模型</strong>，即我到底要加到几次项呢？</li></ol><blockquote><p>越复杂的模型所包含的函数也就越多，那么它包含理想模型的可能性也就越大，但是如果过分地去拟合理想模型，就会出现<strong>过拟合</strong>的情况。<br><img src="https://img-blog.csdnimg.cn/6a394ba6b4a648a994472d8969668601.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><ol><li>横向比较各个多项次，训练集和测试集的失误。理论上，失误会随着高次项而变小，如果变大，则表示模型<strong>过拟合</strong>了。<img src="https://img-blog.csdnimg.cn/979417ba98f6475d941753e0cb71b91c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>所以，如上图所示，在为3次的时候，训练集和测试集的误差差值最小。并且4次开始，以及存在过拟合现象了。因此本案例可选择3次项线性模型。</li></ol><h3 id="优化模型方法二：考虑其他隐藏因素"><a href="#优化模型方法二：考虑其他隐藏因素" class="headerlink" title="优化模型方法二：考虑其他隐藏因素"></a>优化模型方法二：考虑其他隐藏因素</h3><p><strong>回到Step1</strong></p><blockquote><p>输入的特征包括：进化前的CP值（<em>Xcp</em>）、物种（Bulbasaur）（<em>Xs</em>）、血量（HP）（<em>Xhp</em>）、重量（Weight）（<em>Xw</em>）、高度（Height）（<em>Xh</em>）<br>除了之前考虑的进化前的CP值（<em>Xcp</em>），其他均为隐藏因素<br>………………………………………………………………………………<br><strong>这里另外考虑的是神奇宝贝的种类</strong>？因为，有时候一个模型恰恰只能符合一种神奇宝贝，也就是<strong>不同神奇宝贝应该有不同的预测模型</strong>。<img src="https://img-blog.csdnimg.cn/72a4aadb2ba24aefb4584a8a8ca2801e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><ol><li>不同的神奇宝贝有不同的预测模型<img src="https://img-blog.csdnimg.cn/46fe5297bddb46a4bd511a7cea422e3a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>拟合这个模型的曲线，可以看出不同神奇宝贝拟合成了不同的类线性直线<img src="https://img-blog.csdnimg.cn/7a55f95269d642038715e355437600f2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>如果还考虑了其他隐藏元素。如图是横轴是对应的隐藏元素，纵轴是对应进化后的CP值。<img src="https://img-blog.csdnimg.cn/591a6aa5537443428687ba15886ab11d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>如果都采用最高二次项，考虑所有其他的隐藏因素。该案例的数学模型就变成 如图所示的式子。<img src="https://img-blog.csdnimg.cn/0a80407d715e4dcc978bd86dee3451a3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><blockquote><p> 考虑更多的因素反而出现了过拟合的问题，<strong>说明有些因素跟本次实验的CP值预测没有关系</strong>！<br>………………………………………………………………………………<br><strong>过拟合这么讨厌，到底如何减少过拟合问题呢？往下看！</strong></p></blockquote></li></ol><h3 id="优化模型：防止过拟合（为损失函数加一个正则项）"><a href="#优化模型：防止过拟合（为损失函数加一个正则项）" class="headerlink" title="优化模型：防止过拟合（为损失函数加一个正则项）"></a>优化模型：防止过拟合（为损失函数加一个正则项）</h3><h4 id="正则项是什么"><a href="#正则项是什么" class="headerlink" title="正则项是什么?"></a>正则项是什么?</h4><blockquote><p><strong>方法：正则化</strong>?<br>比如先考虑一个参数w，正则化就是在损失函数上加上一个与w（斜率）相关的值（正则项），那么要是loss function越小的话，w也会越小，w越小就使function更加平滑（function没那么大跳跃）<img src="https://img-blog.csdnimg.cn/7a87a38ed54c438c8af37860e982f775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="正则化的缺点"><a href="#正则化的缺点" class="headerlink" title="正则化的缺点"></a>正则化的缺点</h4><p>正则化虽然能够减少过拟合的现象，但是因为加在损失函数后面的值是平白无故加上去的，所以正则化过度的话会导致<strong>bias偏差增大</strong>   ？？？？<img src="https://img-blog.csdnimg.cn/f4c9ea7efb144c8a89b7fee964b75fa8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>1）实现参数的稀疏有什么好处吗？<br>一个好处是可以简化模型，避免过拟合。<strong>因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据表现性能极差</strong>。另一个好处是参数变少可以使整个模型获得更好的可解释性。<br>2）参数值越小代表模型越简单吗？<br>是的。为什么参数越小，说明模型越简单呢，这是因为越复杂的模型，<strong>越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点</strong>，这就容易造成在<strong>较小的区间里预测值产生较大的波动</strong>，这种较大的波动也反映了在这个区间里的<strong>导数很大</strong>，而<strong>只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。</strong></p></blockquote><p><img src="https://img-blog.csdnimg.cn/9dcfac9366564857a226de8d73bf512e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;线性回归Regression 案例一：宝可梦进化CP值&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本栗子：预测Pokemon精灵攻击力。&lt;br&gt;输入：进化前的CP值、物种（Bulbasaur）、血量（HP）、重量（Weight）、高度（H</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性回归" scheme="http://example.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
