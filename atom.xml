<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只柴犬</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-08-07T09:03:41.534Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>凯凯超人</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习 Deep Learning 基础</title>
    <link href="http://example.com/2021/08/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2021/08/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E5%9F%BA%E7%A1%80/</id>
    <published>2021-08-07T09:03:01.000Z</published>
    <updated>2021-08-07T09:03:41.534Z</updated>
    
    <content type="html"><![CDATA[<p>@[TOC](深度学习 Deep Learning 基础)</p><p><strong>深度学习需要明白的几个问题？</strong><br>思路：</p><ol><li>什么是深度学习？为什么需要深度学习？深度学习和机器学习的关系？</li><li>深度学习的步骤</li><li>确定神经网络模型的损失函数，如何优化模型，即调参问题</li><li>如何使用Back Propagation（反向传播） 方法 update DNN（深度神经网络）参数</li></ol><h1 id="深度学习的概念"><a href="#深度学习的概念" class="headerlink" title="深度学习的概念"></a>深度学习的概念</h1><h2 id="什么是深度学习？"><a href="#什么是深度学习？" class="headerlink" title="什么是深度学习？"></a>什么是深度学习？</h2><p>深度学习（Deep Learning，DL）是指多层的人工神经网络和训练它的方法。一层神经网络会把大量矩阵数字作为输入，通过<strong>非线性激活方法</strong>取权重，再产生另一个数据集合作为输出。这就像生物神经大脑的工作机理一样，通过合适的矩阵数量，多层组织链接一起，形成神经网络“大脑”进行精准复杂的处理，就像人们识别物体标注图片一样。</p><ul><li>深度学习的model是一个深度神经网络结构（neural structure）</li><li>深度学习的“深度”是指神经网络的隐层（hidden layer）数量足够多</li><li>深度学习是<strong>自动提取特征</strong>（Feature extractor），<strong>不需要像逻辑回归那样特征转换</strong>（Feature engineering）</li></ul><h2 id="为什么需要深度学习？-深度学习和机器学习的关系？"><a href="#为什么需要深度学习？-深度学习和机器学习的关系？" class="headerlink" title="为什么需要深度学习？ 深度学习和机器学习的关系？"></a>为什么需要深度学习？ 深度学习和机器学习的关系？</h2><ul><li>传统机器学习的模型结构较简单，很依赖算法工程师做特征工程甚至子模型来提升模型效果。就像我们之前上节那个栗子一样，做多分类的问题，四个角对角是一个类的情况，没办法进行分类，所以只能使用特征工程来进行特征的变换。</li><li>深度学习由于其层次化的结构，理论上可以拟合任意函数，整个复杂结构即可以用来对特征进行自动组合（如图像），也可以用来构建复杂的模型（如nlp领域里的LSTM，能够考虑上下文）。</li></ul><p><img src="https://img-blog.csdnimg.cn/b8b8df57f814494d8199e0e7b708e597.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="深度学习的步骤"><a href="#深度学习的步骤" class="headerlink" title="深度学习的步骤"></a>深度学习的步骤</h1><p>其实和机器学习一样分为三步。<img src="https://img-blog.csdnimg.cn/514f075664384cdcb62f6062fd5bd722.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="Step-1：定义一个神经网络结构（neural-structure）"><a href="#Step-1：定义一个神经网络结构（neural-structure）" class="headerlink" title="Step 1：定义一个神经网络结构（neural structure）"></a>Step 1：定义一个神经网络结构（neural structure）</h2><p>神经网络的创建包括3部分：</p><ol><li>神经网络有多少<strong>隐层（layer）</strong></li><li>每一层有多少<strong>神经元（neuron）</strong></li><li>每个神经元之间如何连接</li></ol><p>常见的出名神经网络<img src="https://img-blog.csdnimg.cn/5394cd49b44545c8ab4b8c1191d69239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="神经元怎么定义？"><a href="#神经元怎么定义？" class="headerlink" title="神经元怎么定义？"></a>神经元怎么定义？</h3><p>每个神经元都有一个 bias 和一个 function ，每条输入的边都有一个 weight<img src="https://img-blog.csdnimg.cn/52bc001e984c4bdf980921c7f2458fdf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="一个神经网络的栗子"><a href="#一个神经网络的栗子" class="headerlink" title="一个神经网络的栗子"></a>一个神经网络的栗子</h3><p>下面是一个 3个隐层（layer）、六个神经元（neuron）（每个球都是一个神经元）、全连接<strong>前馈</strong>网络（Fully Connection Feedforward Network）</p><blockquote><p>“<strong>前馈</strong>”是指整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示<br>其实我们常用的网络，都是前馈神经网络，从输入到输出是一个有向图，中间不会有环或者反向传播。<br>当然，我们在训练前馈神经网络的时候，会用到反向传播进行参数调整。但仍不影响整个网络的有向和前馈性质。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/748979f3a3c74e0ab0c19ea60bd13142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="神经网络如何工作？"><a href="#神经网络如何工作？" class="headerlink" title="神经网络如何工作？"></a>神经网络如何工作？</h3><p><img src="https://img-blog.csdnimg.cn/d685eb4da45646df93096525226bb59f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其实 这就是矩阵运算（可以使用GPU加速计算）<img src="https://img-blog.csdnimg.cn/34294cf6a4254049b0f8bbc0aaf631a1.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>总体可以归纳为：<br><img src="https://img-blog.csdnimg.cn/9baa5b003ea04ff7ad3bc25076e2bb58.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="栗子：识别手写数字图像"><a href="#栗子：识别手写数字图像" class="headerlink" title="栗子：识别手写数字图像"></a>栗子：识别手写数字图像</h3><p>从图像中识别是数字几？<br><img src="https://img-blog.csdnimg.cn/5fb1bc809a444ff5bc19325453eb703d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="制作神经网络模型-的FAQ（最容易被问得问题）"><a href="#制作神经网络模型-的FAQ（最容易被问得问题）" class="headerlink" title="制作神经网络模型 的FAQ（最容易被问得问题）"></a>制作神经网络模型 的FAQ（最容易被问得问题）</h3><ul><li>Q1： 神经网络需要几个隐层（layers）？每个层需要多少个神经元呢？<ul><li>反复试验+ 直觉 （说白了就是要慢慢试= = 看经验呗）</li></ul></li><li>Q2：神经网络可以自动确定结构吗？<ul><li>理论上其实是可以的，不过这些方法我们还没学到。Evoluntionary Artifical Neural Networks</li></ul></li><li>Q3：我们可以设计层次之间的结构么？<ul><li>意思就是说例如Layer1 链接 Layer3 这样跳着的 等等。当然可以 CNN就是不按顺序来的，具体看下一节。<br><img src="https://img-blog.csdnimg.cn/6c74000cf96a4af0b33146e9dca6efc3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul></li></ul><h2 id="Step-2：确定神经网络模型的损失函数"><a href="#Step-2：确定神经网络模型的损失函数" class="headerlink" title="Step 2：确定神经网络模型的损失函数"></a>Step 2：确定神经网络模型的损失函数</h2><p>还是和逻辑回归一样，用<strong>交叉熵损失函数</strong>，调参使得交叉熵损失函数最小，如下图所示：<br><img src="https://img-blog.csdnimg.cn/29c36a171c584a36bead0693b41b9027.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>当然上面只是一个点的，把每个样本结合起来看。 <img src="https://img-blog.csdnimg.cn/ef62afa46f1248a09b17cea823c91513.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>不过这个地方有几个疑问？</p><blockquote><ul><li> Q1：神经网络里面有很多层，所以有很多 <em>w</em> 和 <em>b</em> 调整。那是全体都一次性调整么？</li><li> 答：  <strong>现在不知道啊！</strong></li><li> Q2：上面我是把所有的 交叉熵 都加在一起 然后对 整个大的损失函数进行调参是这样嘛？ 那我的训练样本应该是 各个数字都有是吗？</li><li> 答： <strong>现在不知道啊！</strong></li></ul></blockquote><h2 id="Step-3：如何找到一个最好的函数（最佳参数），即调参"><a href="#Step-3：如何找到一个最好的函数（最佳参数），即调参" class="headerlink" title="Step 3：如何找到一个最好的函数（最佳参数），即调参"></a>Step 3：如何找到一个最好的函数（最佳参数），即调参</h2><p>用的还是 <strong>梯度下降法</strong>  </p><ol><li>随机选取一组参数</li><li>输入所有的训练样本</li><li>然后样本数据不变，参数不断变，用梯度下降法更新参数</li></ol><p><img src="https://img-blog.csdnimg.cn/72b790c9e1764d88bfef2b9f447c36c3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/24ebb0869baf46e899e8f509d26ba720.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="Backpropagation（反向传播）-来优化调参速度"><a href="#Backpropagation（反向传播）-来优化调参速度" class="headerlink" title="Backpropagation（反向传播） 来优化调参速度"></a>Backpropagation（反向传播） 来优化调参速度</h1><p>神经网络可能有很多个隐层，因此可能有百万数量级的参数，为了在<strong>梯度下降时有效地快速计算梯度</strong>，使用<strong>反向传播</strong>。<br>下图是 使用的损失函数为：交叉熵损失函数 ， 梯度下降法就是对每个参数求偏导，然后根据偏导大小进行左右移动，找到偏导为0的点，就是最佳参数值。</p><h2 id="计算对参数偏导的表达式"><a href="#计算对参数偏导的表达式" class="headerlink" title="计算对参数偏导的表达式"></a>计算对参数偏导的表达式</h2><p><img src="https://img-blog.csdnimg.cn/52fb4c86e71843b3b44aec5e596d4324.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>交叉熵损失函数，为 正确值乘以 ln的带入x样本的线性模型 +<br>（1-正确值）乘以ln（1-带入x样本线性模型）<img src="https://img-blog.csdnimg.cn/cf25205ad5354582b0355f3aba098387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><p>只考虑刚输入阶段的神经元，可以得到用 <em>sigmoid</em>激活函数之前的量 <em>z</em>，而 <em>z</em> 是由线性模型（按权重分配向量 + 偏值 <em>b</em> 得到）。如下图所示，可以看到对参数 w 的偏导可以按照链式法则为  $$<br>∂C/∂w=∂z/∂w × ∂C/∂z $$  前项为前项传递，后项为后向传递<br><img src="https://img-blog.csdnimg.cn/0438d1f3d1384270bf8a8abc93a6fe81.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ul><li>前项传递非常好看出来，就是对应的输入值，比如 <code>∂C/∂w1=∂z/∂w1 × ∂C/∂z</code> 。其中<code>∂z/∂w1</code> 看图上所示，不就是对 <em>w1</em> 进行偏导么，那就是 <em>x1</em>。</li><li>后项过程比较复杂，根据链式法则，<code>∂C/∂z=∂a/∂z × ∂C/∂a</code>，其中<code>∂a/∂z = σ&#39;(z)</code> 如下图所示<img src="https://img-blog.csdnimg.cn/636c2b53744d478ab268bf80073be7fa.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>归纳一下得到如下：<img src="https://img-blog.csdnimg.cn/c03a971beb4940cd93cb185b340bf470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">这个时候我们从另一观点看待上面的式子：有另外一个神经元（下图中的三角形，表示乘法/放大器），input是<code>∂C/∂z</code>‘与<code>∂C/∂z′&#39;</code>  ，权重分别是 <em>w3,w4</em>，求和经过神经元（乘以σ′(z)），得到 <code>∂C/∂z</code>。（相当于反向传播，先线性加权再乘以一个σ′(z) 和正向非常类似）<img src="https://img-blog.csdnimg.cn/6ec05d31b6ed4eb89ab2c5bcc39b6282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul><h2 id="后项传递-的两种情况"><a href="#后项传递-的两种情况" class="headerlink" title="后项传递 的两种情况"></a>后项传递 的两种情况</h2><p>如上图所示是我们最后得到的后项传递的式子，其中我们还有两个项不知道。那我们现在需要计算这两个部分，但分为两种情况</p><h3 id="case-1：下一层是最终输出层"><a href="#case-1：下一层是最终输出层" class="headerlink" title="case 1：下一层是最终输出层"></a>case 1：下一层是最终输出层</h3><p>第一种情况，z′,z′′ 所接的neuron是output layer的neuron。<br>这个就比较简单，直接根据最后一层的输出反向写出即可。<img src="https://img-blog.csdnimg.cn/5683f53fae4b4b619a617d5513159510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="case-2：下一层不是最终输出层"><a href="#case-2：下一层不是最终输出层" class="headerlink" title="case 2：下一层不是最终输出层"></a>case 2：下一层不是最终输出层</h3><p>第二种情况，z′,z′′ 所接的neuron不是output layer的neuron。<img src="https://img-blog.csdnimg.cn/935435dd3b0b435c9ab31388786acd78.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">就上面那个图，我们可以推得类似后项传递的式子<img src="https://img-blog.csdnimg.cn/783ca76c842846efa9b6d89de1a874d9.png#pic_center" alt="在这里插入图片描述">就这样反复迭代(递归)，直到遇到case1的情况，就可以算出整个后项传递。然后结合之前的前项传递，就是我们要得到的对这个参数的偏微分值。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://img-blog.csdnimg.cn/9796822a131546fabd92e1a1bac1a2d1.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>梯度下降时需要计算损失函数对每个参数偏微分<code>∂C/∂w</code>，<em>w</em> 代表相邻隐层之间的一条连线（即权值），每个 <em>w</em> 只有一个所指的神经元。</p><ul><li>链式法则将计算 <code>∂C/∂w</code> 拆成前向过程与后向过程。</li><li>前向过程计算的是<code>∂z/∂w</code> ，这里 <em>z</em> 是 <em>w</em> 所指neuron的input，<strong>计算结果是与 <em>w</em> 相连的值。</strong><br>后向过程计算的是<code>∂C/∂z</code>，这里 <em>z</em> 仍是 <em>w</em> 所指neuron的input，计算结果通过从后至前递归得到。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@[TOC](深度学习 Deep Learning 基础)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;深度学习需要明白的几个问题？&lt;/strong&gt;&lt;br&gt;思路：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;什么是深度学习？为什么需要深度学习？深度学习和机器学习的关系？&lt;/li&gt;
&lt;li&gt;深度学习的步骤</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="反向传播" scheme="http://example.com/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归 Logistic Regression</title>
    <link href="http://example.com/2021/08/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%20Logistic%20Regression/"/>
    <id>http://example.com/2021/08/05/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%20Logistic%20Regression/</id>
    <published>2021-08-05T15:08:06.000Z</published>
    <updated>2021-08-05T15:08:47.033Z</updated>
    
    <content type="html"><![CDATA[<p>@[TOC](逻辑回归问题 Logistic Regression)</p><p><strong>逻辑回归需要明白的几个问题？</strong></p><p>1、逻辑回归(Logistics Regression) 与 线性回归(Linear Regression)的区别在哪<br><img src="https://img-blog.csdnimg.cn/d50c9e51dfae4dadba381f980f05752a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>2、生成模型(Generative Model) 与 判别模型(Discriminative Model）的区别在哪</p><ul><li>生成模型就是要学习 x 和 y 的联合概率分布 *P(x,y)*，然后根据贝叶斯公式来求得条件概率 *P(y|x)*，预测条件概率最大的 y。</li><li>判别模型就是直接学习条件概率分布 *P(y|x)*。</li></ul><p>3、逻辑回归(Logistics Regression) 与  深度学习（ VS Deep Learning）<br>逻辑回归具有缺陷，需要做特征工程来转变特征，但是这个人为步骤非常麻烦，所以引入了深度学习。</p><h1 id="逻辑回归与线性回归"><a href="#逻辑回归与线性回归" class="headerlink" title="逻辑回归与线性回归"></a>逻辑回归与线性回归</h1><h2 id="什么是逻辑回归？"><a href="#什么是逻辑回归？" class="headerlink" title="什么是逻辑回归？"></a>什么是逻辑回归？</h2><ul><li>逻辑回归是<strong>解决分类问题的一种算法</strong></li><li>它与 <strong>线性模型</strong> 形式上有点像（本质上是在线性模型外面“裹”一个<strong>sigmoid激活函数</strong>，来表示概率的函数）</li><li>它是一种<strong>判别模型</strong>，与前面说的生成模型不同</li><li>它是深度学习的基础</li></ul><h2 id="对比逻辑回归与线性回归"><a href="#对比逻辑回归与线性回归" class="headerlink" title="对比逻辑回归与线性回归"></a>对比逻辑回归与线性回归</h2><h3 id="区别一：模型不同"><a href="#区别一：模型不同" class="headerlink" title="区别一：模型不同"></a>区别一：模型不同</h3><p>本质上是在线性模型外面“裹”一个<strong>sigmoid激活函数</strong>，来表示概率的函数<br><img src="https://img-blog.csdnimg.cn/559dca41ef354a65b9e73ad4d8bb8222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2c3580835cbe4497a28537f12f972bf9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>总结的来看<img src="https://img-blog.csdnimg.cn/e591d052433d421da59cc20086e67959.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h3 id="区别二：损失函数Loss不同"><a href="#区别二：损失函数Loss不同" class="headerlink" title="区别二：损失函数Loss不同"></a>区别二：损失函数Loss不同</h3><p><strong>逻辑回归的</strong><br><img src="https://img-blog.csdnimg.cn/81b9e6bbaf8743c8811b04dab97ae6cd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>为什么似然函数最大，参数就越有可能，越合理？</strong><br>最大似然估计：现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/d853c71170e743b58d68534399dbcd5a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/09f5eac31e1e4c90a1ca26b713423f2c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>总结一下：</strong><br><img src="https://img-blog.csdnimg.cn/c1c93acf9e1a46f9b6b3d821a168ebdf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>问题：为什么逻辑回归不采用线性回归的差平方来做？<br>例如，其实目前离目标点还很远，但梯度已经为0了，这显然不合理。<img src="https://img-blog.csdnimg.cn/2dee2450c8d34bf1b56a4e5c8d305db9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h3 id="区别三：如何调参的方式是一致的"><a href="#区别三：如何调参的方式是一致的" class="headerlink" title="区别三：如何调参的方式是一致的"></a>区别三：如何调参的方式是一致的</h3><ul><li><strong>化简逻辑回归损失函数左侧部分</strong><br><img src="https://img-blog.csdnimg.cn/5a911af3aeb7475b832369858f503b7f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li><strong>化简逻辑回归右侧部分</strong><img src="https://img-blog.csdnimg.cn/c627e39a6c9348d0a3e81c0099430536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>化简，调参如下<img src="https://img-blog.csdnimg.cn/ccfda49ca7004f8f87fc76f238dd5337.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><img src="https://img-blog.csdnimg.cn/771e437cc99945d8b884f331f09132eb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul><h1 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h1><h2 id="什么是生成模型和判别模型？"><a href="#什么是生成模型和判别模型？" class="headerlink" title="什么是生成模型和判别模型？"></a>什么是生成模型和判别模型？</h2><p><strong>从本质上讲，生成模型和判别模型是解决分类问题的两类基本思路 。</strong><br><em>分类问题，就是给定一个数据 x，要判断它对应的所属标签 y</em></p><ul><li>生成模型就是要学习 x 和 y 的联合概率分布 *P(x,y)*，然后根据贝叶斯公式来求得条件概率 *P(y|x)*，预测条件概率最大的 y。</li><li>判别模型就是直接学习条件概率分布 *P(y|x)*。</li></ul><h2 id="两种模型案例-举个栗子？"><a href="#两种模型案例-举个栗子？" class="headerlink" title="两种模型案例 举个栗子？"></a>两种模型案例 举个栗子？</h2><blockquote><p>栗子1：<br>假设你从来没有见过大象和猫，连听都没有听过，这时，给你看了一张大象的照片和一张猫的照片。<br><strong>Q : 你看过之后，有人牵了一头真的大象过来，问你只是大象还是猫？</strong></p><ul><li>用<strong>判别模型</strong>的思路回答：你回想刚才看过的照片，大象比猫很明显有个长鼻子，所以眼前这个有着长鼻子的动物就是大象。</li><li>用<strong>生成模型</strong>的思路回答：你回想刚才看过的照片，然后用笔把它们画在了纸上，拿着纸和我家的大象做比较，你发现，眼前的动物更像是大象。</li></ul><p> 第一个解决问题的思路就是判别模型，<strong>因为你只记住了大象和猫之间的不同之处</strong>。第二个解决问题的思路就是生成模型，因为你<strong>实际上学习了什么是大象，什么是猫</strong>。</p></blockquote><blockquote><p>栗子2：<br>有四个形式为(x,y)的样本。(1,0), (1,0), (2,0), (2,1）。假设，我们想从这四个样本中，<strong>学习到如何通过x判断y的模型</strong>。</p><ul><li>用生成模型，我们要学习 *P(x,y)*。如下所示：<img src="https://img-blog.csdnimg.cn/b435c974485141b1a36542a5c933ac01.png#pic_center" alt="在这里插入图片描述">我们学习到了四个概率值，它们的总和是1，这就是联合分布律P(x,y)。（因为这是离散的，连续的话叫联合概率密度）</li><li>用判别模型，我们要学习 *P(y|x)*，如下所示：<img src="https://img-blog.csdnimg.cn/4bde84a64231412d951a1c7c31d180f1.png#pic_center" alt="在这里插入图片描述">因为这是条件分布律，每一行概率值相加都为1。</li></ul><p><strong>Q : 当 <em>x=1</em> 时，请问 <em>y</em> 是 <em>0</em> 还是 <em>1</em> 呢？</strong></p><ul><li>用<strong>生成模型</strong>，我们会比较<br>P(x=1,y=0) = 1/2<br>P(x=1,y=1) = 0<br>我们发现 *P(x=1,y=0)*的概率要比 <em>P(x=1,y=1)<em>的概率大，所以，我们判断：</em>x=1时，y=0</em>。</li><li>用<strong>判别模型</strong>，我们会比较：<br>P(y=0|x=1) = 1<br>P(y=1|x=1) = 0<br>同样，<em>P(y=0|x=1)</em> 要比 <em>P(y=1|x=1)<em>大，所以，我们判断：</em>x=1时，y=0</em>。</li></ul><p> 我们看到，虽然最后预测的结果一样，但是得出结果的逻辑却是完全不同的。</p></blockquote><h2 id="生成模型为啥叫生成模型？"><a href="#生成模型为啥叫生成模型？" class="headerlink" title="生成模型为啥叫生成模型？"></a>生成模型为啥叫生成模型？</h2><p>生成模型之所以叫生成模型，是因为：<br>它背后的思想是，x是特征，y是标签，什么样的标签就会生成什么样的特征。好比说，标签是大象，那么可能生成的特征就有大耳朵，长鼻子等等。<br>当我们来根据x来判断y时，我们实际上是在比较，<strong>什么样的y标签更可能生成特征x，我们预测的结果就是更可能生成x特征的y标签</strong>。</p><h2 id="为什么一般来说，判别模型表现得会比生成模型好？"><a href="#为什么一般来说，判别模型表现得会比生成模型好？" class="headerlink" title="为什么一般来说，判别模型表现得会比生成模型好？"></a>为什么一般来说，判别模型表现得会比生成模型好？</h2><p>我们举一个栗子，现在有Class1 和 Class2 两类数据。现在训练集数据如图所示：<img src="https://img-blog.csdnimg.cn/bd00dca74d524c1a83b9b90bb3745fcf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>Q : 问请问如下的测试集，他是应该属于Class1 还是 Class2 呢？</strong><img src="https://img-blog.csdnimg.cn/a1ee9138baba426883e87dcd9b3f2358.png#pic_center" alt="在这里插入图片描述"><br>我们这边用判别模型来计算，算出如下图所示的数据：<img src="https://img-blog.csdnimg.cn/3f0b16520085492da2e7639d2646130c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>然后用贝叶斯公式算出 <em>P(C1 | x)</em> ，就是当x1和x2全为1 的情况下的概率是多少。<img src="https://img-blog.csdnimg.cn/a6f8b3138add4e2bbb548322591e0584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>算出的结果是 <em>＜ 0.5</em> 的，但从我们人的角度看，其实应该是属于Class1的，因为Class2里面压根就没有x1，x2同时为1的存在。<strong>这是为什么呢？因为我们的样本太少了，用判别模型是可以帮助我们在有限的数据样本中假象数据。</strong></p><blockquote><p><strong>那用生成模型怎么做呢？</strong></p></blockquote><p>所以判别模型的优势在于</p><ol><li>样本量少的时候表现比判别模型好，因为它能自己脑补出一个假想模型</li><li>噪声对它影响较小，因为它没有过分依赖数据，它是按照自己假想模型走的</li></ol><h2 id="常见的生成模型和判别模型有哪些呢？"><a href="#常见的生成模型和判别模型有哪些呢？" class="headerlink" title="常见的生成模型和判别模型有哪些呢？"></a>常见的生成模型和判别模型有哪些呢？</h2><p>生成模型</p><ul><li>HMM（隐马尔可夫模型）</li><li>朴素贝叶斯</li></ul><p>判别模型</p><ul><li>逻辑回归</li><li>SVM（支持向量机）</li><li>CRF（条件随机场）</li><li>最近邻</li><li>一般的神经网络</li></ul><h1 id="逻辑回归与深度学习"><a href="#逻辑回归与深度学习" class="headerlink" title="逻辑回归与深度学习"></a>逻辑回归与深度学习</h1><h2 id="逻辑回归解决多分类问题"><a href="#逻辑回归解决多分类问题" class="headerlink" title="逻辑回归解决多分类问题"></a>逻辑回归解决多分类问题</h2><p>逻辑回归是解决分类问题的，实际中的问题大多是多分类的问题，多分类问题会用到softmax。 <em>逻辑回归其实就是线性回归在外面加了个sigmoid激活函数（二分类）或者softmax激活函数（多分类）。</em></p><blockquote><p><strong>sigmoid激活函数 和 softmax函数的区别：</strong><br>通常在<strong>二分类</strong>中使用<strong>sigmoid作为最后的激活层</strong>。在<strong>多分类</strong>单标签中使用<strong>softmax作为激活层</strong>，取概率最高即可。多标签问题中使用sigmoid作为激活层，相当于把每一个类别都当成了二分类来处理。<img src="https://img-blog.csdnimg.cn/52837028507c423c97e4ba53a831270d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>多分类问题解决<br><img src="https://img-blog.csdnimg.cn/f92fa56ec4154bf687f88bc0192dcb08.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b674d12040f14c7dacc30b4a19f3af0b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h2 id="逻辑回归的局限性"><a href="#逻辑回归的局限性" class="headerlink" title="逻辑回归的局限性"></a>逻辑回归的局限性</h2><p><img src="https://img-blog.csdnimg.cn/d1fd15ed70b44e689619a1dfecaed0f4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="用深度学习去解决这个问题"><a href="#用深度学习去解决这个问题" class="headerlink" title="用深度学习去解决这个问题"></a>用深度学习去解决这个问题</h2><p><img src="https://img-blog.csdnimg.cn/23f73c28c3f54fa19624a03839a18958.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>这个怎么变换过来的啊？</strong><br>是需要用特征工程的方法的，而特征工程是需要我们人为地去建立一个特征函数去把这些点转化，<strong>实际上是比较难的</strong>，或者说比较费工夫的<br>这个时候我们需要引入 <strong>深度学习</strong><br><img src="https://img-blog.csdnimg.cn/39cb9b68112d48e384f6958820a041c9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@[TOC](逻辑回归问题 Logistic Regression)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;逻辑回归需要明白的几个问题？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1、逻辑回归(Logistics Regression) 与 线性回归(Linear Regression)的区</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="分类问题" scheme="http://example.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    
    <category term="逻辑回归" scheme="http://example.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>分类问题 Classification 案例一：神奇宝贝是水系还是普通系？</title>
    <link href="http://example.com/2021/08/04/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%20Classification%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E7%A5%9E%E5%A5%87%E5%AE%9D%E8%B4%9D%E6%98%AF%E6%B0%B4%E7%B3%BB%E8%BF%98%E6%98%AF%E6%99%AE%E9%80%9A%E7%B3%BB%EF%BC%9F/"/>
    <id>http://example.com/2021/08/04/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%20Classification%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E7%A5%9E%E5%A5%87%E5%AE%9D%E8%B4%9D%E6%98%AF%E6%B0%B4%E7%B3%BB%E8%BF%98%E6%98%AF%E6%99%AE%E9%80%9A%E7%B3%BB%EF%BC%9F/</id>
    <published>2021-08-04T06:21:06.000Z</published>
    <updated>2021-08-06T14:47:11.946Z</updated>
    
    <content type="html"><![CDATA[<p>@[TOC](分类问题 Classification)</p><blockquote><p>概念：（从概率生成模型到判别模型）<br>概率生成模型：由数据学习联合概率密度分布 <em>P(X,Y)</em> ，然后求出条件概率分布<em>P(Y|X)</em> 作为预测的模型。例如：朴素贝叶斯、隐马尔可夫（em算法）<br>判别模型：由数据直接学习决策函数 <em>Y=f(X)</em> 或者条件概率分布 <em>P(Y|X)</em>  作为预测的模型。例如：k近邻法、感知机、决策树、逻辑回归、线性回归、最大熵模型、支持向量机(SVM)、提升方法、条件随机场（CRF）</p></blockquote><p><strong>分类问题的思路</strong></p><ol><li>分类问题及其解决方法的讨论<pre><code> 1. 首先，什么是分类问题？ 2. 接着，分类问题该如何解决呢？</code></pre></li><li>建立概率生成模型的步骤（以朴素贝叶斯分类器为例）<br>step1：求先验概率<br>step2：确定<strong>数据属于哪一个分布，用最大似然估计出分布函数的参数</strong><br>step3：求出后验概率</li><li>生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出</li></ol><h1 id="分类问题及其解决方法的讨论"><a href="#分类问题及其解决方法的讨论" class="headerlink" title="分类问题及其解决方法的讨论"></a>分类问题及其解决方法的讨论</h1><h2 id="什么是分类问题？"><a href="#什么是分类问题？" class="headerlink" title="什么是分类问题？"></a>什么是分类问题？</h2><p><img src="https://img-blog.csdnimg.cn/06b2e30dc04b4fa1a871ee3acee382c3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>说白了 就是输入一个事务的一些参数特征，通过数学模型，可以得到这个东西是什么。 这也包括二分问题（结果是由两面）与多分问题。</p><p>案例：<br>举个栗子，输入一个神奇宝贝，输出：他是属于什么属性的？ <strong>这是一个典型的多分问题，因为属性有好几种啊。</strong><br><img src="https://img-blog.csdnimg.cn/d6dd520c2c1b4c8d9e6396369b5b1f56.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>举个栗子，输入一个神奇宝贝图片，输出：他是可达鸭么？ <strong>这是一个典型的二分问题，因为结果只有两种 是还是不是。</strong><br>举个栗子，假设有两个类别（水系和普通系），每个类别有不同的精灵，现在我抓到一个精灵，那么它是属于水系和普通系的概率分别是多少。这也是个<strong>典型的二分问题</strong>。</p><h2 id="如何解决分类问题？"><a href="#如何解决分类问题？" class="headerlink" title="如何解决分类问题？"></a>如何解决分类问题？</h2><h3 id="如何解决二分问题？"><a href="#如何解决二分问题？" class="headerlink" title="如何解决二分问题？"></a>如何解决二分问题？</h3><h4 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h4><p>对于二分类问题，定义一个function也就是数学model，<strong>当输出函数值大于0就划分为<em>类别1</em>，否则就为<em>类别2</em>.</strong> <strong>而损失函数定义为在测试数据上误分类的次数</strong>。<img src="https://img-blog.csdnimg.cn/3ee112c779004e5da28ca9878930b369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="那这个function我到底怎么定义呢？"><a href="#那这个function我到底怎么定义呢？" class="headerlink" title="那这个function我到底怎么定义呢？"></a>那这个function我到底怎么定义呢？</h4><p>实际上，可以将其定义为一个<strong>概率模型</strong>。<br>它可以是一个<strong>条件概率模型 <em>P(C1 | X)</em></strong>,当  <em><strong>P(C1 | X) &gt; 0.5</strong></em> ,比如在神奇宝贝二分问题中，我们定义X是这张图片的参数，而C1表示是可达鸭，整个的意思就变为了在这些图片参数的条件下这张图片是可达鸭的概率是多少，概率大于一半，说明确实很有可能就是可达鸭。或者对于第二个栗子，同样的可以规定**条件概率模型 *P(C2 | X)***，表示在捕捉了一个精灵后，他的参数条件下，是水系的概率是多少？（这边C2表示，捕捉的是水系）</p><p>这边我们以栗子2为例，假如我们捕捉了一只神奇宝贝(其实他就是可达鸭)，问他是水系的概率是多少？（理论上其实，他就是水系的，但机器需要通过概率论去推，需要包括以下的概率推导）<br><img src="https://img-blog.csdnimg.cn/dfa3d538674d4dcd8cb32ccbd52ea7f2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>如上图所示： 　 x就表示是可达鸭<br>先验概率：这里是P(C1)——训练样本中的精灵是水系的概率;同理P(C2)——训练样本中的精灵是普通系的概率<br>——————————————————————————————————————————————————————————————————<br>*P(x | C1)*：这里是指在水系中是可达鸭的概率<br>*P(x | C2)*：这里是指在普通系中是可达鸭的概率<br>两者可以用<strong>最大似然估计法</strong>求出——————————————————————————————————————————————————————————————————<br>后验概率：这里指抓到的神奇宝贝可达鸭是水系的概率。其用贝叶斯公式算出，公式如上图所示</p></blockquote><h5 id="求先验概率-P-C1-：训练样本中的精灵是水系的概率"><a href="#求先验概率-P-C1-：训练样本中的精灵是水系的概率" class="headerlink" title="求先验概率 P(C1)：训练样本中的精灵是水系的概率"></a>求先验概率 P(C1)：训练样本中的精灵是水系的概率</h5><p>根据训练样本，分别算出水系和普通系的概率<br><img src="https://img-blog.csdnimg.cn/67108f6b24044efbb896cb62ac2bd2c0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h5 id="选择概率分布函数，用最大似然估计出分布函数的参数（就是调参）"><a href="#选择概率分布函数，用最大似然估计出分布函数的参数（就是调参）" class="headerlink" title="选择概率分布函数，用最大似然估计出分布函数的参数（就是调参）"></a>选择概率分布函数，用最大似然估计出分布函数的参数（就是调参）</h5><blockquote><p>注：</p><ul><li>当样本数据x取实数值时，采用正太分布(高斯分布)</li><li>当每种特征的数值都在0-1内时，采用伯努利分布</li><li>当每种特征取值在{1, 2 , 3 , …，K}，采用多项式分布（Multinomial Distribution）</li></ul></blockquote><ol><li>　首先，我们目标是求水系样本中的79个精灵中，抓到其中一种神奇宝贝的概率 <strong>P(x | C1)</strong> ，那么这个概率应该是跟精灵的属性有关的。<br>这里我们选择<strong>两种属性（物防和法防）讨论</strong>，此时数据中（x,水系）中的x应该是一个神奇宝贝<strong>向量（[x1物防，x2法防] , 水系）。</strong><br><img src="https://img-blog.csdnimg.cn/4eaa50360e3e4c8492b900326980f697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>2、然后，这里选择正太分布（二维）：也就是说在水系样本中的79个精灵中，抓到其中一种精灵的概率 <em>P( x | C1)</em> 呈正太分布。<br><img src="https://img-blog.csdnimg.cn/86f5ab53fd124a47918b1da54cef4c3a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>接着，用最大似然估计法算出分布函数的参数<br>似然函数<em>L</em>：x1,x2…x79同时出现的概率函数。<br>最大似然估计：使似然函数最大时的参数估计。<img src="https://img-blog.csdnimg.cn/8ea6f6da88ba4657999c75674b70170b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>调整的参数<br><img src="https://img-blog.csdnimg.cn/528ff3d528f04fcb8a799f08f3712f4a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><h5 id="最后求出后验概率-即P-C1-x-抓到的可达鸭是水系的概率"><a href="#最后求出后验概率-即P-C1-x-抓到的可达鸭是水系的概率" class="headerlink" title="最后求出后验概率 即P(C1 | x): 抓到的可达鸭是水系的概率"></a>最后求出后验概率 即P(C1 | x): 抓到的可达鸭是水系的概率</h5>利用的就是贝叶斯公式。<br>整体每个部分的逻辑可以看下图所示：<img src="https://img-blog.csdnimg.cn/e1d5794c439749dc8e2286bfe3af4ea3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><h4 id="效果不好的解决方法"><a href="#效果不好的解决方法" class="headerlink" title="效果不好的解决方法"></a>效果不好的解决方法</h4></li></ol><p><strong>实验结果</strong><br><img src="https://img-blog.csdnimg.cn/67640ca1732f41189f0bc43688a71a0e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>分类后，准确率并不高。理论上考虑其他因素，即增加维度可以增大准确率。但效果还是不佳，这该怎么办。<br><img src="https://img-blog.csdnimg.cn/841b72c2ccb243df9a91141dc200370f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>实验结果<br><img src="https://img-blog.csdnimg.cn/0b0ee16c14424df59cf27c80a49a0e1b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出"><a href="#生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出" class="headerlink" title="生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出"></a>生成模型解决分类问题的总结以及逻辑回归方法（判别模型）的引出</h2><h3 id="回到如何用机器学习的三大步骤解决分类问题："><a href="#回到如何用机器学习的三大步骤解决分类问题：" class="headerlink" title="回到如何用机器学习的三大步骤解决分类问题："></a>回到如何用机器学习的三大步骤解决分类问题：</h3><p><img src="https://img-blog.csdnimg.cn/b62ddf0753f84cce856bf1991f2d1f81.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="逻辑回归方法（判别模型）的引出"><a href="#逻辑回归方法（判别模型）的引出" class="headerlink" title="逻辑回归方法（判别模型）的引出"></a>逻辑回归方法（判别模型）的引出</h3><p><img src="https://img-blog.csdnimg.cn/5b57d3ebf742485eb514f106dba3de5a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="化简推到一下（纯数学）"><a href="#化简推到一下（纯数学）" class="headerlink" title="化简推到一下（纯数学）"></a>化简推到一下（纯数学）</h3><p><img src="https://img-blog.csdnimg.cn/f2323d6bd8474f5891cf63af3f18c47e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/e8faa78abb0641adae81388745601c13.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>所以其实不用考虑，N1，N2，μ1，μ2，∑的值。 直接就是w和b两个参数，这也是为什么之前，我们将∑按权值分配后，图像由线性分类的原因。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@[TOC](分类问题 Classification)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;概念：（从概率生成模型到判别模型）&lt;br&gt;概率生成模型：由数据学习联合概率密度分布 &lt;em&gt;P(X,Y)&lt;/em&gt; ，然后求出条件概率分布&lt;em&gt;P(Y|X)&lt;/em&gt; 作为预测</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="分类问题" scheme="http://example.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降算法 进阶</title>
    <link href="http://example.com/2021/08/04/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%20%E8%BF%9B%E9%98%B6/"/>
    <id>http://example.com/2021/08/04/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%20%E8%BF%9B%E9%98%B6/</id>
    <published>2021-08-04T05:16:06.000Z</published>
    <updated>2021-08-04T12:49:04.332Z</updated>
    
    <content type="html"><![CDATA[<p>@[TOC](梯度算法 进阶)</p><h1 id="回顾梯度算法"><a href="#回顾梯度算法" class="headerlink" title="回顾梯度算法"></a>回顾梯度算法</h1><p><strong>是一种迭代的算法，每看一个参数都会更新。</strong><br><img src="https://img-blog.csdnimg.cn/eee493cfe8f24755a2aa3cfd41f486f4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/62ccf5ddd84348308c79a70233b0da19.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="学习率η（Learning-Rate）"><a href="#学习率η（Learning-Rate）" class="headerlink" title="学习率η（Learning Rate）"></a>学习率η（Learning Rate）</h1><h2 id="自定义的学习率对参数选择的影响"><a href="#自定义的学习率对参数选择的影响" class="headerlink" title="自定义的学习率对参数选择的影响"></a>自定义的学习率对参数选择的影响</h2><p><img src="https://img-blog.csdnimg.cn/6c39ce49d85048098ac7303a552bc184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>学习率需要选取合适的值，过小会导致模型调参的速度太慢，但过大会导致错失掉了最佳参数点</strong></p><h2 id="如何调整学习率η？"><a href="#如何调整学习率η？" class="headerlink" title="如何调整学习率η？"></a>如何调整学习率η？</h2><ol><li><p>在最开始的时候，随机点离目标点很远，我们一般会选取一个比较大的学习率；当做了几期后，我们离目标点很近了，所以我们会减小学习率 缩减为 如下图所示的公式<img src="https://img-blog.csdnimg.cn/b9bc45580c004a6cbb708595125b1a1a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></li><li><p><strong>不同的参数应该设置不同的学习率</strong></p></li></ol><p>下面是常用的 调整学习率的方法：</p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Adagrad是解决不同参数应该使用不同的更新速率的问题。<strong>Adagrad自适应地为各个参数分配不同学习率的算法。</strong></p><blockquote><p><strong>其原理为：</strong><img src="https://img-blog.csdnimg.cn/4423c0afa60f462cb444f37cd1307576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>里面比较核心的部分在 每次σ的取值。<strong>每次σ规定为之前所有<em>g</em>平方对应的之和的均方根</strong>。例如下图所示：其中 <em>g</em> 是每次的偏导值<img src="https://img-blog.csdnimg.cn/82780afd2b0347ad8505db41024b9f7d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>结合之前说的 <em>η</em> 值的变化 可以得到如下的公式推导：<img src="https://img-blog.csdnimg.cn/cf79f34d94b44c72abdd283ebb0f3431.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><p><strong>提问:</strong>    发现一个现象，本来应该是随着gradient的增大，我们的学习率是希望增大的，也就是图中的g上标t；但是与此同时随着gradient的增大，我们的分母是在逐渐增大，也就对整体学习率是减少的，这是为什么呢？<img src="https://img-blog.csdnimg.cn/64154f52e91f4486be92171800f09815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这是因为随着我们更新次数的增大，我们是希望我们的学习率越来越慢。因为我们认为在学习率的最初阶段，我们是距离损失函数最优解很远的，随着更新的次数的增多，<strong>我们认为越来越接近最优解，于是学习速率也随之变慢。</strong>  为什么化简之后是如上这个式子呢，其在图像上的意义是 一阶导数比上二阶数的值。如下图所示：<img src="https://img-blog.csdnimg.cn/90fda3afedce4958a877451546c006e2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="Stochastic-Gradient-Descent（SGD随机梯度下降法）-？？？"><a href="#Stochastic-Gradient-Descent（SGD随机梯度下降法）-？？？" class="headerlink" title="Stochastic Gradient Descent（SGD随机梯度下降法） ？？？"></a>Stochastic Gradient Descent（SGD随机梯度下降法） ？？？</h3><p>其和 普通的梯度下降算法区别在。普通遍历在求和的时候需要浪费大量时间，进而去掉求和产生了随机梯度下降算法。<img src="https://img-blog.csdnimg.cn/e92e87942f4443cbac50e52369c68151.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>和下图看到的一样：<br><img src="https://img-blog.csdnimg.cn/76673f0f26614df6a9ac37311fe015b1.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p> 只是要注意一下标准的梯度下降和随机梯度下降的区别：</p><ol><li>标准下降时在权值更新前汇总所有样例得到的标准梯度，随机下降则是通过考察每次训练实例来更新（<strong>就是随机选择一些按顺序的后续样本点来，而不是全体数据</strong>）。<img src="https://img-blog.csdnimg.cn/32da14cad9bf4a2ba439745929b7ec11.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ol></blockquote><blockquote><ol start="2"><li>对于步长 <em>η</em> 的取值，标准梯度下降的 <em>η</em> 比随机梯度下降的大。因为标准梯度下降的是使用准确的梯度，理直气壮地走，随机梯度下降<strong>使用的是近似的梯度</strong>，就得小心翼翼地走，怕一不小心误入歧途南辕北辙了。</li><li>当损失函数有多个局部极小值时，随机梯度反而更可能避免进入局部极小值中。</li></ol></blockquote><h4 id="小批量随机梯度下降（batch-gradient-descent）"><a href="#小批量随机梯度下降（batch-gradient-descent）" class="headerlink" title="小批量随机梯度下降（batch gradient descent）"></a>小批量随机梯度下降（batch gradient descent）</h4><p>如果在每次迭代中，梯度下降是用整个训练数据集来计算梯度的话，则会带来大量的计算量。因此提出批量梯度下降来进行优化。<strong>每次优化不再是对整体数据集来计算损失，取而代之使用随机采样小批量的样本来计算梯度。</strong><img src="https://img-blog.csdnimg.cn/0d8e8d1e6c234e36b0972e5b95c9f46a.png#pic_center" alt="在这里插入图片描述"></p><h3 id="Feature-Scaling-（特征缩放）"><a href="#Feature-Scaling-（特征缩放）" class="headerlink" title="Feature Scaling （特征缩放）"></a>Feature Scaling （特征缩放）</h3><p>其意思就是说要<strong>将所有特征有相同的规模</strong>。例如下图所示，<em>X2</em> 的范围明显大宇 <em>X1</em>，所以要将 <em>X2</em> 进行缩放。<br><img src="https://img-blog.csdnimg.cn/2b4681ef27884c0bbf400db0624e8740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="为什么要做Feature-Scaling？"><a href="#为什么要做Feature-Scaling？" class="headerlink" title="为什么要做Feature Scaling？"></a>为什么要做Feature Scaling？</h4><blockquote><p>为什么要做Feature Scaling？<img src="https://img-blog.csdnimg.cn/456a5666c6bf40b7a1e5f7a8727b529f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如左图，<em>X1</em> 和 <em>X2</em> 数值规模大小相差很大，那 <em>W2</em> 这参数的变动会极大的影响损失函数，而 <em>W1</em> 影响度就很小。所以我们<strong>需要对 <em>X2</em> 进行特征缩放，使其与 <em>X1</em> 保持一个规模</strong>。并且其实从图像可以看出，如果按左图来做的话，我们很难找到一组合适的参数集合，因为他梯度下降的方法不是直线的而是曲线；而右图是近乎直线。（<strong>最里面圈的损失函数最小</strong>）</p></blockquote><h4 id="如何实现Feature-Scaling？"><a href="#如何实现Feature-Scaling？" class="headerlink" title="如何实现Feature Scaling？"></a>如何实现Feature Scaling？</h4><p><img src="https://img-blog.csdnimg.cn/f129b6a000c841ec926df684d2c77a15.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>如上图所示，假如有R个数据样本，每个样本有<em>X1 - Xi</em> 个特征。我们用上面的公式计算出其应该的scale（其实说实在的这不就是化成标准正太分布么）</p><h1 id="梯度下降算法数学总结"><a href="#梯度下降算法数学总结" class="headerlink" title="梯度下降算法数学总结"></a>梯度下降算法数学总结</h1><p>提出问题：如下图所示<strong>（我怎么样才能在红圈里找到最小损失的那个点呢）</strong><img src="https://img-blog.csdnimg.cn/53c25201a11349b6bc3af9e1dfc9fcc6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="首先要回顾泰勒公式"><a href="#首先要回顾泰勒公式" class="headerlink" title="首先要回顾泰勒公式"></a>首先要回顾泰勒公式</h2><p><img src="https://img-blog.csdnimg.cn/c9b154d4d14c43c8bf5e2609789bb094.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/205bb83b97d849fc9b8c7451e044fdab.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>二元泰勒：<br><img src="https://img-blog.csdnimg.cn/2efa8bf67ffa419e90221154515a3712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="用泰勒展开损失函数"><a href="#用泰勒展开损失函数" class="headerlink" title="用泰勒展开损失函数"></a>用泰勒展开损失函数</h2><p><img src="https://img-blog.csdnimg.cn/3ce8a2749e644d1f978c43d485798dde.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>理解为点乘，反向180度的时候，损失函数才是最小的：<img src="https://img-blog.csdnimg.cn/235fc97c36b74012b0663975ba0dc8a9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>最终可以表达成：<img src="https://img-blog.csdnimg.cn/83f0037797f64c048376868ee2c21fae.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="梯度下降算法缺点在哪"><a href="#梯度下降算法缺点在哪" class="headerlink" title="梯度下降算法缺点在哪"></a>梯度下降算法缺点在哪</h1><p>其不仅仅包括可能有找到是局部最优解问题，有可能在中间的时候就有偏微分为0的时候，而这时这个点可能离全局最优解点 很远。<br><img src="https://img-blog.csdnimg.cn/cd31675e6d8e4c83925852218d4f2acc.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@[TOC](梯度算法 进阶)&lt;/p&gt;
&lt;h1 id=&quot;回顾梯度算法&quot;&gt;&lt;a href=&quot;#回顾梯度算法&quot; class=&quot;headerlink&quot; title=&quot;回顾梯度算法&quot;&gt;&lt;/a&gt;回顾梯度算法&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;是一种迭代的算法，每看一个参数都会更新。&lt;/s</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>模型误差的来源分析</title>
    <link href="http://example.com/2021/08/02/%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/2021/08/02/%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90/</id>
    <published>2021-08-02T05:16:06.000Z</published>
    <updated>2021-08-04T12:48:42.267Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90">TOC</a></p><blockquote><p>思路：<br>1、首先，误差的两大来源 <em>bias</em>（偏差） 和 <em>variance</em>（方差） 是指什么<br>2、然后，<em>bias</em>（偏差）和 <em>variance</em>（方差）是怎么产生的<br>3、进一步，如何判断你的模型是 <em>bias大</em>（欠拟合）还是<em>variance大</em>（过拟合），如何解决 ？</p></blockquote><h1 id="模型的两大来源bias（偏差）和variance（方差）"><a href="#模型的两大来源bias（偏差）和variance（方差）" class="headerlink" title="模型的两大来源bias（偏差）和variance（方差）"></a>模型的两大来源bias（偏差）和variance（方差）</h1><blockquote><p>什么是bias和variance呢？<br>思路：</p><ol><li>首先要知道什么是误差</li><li>在了解什么是bias和variance</li></ol></blockquote><h2 id="什么是误差？"><a href="#什么是误差？" class="headerlink" title="什么是误差？"></a>什么是误差？</h2><p><strong>机器学习就是寻找一个函数，然后给它一个输入，就能得到一个理想的输出。</strong><br>f head是理论上找到的最佳函数，f star 是我们用模型预测出来的函数，<strong>两者的差值就是误差</strong>。<img src="https://img-blog.csdnimg.cn/39bb552a5ba348b88a8882c744f74566.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="什么是bias和variance（偏差和方差）"><a href="#什么是bias和variance（偏差和方差）" class="headerlink" title="什么是bias和variance（偏差和方差）"></a>什么是bias和variance（偏差和方差）</h2><h3 id="什么是bias（偏差）？"><a href="#什么是bias（偏差）？" class="headerlink" title="什么是bias（偏差）？"></a>什么是bias（偏差）？</h3><p>举个栗子说明，下图是用一定样本数的均值m来估计假设的随机变量的<strong>平均值u</strong>，这是一种<strong>无偏估计（unbiased）</strong>。<img src="https://img-blog.csdnimg.cn/eaeb2438d6a645d7b1236e6a9df59da8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>也就是说，<strong>估计值的期望等于假设值</strong>（如上文的E(m)=u），<strong>即为无偏差，反之有偏差（bias）</strong>。<br><strong>当样本数越来越大时，m就越靠近u。</strong></p><h3 id="什么是variance（方差）"><a href="#什么是variance（方差）" class="headerlink" title="什么是variance（方差）"></a>什么是variance（方差）</h3><p><strong>方差表达的是数据的离散程度</strong><img src="https://img-blog.csdnimg.cn/28ad8de1e99b4c91a3f16a1088968ca4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里对于方差的估计是有差估计：<img src="https://img-blog.csdnimg.cn/c9ee7e91fff24cd498b5a21420ad1ed5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="总结直观理解"><a href="#总结直观理解" class="headerlink" title="总结直观理解"></a>总结直观理解</h3><p>最后，bias和variance的直观理解：<img src="https://img-blog.csdnimg.cn/357d51b0a7f34b06933c5a86c042c8dc.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ol><li>bias表示的是预测的f bar（f star预测值的期望）与f head（实际正确值）的距离</li><li>variance表示的是每次预测的f star（预测值）与f bar（预测值的期望）的距离（看图）</li></ol><h2 id="bias和variance是怎么产生的"><a href="#bias和variance是怎么产生的" class="headerlink" title="bias和variance是怎么产生的"></a>bias和variance是怎么产生的</h2><blockquote><p>下面结合实际的实验说明，bias和variance是如何产生的。<br>————————————————————————————————————————————————————<br>bias如何知晓？，就要做多次实验，确定多个f star（一次实验一个预测值），然后求出f star 样本集的期望（E(f star)）<br>那么首先，我们虚拟出100个神奇宝贝平行宇宙（相当于设置了100组实验），每个预祝<strong>一个神奇宝贝训练家捕捉10只神奇宝贝</strong>（相当于每组实验10个数据），如下图：<img src="https://img-blog.csdnimg.cn/ef3d6cfdd1ca4c179d091c2f5130a42c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>然后思考，对于这样的数据，<strong>我们选用什么model比较好</strong> ?<strong>哪一个model最后的bias比较小</strong>？<img src="https://img-blog.csdnimg.cn/c05149fd285d4c96bd182567c2d891fd.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如上图，不同宇宙的神奇宝贝数据非常随机，项次越高，模型越复杂</p></blockquote><h3 id="方差-variance"><a href="#方差-variance" class="headerlink" title="方差 variance"></a>方差 variance</h3><p><img src="https://img-blog.csdnimg.cn/fea8cce3d3b64a768fbf007ddefe0021.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对于方差而言，其表示结果的离散程度，项次越高，模型越复杂，则其离散程度肯定是越大的。<strong>因为模型越简单，收到数据的影响也就越低</strong>，比如：我极端一点，f(x)=c，数据根本不会影响model最后的预测值</p><h3 id="偏差-bias"><a href="#偏差-bias" class="headerlink" title="偏差 bias"></a>偏差 bias</h3><p><img src="https://img-blog.csdnimg.cn/ade6e8be3e52487bafd38dd0f8e750a9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/759ad31af9b04b6a8cdeb92364abe4f7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>图中看出，简单model可能并不包含目标，因此会造成较大的bias，而复杂的model是涵盖目标的，所以bias小。</p><h3 id="偏差和方差的总结"><a href="#偏差和方差的总结" class="headerlink" title="偏差和方差的总结"></a>偏差和方差的总结</h3><blockquote><p>简单的模型（次数小），bias会比较大，但variance会比较小，预测值更加集中。<br>复杂的模型（次数大），bias会比较小，但variance会比较大，预测值更加的离散。<br>因此，我们理想中的目标是找到一个平衡点，使bias和variance尽可能小。<img src="https://img-blog.csdnimg.cn/c5a88735d7d94da79046122ca8310740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h2 id="判断你的模型是bias大（欠拟合）还是variance大（过拟合）并解决"><a href="#判断你的模型是bias大（欠拟合）还是variance大（过拟合）并解决" class="headerlink" title="判断你的模型是bias大（欠拟合）还是variance大（过拟合）并解决"></a>判断你的模型是bias大（欠拟合）还是variance大（过拟合）并解决</h2><h3 id="如何判断"><a href="#如何判断" class="headerlink" title="如何判断"></a>如何判断</h3><p><img src="https://img-blog.csdnimg.cn/c8150c9d246e4b10841294be51ab0f4a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="bias过大（欠拟合）"><a href="#bias过大（欠拟合）" class="headerlink" title="bias过大（欠拟合）"></a>bias过大（欠拟合）</h3><p>需要重新设计模型</p><ol><li>模型考虑的特征没有全，也就是很多其实没有作用。关键特征可能还没考虑到，需要加入进去</li><li>模型应该需要更加的复杂，增加更高的次项。</li></ol><h3 id="variance过大（过拟合）"><a href="#variance过大（过拟合）" class="headerlink" title="variance过大（过拟合）"></a>variance过大（过拟合）</h3><ol><li>需要更多的数据，有些数据不够有特点</li><li>可以增加一个 正则项，加强模型的平滑度，使其预测值分布不要太离散</li></ol><h3 id="解决实际测试比共有数据集误差更大的问题"><a href="#解决实际测试比共有数据集误差更大的问题" class="headerlink" title="解决实际测试比共有数据集误差更大的问题"></a>解决实际测试比共有数据集误差更大的问题</h3><p><img src="https://img-blog.csdnimg.cn/d742c18f9bc04a8ea7981b28a9c2970b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>将训练集，分为两部分。这叫做2-折交叉验证。一部分还是当做训练样本，帮助我们进行调参；另一部分用作验证，验证我的模型损失如何，是否合理（充当原来共有训练集的作用）。而现在原有训练集的部分用来充当实际样本数据，算出误差值，以便于真正在实际中误差太大。</p><p><img src="https://img-blog.csdnimg.cn/1c0fa7f4983a4dc585448876cab974d4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;思路：&lt;br&gt;1、首先，误差的两</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性回归Regression 案例一：宝可梦进化CP值</title>
    <link href="http://example.com/2021/07/31/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Regression%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E5%AE%9D%E5%8F%AF%E6%A2%A6%E8%BF%9B%E5%8C%96CP%E5%80%BC/"/>
    <id>http://example.com/2021/07/31/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Regression%20%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E5%AE%9D%E5%8F%AF%E6%A2%A6%E8%BF%9B%E5%8C%96CP%E5%80%BC/</id>
    <published>2021-07-31T03:16:06.000Z</published>
    <updated>2021-08-04T12:49:26.964Z</updated>
    
    <content type="html"><![CDATA[<p>@[TOC](线性回归Regression 案例一：宝可梦进化CP值)</p><blockquote><p>本栗子：预测Pokemon精灵攻击力。<br>输入：进化前的CP值、物种（Bulbasaur）、血量（HP）、重量（Weight）、高度（Height）<br>输出：进化后的CP值</p></blockquote><p><img src="https://img-blog.csdnimg.cn/img_convert/d53b2b95d30c7af9574c7a0bbaba2c82.png#pic_center" alt="在这里插入图片描述"></p><h1 id="实现回归的步骤（机器学习的步骤）"><a href="#实现回归的步骤（机器学习的步骤）" class="headerlink" title="实现回归的步骤（机器学习的步骤）"></a>实现回归的步骤（机器学习的步骤）</h1><h2 id="Step1-确定一个model-线性模型"><a href="#Step1-确定一个model-线性模型" class="headerlink" title="Step1 确定一个model - 线性模型"></a>Step1 确定一个model - 线性模型</h2><blockquote><p>输入的特征包括：进化前的CP值（<em>Xcp</em>）、物种（Bulbasaur）（<em>Xs</em>）、血量（HP）（<em>Xhp</em>）、重量（Weight）（<em>Xw</em>）、高度（Height）（<em>Xh</em>）</p></blockquote><p>先从简单的<strong>单个特征</strong> 进化前的CP值（<em>Xcp</em>）开始（<strong>后面改进再考虑多个特征</strong>）。<br><img src="https://img-blog.csdnimg.cn/img_convert/5b4df493600379c7aa0372e103eb7d79.png#pic_center" alt="在这里插入图片描述"></p><h2 id="Step2-goodness-of-function（函数优化）——损失函数"><a href="#Step2-goodness-of-function（函数优化）——损失函数" class="headerlink" title="Step2 goodness of function（函数优化）——损失函数"></a>Step2 goodness of function（函数优化）——损失函数</h2><ol><li>确定model后（案例为线性模型），开始训练数据（使用的为训练样本集）<img src="https://img-blog.csdnimg.cn/img_convert/ebd00bf7bee79841d6c35eac2865fd61.png#pic_center" alt="在这里插入图片描述"></li><li>训练10个训练样本后得到10个的预测进化CP值（<em>y</em>）如下图所示。左侧为训练样本本身真实的进化CP值，右侧为横轴为进化前CP值（<em>Xcp</em>）<img src="https://img-blog.csdnimg.cn/img_convert/fbba4351217b730cd11ca92a7427cd51.png#pic_center" alt="在这里插入图片描述"></li><li><strong>确定损失函数</strong>。损失函数用于评价一个模型的好坏。损失函数的值越小，那么模型越好。<strong>对于本案例，损失函数采用最简单的距离表示</strong>。即求<strong>实际进化后的CP值与模型预测的CP值差</strong>，来判定模型的好坏。<img src="https://img-blog.csdnimg.cn/img_convert/3abae7108681dbad20cc15a952d20bba.png#pic_center" alt="在这里插入图片描述"></li><li>List item</li></ol><h2 id="Step3-best-function（找出最好的一个函数-即调参）——使用梯度下降法"><a href="#Step3-best-function（找出最好的一个函数-即调参）——使用梯度下降法" class="headerlink" title="Step3 best function（找出最好的一个函数 即调参）——使用梯度下降法"></a>Step3 best function（找出最好的一个函数 即调参）——使用梯度下降法</h2><ol><li>案例采用<strong>梯度下降法</strong>来帮助选择 <em>w</em> 和 <em>b</em> 两个参数取何值时损失函数最小，也就意味着构建的模型越准确。 <img src="https://img-blog.csdnimg.cn/img_convert/7d75962cbe0b2dc3c54f370383912138.png#pic_center" alt="在这里插入图片描述"><blockquote><p>什么是梯度下降法？梯度指的是？<br>………………………………………………………………………………………………………<br>梯度？<br>在<strong>单变量的函数</strong>中，梯度其实就是对应点<strong>函数的微分</strong>，代表着这个函数在<strong>某个给定点的切线的斜率</strong><br>在<strong>多变量函数</strong>中，梯度是一个<strong>向量</strong>，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向  <strong>即例如二维就是 偏导 i + 偏导 j</strong><br>………………………………………………………………………………………………………<br>梯度下降法(SGD  Stochastic gradient descent）？答： “下山最快路径”的一种算法<br>我们是尝试使用偏导来衡量函数随自变量的值变化关系，选择变化更为平缓的那一处  对应的 <em>w</em> 和 <em>b</em> 作为调整后的参数</p></blockquote></li></ol><h3 id="一维视角：只考虑一个参数-w"><a href="#一维视角：只考虑一个参数-w" class="headerlink" title="一维视角：只考虑一个参数 w"></a>一维视角：只考虑一个参数 <em>w</em></h3><p><img src="https://img-blog.csdnimg.cn/3c72961665a942ac87f066a129a30183.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>如上图  learning rate（学习率）：移动的步长 一般用η来表示<br>………………………………………………………………………………………………………<br>如何寻找一个最合适的 <em>w</em> ？<br>步骤1：随机在横轴上选取一个 <em>w0</em> 。<br>步骤2：计算微分，也就是当前的斜率，根据斜率来判定移动的方向。微分大于0应向右移动（增加 <em>w</em> ）；微分小于0向左移动（减少 <em>w</em> ）<br>步骤3：根据学习率，按步骤2得到的方向移动<br>重复步骤2和步骤3，直到找到<strong>最低点</strong>。横轴即对于的最佳 <em>w</em>  参数<br>………………………………………………………………………………………………………<br>如下图，是经过迭代多次后找到的最佳点（得是全局最优解） 。注：大部分损失函数都为正<br><img src="https://img-blog.csdnimg.cn/588186b690dd43c38a10a80600802ab0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h3 id="二维视角：考虑两个参数-w-和-b"><a href="#二维视角：考虑两个参数-w-和-b" class="headerlink" title="二维视角：考虑两个参数 w 和 b"></a>二维视角：考虑两个参数 <em>w</em> 和 <em>b</em></h3><p><img src="https://img-blog.csdnimg.cn/55f2f295750242bfa35f9ac576602194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>如何寻找一个最合适的 <em>w</em> ？<br>步骤1：随机在横轴上选取一个 <em>w0</em> ，<em>b0</em> 。<br>步骤2：计算各偏导，也就是当前偏导的斜率，根据斜率来判定移动的方向。微分大于0应向右移动（增加 <em>w</em> 或 <em>b</em>）；微分小于0向左移动（减少 <em>w</em> 或 <em>b</em>）<img src="https://img-blog.csdnimg.cn/6a082b40869947c2a0723b298cd36027.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>步骤3：根据学习率，按步骤2得到的方向移动<br>重复步骤2和步骤3，直到找到<strong>最低点</strong>。横轴即对于的最佳 <em>w</em> 和 <em>b</em>  参数</p></blockquote><p>二维情况：梯度下降法的效果<br>颜色约深的区域代表的损失函数越小？ <strong>为什么呢？</strong><br><img src="https://img-blog.csdnimg.cn/299d0d615f6640a6ae092e51579d09ea.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="梯度算法的优缺点"><a href="#梯度算法的优缺点" class="headerlink" title="梯度算法的优缺点"></a>梯度算法的优缺点</h3><blockquote><p>总结一下梯度下降法：<br>　　　我们要解决使<strong>损失函数<em>L(w,b)</em></strong> 最小时参数的最佳值，梯度下降法是每次update参数值，直到损失函数最小时找到对应最佳的参数<em>w，b</em></p></blockquote><p><strong>缺点1</strong>：求解的未必是全局最优解，有可能是局部最优解。</p><blockquote><p><strong>用下山的例子讲</strong>：<br>　　　比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在<strong>每走到一个位置的时候</strong>，<strong>求解当前位置的梯度</strong>，沿着<strong>梯度的负方向</strong>，也就是<strong>当前最陡峭的位置向下走一步</strong>，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，<strong>有可能我们不能走到山脚，而是到了某一个局部的山峰低处。</strong><br>………………………………………………………………………………………………………<br>　　　从上面的解释可以看出，<strong>梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解</strong>。当然，如果损失函数是<strong>凸函数</strong>，梯度下降法得到的解就<strong>一定是</strong>全局最优解。</p></blockquote><p><strong>优点1</strong>：无论随机从哪个点出发，得到的最佳参数一定是同一组</p><blockquote><p><strong>用下山的例子讲</strong>：<br>　　　因为山只有一处是最低点，在确保能找到全局最优解的情况下，无论人从山上哪一点出发，一定能找到这特定一处的最低洼处。</p></blockquote><h2 id="Step4-回归结果分析"><a href="#Step4-回归结果分析" class="headerlink" title="Step4 回归结果分析"></a>Step4 回归结果分析</h2><blockquote><p>经过上述三个步骤后，得到了<strong>训练后</strong>的“最佳参数<em>w</em>，<em>b</em>”，那么现在这个模型在<strong>测试集</strong>上是什么表现呢？<br>得到的结果是：<br><strong>测试集的误差比在训练集上得到的损失值大</strong> 这个事非常正常。因为你训练集里面可能有些数据是不典型的，同样测试集中很多也包含了训练集中没有的因素。所以一个函数模型在实际应用中，效果基本上时打折扣的。<strong>一般会采用两种方式来加强这个函数模型</strong>：<br>　　　select another model（选择另一个模型）即增加维度，增加高此项多项式<br>　　　consider the hidden factors（考虑其他隐藏因素）</p></blockquote><h3 id="优化模型方法一：增加高次项（一般用于拟合度不够的情况）"><a href="#优化模型方法一：增加高次项（一般用于拟合度不够的情况）" class="headerlink" title="优化模型方法一：增加高次项（一般用于拟合度不够的情况）"></a>优化模型方法一：增加高次项（一般用于拟合度不够的情况）</h3><p><strong>回到Step1</strong></p><ol><li>尝试二次项、三次项、四次项、五次项…… 右上图为训练集损失，下图是测试集的损失。第五次<strong>过拟合</strong>导致了，测试集损失度很高。<img src="https://img-blog.csdnimg.cn/c707638cef5c4d698813e6acfe4ec172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/ce26490bbfc64ea6967cd7b8e1e8184d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/166aef516df14c339c45fee48a78e16a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/87e7491ce6a1464dad4c023dc9004274.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li><strong>选择合适的模型</strong>，即我到底要加到几次项呢？</li></ol><blockquote><p>越复杂的模型所包含的函数也就越多，那么它包含理想模型的可能性也就越大，但是如果过分地去拟合理想模型，就会出现<strong>过拟合</strong>的情况。<br><img src="https://img-blog.csdnimg.cn/6a394ba6b4a648a994472d8969668601.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><ol start="3"><li>横向比较各个多项次，训练集和测试集的失误。理论上，失误会随着高次项而变小，如果变大，则表示模型<strong>过拟合</strong>了。<img src="https://img-blog.csdnimg.cn/979417ba98f6475d941753e0cb71b91c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>所以，如上图所示，在为3次的时候，训练集和测试集的误差差值最小。并且4次开始，以及存在过拟合现象了。因此本案例可选择3次项线性模型。</li></ol><h3 id="优化模型方法二：考虑其他隐藏因素"><a href="#优化模型方法二：考虑其他隐藏因素" class="headerlink" title="优化模型方法二：考虑其他隐藏因素"></a>优化模型方法二：考虑其他隐藏因素</h3><p><strong>回到Step1</strong></p><blockquote><p>输入的特征包括：进化前的CP值（<em>Xcp</em>）、物种（Bulbasaur）（<em>Xs</em>）、血量（HP）（<em>Xhp</em>）、重量（Weight）（<em>Xw</em>）、高度（Height）（<em>Xh</em>）<br>除了之前考虑的进化前的CP值（<em>Xcp</em>），其他均为隐藏因素<br>………………………………………………………………………………<br><strong>这里另外考虑的是神奇宝贝的种类</strong>？因为，有时候一个模型恰恰只能符合一种神奇宝贝，也就是<strong>不同神奇宝贝应该有不同的预测模型</strong>。<img src="https://img-blog.csdnimg.cn/72a4aadb2ba24aefb4584a8a8ca2801e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><ol><li>不同的神奇宝贝有不同的预测模型<img src="https://img-blog.csdnimg.cn/46fe5297bddb46a4bd511a7cea422e3a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>拟合这个模型的曲线，可以看出不同神奇宝贝拟合成了不同的类线性直线<img src="https://img-blog.csdnimg.cn/7a55f95269d642038715e355437600f2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>如果还考虑了其他隐藏元素。如图是横轴是对应的隐藏元素，纵轴是对应进化后的CP值。<img src="https://img-blog.csdnimg.cn/591a6aa5537443428687ba15886ab11d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li><li>如果都采用最高二次项，考虑所有其他的隐藏因素。该案例的数学模型就变成 如图所示的式子。<img src="https://img-blog.csdnimg.cn/0a80407d715e4dcc978bd86dee3451a3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><blockquote><p> 考虑更多的因素反而出现了过拟合的问题，<strong>说明有些因素跟本次实验的CP值预测没有关系</strong>！<br>………………………………………………………………………………<br><strong>过拟合这么讨厌，到底如何减少过拟合问题呢？往下看！</strong></p></blockquote></li></ol><h3 id="优化模型：防止过拟合（为损失函数加一个正则项）"><a href="#优化模型：防止过拟合（为损失函数加一个正则项）" class="headerlink" title="优化模型：防止过拟合（为损失函数加一个正则项）"></a>优化模型：防止过拟合（为损失函数加一个正则项）</h3><h4 id="正则项是什么"><a href="#正则项是什么" class="headerlink" title="正则项是什么?"></a>正则项是什么?</h4><blockquote><p><strong>方法：正则化</strong>?<br>比如先考虑一个参数w，正则化就是在损失函数上加上一个与w（斜率）相关的值（正则项），那么要是loss function越小的话，w也会越小，w越小就使function更加平滑（function没那么大跳跃）<img src="https://img-blog.csdnimg.cn/7a87a38ed54c438c8af37860e982f775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h4 id="正则化的缺点"><a href="#正则化的缺点" class="headerlink" title="正则化的缺点"></a>正则化的缺点</h4><p>正则化虽然能够减少过拟合的现象，但是因为加在损失函数后面的值是平白无故加上去的，所以正则化过度的话会导致<strong>bias偏差增大</strong>   ？？？？<img src="https://img-blog.csdnimg.cn/f4c9ea7efb144c8a89b7fee964b75fa8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><blockquote><p>1）实现参数的稀疏有什么好处吗？<br>一个好处是可以简化模型，避免过拟合。<strong>因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据表现性能极差</strong>。另一个好处是参数变少可以使整个模型获得更好的可解释性。<br>2）参数值越小代表模型越简单吗？<br>是的。为什么参数越小，说明模型越简单呢，这是因为越复杂的模型，<strong>越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点</strong>，这就容易造成在<strong>较小的区间里预测值产生较大的波动</strong>，这种较大的波动也反映了在这个区间里的<strong>导数很大</strong>，而<strong>只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。</strong></p></blockquote><p><img src="https://img-blog.csdnimg.cn/9dcfac9366564857a226de8d73bf512e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@[TOC](线性回归Regression 案例一：宝可梦进化CP值)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本栗子：预测Pokemon精灵攻击力。&lt;br&gt;输入：进化前的CP值、物种（Bulbasaur）、血量（HP）、重量（Weight）、高度（Height）&lt;br&gt;输</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="线性回归" scheme="http://example.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
