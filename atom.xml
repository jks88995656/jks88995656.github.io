<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只柴犬</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-09-13T13:17:51.173Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>凯凯超人</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Pytorch 数据读取 DataLoader与Dataset 概念</title>
    <link href="http://example.com/2021/09/13/Pytorch%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%20DataLoader%E4%B8%8EDataset%20%E6%A6%82%E5%BF%B5/"/>
    <id>http://example.com/2021/09/13/Pytorch%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%20DataLoader%E4%B8%8EDataset%20%E6%A6%82%E5%BF%B5/</id>
    <published>2021-09-13T13:17:01.000Z</published>
    <updated>2021-09-13T13:17:51.173Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 数据读取 DataLoader与Dataset 概念">TOC</a></p><p>在机器学习中，我们对数据的处理主要分为4个阶段，如下图所示：<br><img src="https://img-blog.csdnimg.cn/b32a357d69624036babcbab934d6ea0f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><ol><li>第一步，收集需要的数据，数据包括原始样本 和 对应标签；</li><li>第二步，对数据集进行划分，把数据集划分为 <strong>训练集、验证集和测试集</strong> ；训练集用于训练模型，验证集用于验证模型是否过拟合（也可以理解为用验证集挑选模型的超参数），测试集用于测试模型的性能，测试模型的泛化能力；</li><li><p>第三步，从本地读取数据，要按 Mini-batch 分批训练。一起训练，内存不够。使用的方法为 <strong>DataLoader</strong> ，其又分为两个部分：</p><ol><li><strong>Sample</strong>  用于生成索引，即样本的序号；</li><li><strong>Dataset</strong> 是根据索引去读取图片以及对应的标签；</li></ol></li><li><p>第四步，数据预处理，把数据读取进来往往还需要对数据进行一系列的图像预处理，比如说<strong>数据的中心化，标准化，旋转或者翻转</strong>等等。Pytorch 中数据预处理是通过 <strong>transforms</strong> 进行处理的；</p></li></ol><h1 id="DataLoader-和-Dataset"><a href="#DataLoader-和-Dataset" class="headerlink" title="DataLoader 和 Dataset"></a>DataLoader 和 Dataset</h1><h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p><img src="https://img-blog.csdnimg.cn/d48e19e8e3a44038a1ee9e38c54ae4dd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>torch.utils.data.DataLoader</strong></p><p>功能：构建可迭代的数据装载器；<br>dataset: Dataset类，决定数据从哪里读取及如何读取；<br>batchsize：批次样本数量大小；<br>num_works:是否多进程读取数据； 可以设置为几个进程<br>shuffle：每个epoch是否乱序；<br>drop_last：当样本数不能被 batchsize 整除时，是否舍弃最后一批数据；</p></blockquote><p>Epoch，Iteration，Batchsize的区别</p><blockquote><p>Epoch：所有训练样本都已输入到模型中，称为一个Epoch<br>Iteration：一批样本输入到模型中，称之为一个Iteration；<br>Batchsize：批次样本数量大小，决定一个Epoch中有多少个Iteration； Iteration = Epoch ➗ Batchsize<br><img src="https://img-blog.csdnimg.cn/9819b23c3821402ca0fe804a641bfd00.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">样本总数：<span class="number">87</span>，Batchsize=<span class="number">8</span> （样本不能被Batchsize整除）</span><br><span class="line"><span class="number">1</span> Epoch = <span class="number">10</span> Iteration，drop_last = <span class="literal">True</span></span><br><span class="line"><span class="number">1</span> Epoch = <span class="number">11</span> Iteration， drop_last = <span class="literal">False</span></span><br></pre></td></tr></table></figure><br>用法例如：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_loader = DataLoader(dataset=diabetesdataset,batch_size=<span class="number">32</span>,num_workers=<span class="number">2</span>,shuffle=<span class="literal">True</span>,drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p><img src="https://img-blog.csdnimg.cn/4cb1ec48dac047f3b025dd652a96e11b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>torch.utils.data.Dataset</strong><br>Dataset是用来定义数据从哪里读取，以及如何读取的问题；</p><p><font color="red">功能：Dataset抽象类，所有自定义的Dataset需要继承它，并且重写<strong>getitem</strong>()； </font><br><strong> getitem </strong>()：接收一个索引，返回一个样本</p></blockquote><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DiabetesDataset 继承 Dataset</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,filepath</span>):</span></span><br><span class="line">        xy = np.loadtxt(filepath,delimiter=<span class="string">&#x27;,&#x27;</span>,dtype=np.float32)</span><br><span class="line">        <span class="comment"># 一共有几个样本</span></span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">        self.x_train = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_train = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 继承Dataset的方法并重写</span></span><br><span class="line">    <span class="comment"># 其实用来帮助找索引位置的  dataset[index]</span></span><br><span class="line">    <span class="comment"># 函数功能是根据index索引去返回数据样本以及标签label</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.x_train[index],self.y_train[index]</span><br><span class="line">    <span class="comment"># 函数功能是用来查看数据的长度，也就是 dataset 样本的数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br></pre></td></tr></table></figure><h2 id="Dataloader的运行机制"><a href="#Dataloader的运行机制" class="headerlink" title="Dataloader的运行机制"></a>Dataloader的运行机制</h2><h3 id="数据读取的三个问题：1、读哪些数据；2、从哪读数据；3、怎么读数据？"><a href="#数据读取的三个问题：1、读哪些数据；2、从哪读数据；3、怎么读数据？" class="headerlink" title="数据读取的三个问题：1、读哪些数据；2、从哪读数据；3、怎么读数据？"></a>数据读取的三个问题：1、读哪些数据；2、从哪读数据；3、怎么读数据？</h3><ol><li>从代码中可以发现，index 是从 sampler.py 中输出的，所以读哪些数据是由sampler得到的；</li><li>从代码中看，是从Dataset中的 文件地址参数 告诉我们 Pytorch 是从硬盘中的哪一个文件夹获取数据；</li><li>从代码中可以发现，Pytorch是从Dataset的getitem()中具体实现的，根据索引去读取数据；</li></ol><h3 id="DataLoader数据读取流程"><a href="#DataLoader数据读取流程" class="headerlink" title="DataLoader数据读取流程"></a>DataLoader数据读取流程</h3><p><img src="https://img-blog.csdnimg.cn/323718d3de4e415d9ba5af77011315cc.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>简单描述一下流程图，首先在for循环中去使用DataLoader，进入DataLoader之后是否采用多进程进入DataLoaderlter，进入DataLoaderIter之后会使用sampler去获取Index，拿到索引之后传输到DatasetFetcher，在DatasetFetcher中会调用Dataset，Dataset根据给定的Index，在getitem中从硬盘里面去读取实际的Img和Label，读取了一个batch_size的数据之后，通过一个collate_fn将数据进行整理，整理成batch_Data的形式，接着就可以输入到模型中训练；</p><p><strong>读哪些是由Sampler决定的，从哪读是由Dataset决定的，怎么读是由getitem决定的</strong></p><h1 id="详细原文转载"><a href="#详细原文转载" class="headerlink" title="详细原文转载"></a>详细原文转载</h1><p><a href="https://blog.csdn.net/qq_37388085/article/details/102663166">https://blog.csdn.net/qq_37388085/article/details/102663166</a></p><h1 id="实验代码"><a href="#实验代码" class="headerlink" title="实验代码"></a>实验代码</h1><p>糖尿病案例。 数据读取采用 批处理 DataLoader。<br>理论上 测试集和训练集 都应该分别有一个 DataLoader</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Dataset 是个抽象类，所以其不可以被实例化</span></span><br><span class="line"><span class="comment"># Dataset 可以为其他的子类所继承的</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;../dataset/diabetes.csv&#x27;</span>, delimiter=<span class="string">&quot;,&quot;</span>, dtype=np.float32)</span><br><span class="line"><span class="comment"># (759,9) 759行 9列</span></span><br><span class="line"><span class="comment"># 当测试样本 的 一组数据</span></span><br><span class="line">x_test = torch.from_numpy(xy[-<span class="number">1</span>:, :-<span class="number">1</span>])</span><br><span class="line">y_test = torch.from_numpy(xy[-<span class="number">1</span>:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DiabetesDataset 继承 Dataset</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filepath</span>):</span></span><br><span class="line">        xy = np.loadtxt(filepath, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">        <span class="comment"># 一共有几个样本</span></span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">        self.x_train = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_train = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 继承Dataset的方法并重写</span></span><br><span class="line">    <span class="comment"># 其实用来帮助找索引位置的  dataset[index]</span></span><br><span class="line">    <span class="comment"># 函数功能是根据index索引去返回数据样本以及标签label</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.x_train[index], self.y_train[index]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 函数功能是用来查看数据的长度，也就是 dataset 样本的数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个类</span></span><br><span class="line">diabetesdataset = DiabetesDataset(<span class="string">&#x27;../dataset/diabetes.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个 loader</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    batch_size 就是批次大小</span></span><br><span class="line"><span class="string">    shuffle True的话就表示要打乱数据</span></span><br><span class="line"><span class="string">    num_workers  读取Mni-batch的时候要多线程</span></span><br><span class="line"><span class="string">                2就表示由2个线程进行读取</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 返回 （x,y）</span></span><br><span class="line">train_loader = DataLoader(dataset=diabetesdataset, batch_size=<span class="number">32</span>, num_workers=<span class="number">2</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二部 定义模型</span></span><br><span class="line"><span class="comment"># 这个其实有点类似与 神经网络的结构了 每个线性层后面都加了一个sigmoid函数 做了次非线性变换</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullLeanerModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 定义多层线性模型的结构</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FullLeanerModel, self).__init__()</span><br><span class="line">        <span class="comment"># 第一个线性转换 将8维转换为6维</span></span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前馈 运行</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 用一个变量比较 简单</span></span><br><span class="line">        x = self.sigmoid(self.linear1(x))</span><br><span class="line">        x = self.sigmoid(self.linear2(x))</span><br><span class="line">        x = self.sigmoid(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">FullLeanerModel = FullLeanerModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 定义 损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(FullLeanerModel.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            <span class="comment"># 1. Prepare data</span></span><br><span class="line">            inputs, labels = data</span><br><span class="line">            <span class="comment"># 2. Forward</span></span><br><span class="line">            y_pred = FullLeanerModel(inputs)</span><br><span class="line">            loss = criterion(y_pred, labels)</span><br><span class="line">            <span class="comment"># 3. Backward</span></span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">            loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">            <span class="comment"># 4. Update</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第五步 评估模型</span></span><br><span class="line">    <span class="comment"># 输出各层  权重w 和 偏置b</span></span><br><span class="line">    <span class="keyword">for</span> weight, bias <span class="keyword">in</span> FullLeanerModel.state_dict().items():  <span class="comment"># param is weight or bias(Tensor)</span></span><br><span class="line">        <span class="built_in">print</span>(weight, bias)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_yuce = FullLeanerModel(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;测试样本的预测值为&quot;</span>, y_yuce.data, <span class="string">&quot;实际样本的标签值为&quot;</span>, y_test.data)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://img-blog.csdnimg.cn/6df0d9baee164ff69400094abdfff61a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 数据读取 DataLoader与Dataset 概念&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在机器学习中，我们对数据的处理主要分为4个阶段，如下图所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/b32a</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="DataLoader" scheme="http://example.com/tags/DataLoader/"/>
    
  </entry>
  
  <entry>
    <title>数据集加载的各种方式方法</title>
    <link href="http://example.com/2021/09/13/%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E7%9A%84%E5%90%84%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%96%B9%E6%B3%95/"/>
    <id>http://example.com/2021/09/13/%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E7%9A%84%E5%90%84%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%96%B9%E6%B3%95/</id>
    <published>2021-09-13T03:37:01.000Z</published>
    <updated>2021-09-13T13:18:26.089Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 实现 多维特征的输入——糖尿病预测">TOC</a></p><h1 id="全部读取-本地-csv-文件"><a href="#全部读取-本地-csv-文件" class="headerlink" title="全部读取 本地 csv 文件"></a>全部读取 本地 csv 文件</h1><p>读入本地 csv 文件内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    numpy读取本地文件</span></span><br><span class="line"><span class="string">    delimiter 数据的分割号</span></span><br><span class="line"><span class="string">    dtype 读取数据类型  一般机器学习类的都是float32 </span></span><br><span class="line"><span class="string">    因为显卡一般他内核里面是按32位工作的</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;./dataset/diabetes.csv&#x27;</span>,delimiter=<span class="string">&quot;,&quot;</span>,dtype=np.float32)</span><br></pre></td></tr></table></figure><p>用Pytorch使用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">x_train = torch.from_numpy(xy[:-<span class="number">1</span>,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line">y_train = torch.from_numpy(xy[:-<span class="number">1</span>,[-<span class="number">1</span>]])</span><br></pre></td></tr></table></figure></p><h1 id="批次读取-本地-csv-文件"><a href="#批次读取-本地-csv-文件" class="headerlink" title="批次读取 本地 csv 文件"></a>批次读取 本地 csv 文件</h1><p>使用 DataLoader+Dataset </p><h2 id="Step1：引入包"><a href="#Step1：引入包" class="headerlink" title="Step1：引入包"></a>Step1：引入包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dataset 是个抽象类，所以其不可以被实例化</span></span><br><span class="line"><span class="comment"># Dataset 可以为其他的子类所继承的</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br></pre></td></tr></table></figure><h2 id="Step2：定义一个自己的Dataset类-并继承原有的抽象类Dataset"><a href="#Step2：定义一个自己的Dataset类-并继承原有的抽象类Dataset" class="headerlink" title="Step2：定义一个自己的Dataset类 并继承原有的抽象类Dataset"></a>Step2：定义一个自己的Dataset类 并继承原有的抽象类Dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DiabetesDataset 继承 Dataset</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filepath</span>):</span></span><br><span class="line">        xy = np.loadtxt(filepath, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">        <span class="comment"># 一共有几个样本</span></span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">        self.x_train = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_train = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 继承Dataset的方法并重写</span></span><br><span class="line">    <span class="comment"># 其实用来帮助找索引位置的  dataset[index]</span></span><br><span class="line">    <span class="comment"># 函数功能是根据index索引去返回数据样本以及标签label</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.x_train[index], self.y_train[index]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 函数功能是用来查看数据的长度，也就是 dataset 样本的数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个类</span></span><br><span class="line">diabetesdataset = DiabetesDataset(<span class="string">&#x27;../dataset/diabetes.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Step3：使用DataLoader"><a href="#Step3：使用DataLoader" class="headerlink" title="Step3：使用DataLoader"></a>Step3：使用DataLoader</h2><p>其返回的是 对应索引的数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个 loader</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    batch_size 就是批次大小</span></span><br><span class="line"><span class="string">    shuffle True的话就表示要打乱数据</span></span><br><span class="line"><span class="string">    num_workers  读取Mni-batch的时候要多进程</span></span><br><span class="line"><span class="string">                2就表示由2个进程进行读取</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 返回 （x,y）</span></span><br><span class="line">train_loader = DataLoader(dataset=diabetesdataset, batch_size=<span class="number">32</span>, num_workers=<span class="number">2</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></p><h2 id="Step4：用迭代的方式拿到数据"><a href="#Step4：用迭代的方式拿到数据" class="headerlink" title="Step4：用迭代的方式拿到数据"></a>Step4：用迭代的方式拿到数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            <span class="comment"># 1. Prepare data</span></span><br><span class="line">            inputs, labels = data</span><br><span class="line">            <span class="comment"># 2. Forward</span></span><br><span class="line">            y_pred = FullLeanerModel(inputs)</span><br><span class="line">            loss = criterion(y_pred, labels)</span><br><span class="line">            <span class="comment"># 3. Backward</span></span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">            loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">            <span class="comment"># 4. Update</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 实现 多维特征的输入——糖尿病预测&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;全部读取-本地-csv-文件&quot;&gt;&lt;a href=&quot;#全部读取-本地-csv-文件&quot; class=&quot;headerlink&quot; title=&quot;全部读取 本地 csv </summary>
      
    
    
    
    <category term="数据集加载" scheme="http://example.com/categories/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="数据集" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 实现 多维特征的输入——糖尿病预测</title>
    <link href="http://example.com/2021/09/13/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E7%9A%84%E8%BE%93%E5%85%A5%E2%80%94%E2%80%94%E7%B3%96%E5%B0%BF%E7%97%85%E9%A2%84%E6%B5%8B/"/>
    <id>http://example.com/2021/09/13/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E7%9A%84%E8%BE%93%E5%85%A5%E2%80%94%E2%80%94%E7%B3%96%E5%B0%BF%E7%97%85%E9%A2%84%E6%B5%8B/</id>
    <published>2021-09-13T03:30:01.000Z</published>
    <updated>2021-09-13T13:24:52.355Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 实现 多维特征的输入——糖尿病预测">TOC</a></p><p>整体的设计思路<br>大致的设计步骤 分为5步 如下所示：<br><img src="https://img-blog.csdnimg.cn/66ed2d3a9e914f229e8c67a8248b1dee.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_9,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="实现的步骤"><br>第五步：是进行 评估模型并预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h1 id="Step1：准备数据"><a href="#Step1：准备数据" class="headerlink" title="Step1：准备数据"></a>Step1：准备数据</h1><p>数据格式包含759个样本，其中有8个特征已经其是否会加剧糖尿病的预测标签（1或0）。<br><img src="https://img-blog.csdnimg.cn/7475f37ead424166967915beceedd5ec.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="数据维度预览"><br>我们将759个样本，分为758个训练样本以及1个测试样本。</p><p>先读入本地 csv 文件内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    numpy读取本地文件</span></span><br><span class="line"><span class="string">    delimiter 分割号</span></span><br><span class="line"><span class="string">    dtype 读取数据类型  一般机器学习类的都是float32 </span></span><br><span class="line"><span class="string">    因为显卡一般他内核里面是按32位工作的</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;./dataset/diabetes.csv&#x27;</span>,delimiter=<span class="string">&quot;,&quot;</span>,dtype=np.float32)</span><br></pre></td></tr></table></figure><p>可以查看一下 读入的数据维度情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (759,9) 759行 9列</span></span><br><span class="line"><span class="built_in">print</span>(xy.shape)</span><br></pre></td></tr></table></figure><p>其中758个样本作为测试集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">x_train = torch.from_numpy(xy[:-<span class="number">1</span>,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line">y_train = torch.from_numpy(xy[:-<span class="number">1</span>,[-<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p>最后一个样本作为训练集，用于预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当测试样本 的 一组数据</span></span><br><span class="line">x_test = torch.from_numpy(xy[-<span class="number">1</span>:,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_test.shape)</span><br><span class="line">y_test = torch.from_numpy(xy[-<span class="number">1</span>:,[-<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><h1 id="Step2：定义模型"><a href="#Step2：定义模型" class="headerlink" title="Step2：定义模型"></a>Step2：定义模型</h1><p>同样每一层都为 逻辑回归模型。但这边有8个特征，所以导入的应该是个矩阵。<br><img src="https://img-blog.csdnimg.cn/79a71c6a9f0245d9914e4fcef6607793.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="怎么构建线性模型"><br><img src="https://img-blog.csdnimg.cn/ad19f89d0a57481e8f2bdb1e37e8d7ee.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="数据维度的变换 输入n行8个特征得到n行1列的预测"></p><h2 id="torch-nn-Linear-in-features-out-features-bias-True-方法"><a href="#torch-nn-Linear-in-features-out-features-bias-True-方法" class="headerlink" title="torch.nn.Linear(in_features,out_features,bias=True) 方法"></a>torch.nn.Linear(in_features,out_features,bias=True) 方法</h2><p><img src="https://img-blog.csdnimg.cn/0e75527101244e56a7687acb4eaa9315.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="torch.nn.Linear函数详解"><br>下面这个函数 使得8维度线性变换为6维度<br>注：1.整个模型都是以 列向量为操作单位的（这么做其实是为了利用计算机的并行计算能力加快训练速度），所以维度指的是有多少列，比如左下角这个维度就是8。<br>2.激活函数的引入，其实就是为了引入非线性的因素。这样就可以使得我们可以非线性的变换矩阵维度。<br><img src="https://img-blog.csdnimg.cn/1c73a9cbdba047818bda33c1c9ab517f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="8D到6D的真正含义"><br>下面是我们 构建的整体的 线性网络模型<br><img src="https://img-blog.csdnimg.cn/06e1c8bb27054087bce6ccb4c0eee8c0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="整体网络模型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二部 定义模型</span></span><br><span class="line"><span class="comment"># 这个其实有点类似与 神经网络的结构了 每个线性层后面都加了一个sigmoid函数 做了次非线性变换</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullLeanerModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 定义多层线性模型的结构</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FullLeanerModel,self).__init__()</span><br><span class="line">        <span class="comment"># 第一个线性转换 将8维转换为6维</span></span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>,<span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>,<span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前馈 运行输出结果</span></span><br><span class="line">    <span class="comment"># 会被自动调用 是方法的重写</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="comment"># 用一个变量比较 简单</span></span><br><span class="line">        x = self.sigmoid(self.linear1(x))</span><br><span class="line">        x = self.sigmoid(self.linear2(x))</span><br><span class="line">        x = self.sigmoid(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span>  x</span><br></pre></td></tr></table></figure><p>实例化这个模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">FullLeanerModel = FullLeanerModel()</span><br></pre></td></tr></table></figure><h1 id="Step3：定义损失函数和优化器"><a href="#Step3：定义损失函数和优化器" class="headerlink" title="Step3：定义损失函数和优化器"></a>Step3：定义损失函数和优化器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三步 定义 损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(FullLeanerModel.parameters(),lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h1 id="Step4：训练模型"><a href="#Step4：训练模型" class="headerlink" title="Step4：训练模型"></a>Step4：训练模型</h1><p>训练模型100次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>):</span><br><span class="line">    y_pred = FullLeanerModel(x_train)</span><br><span class="line">    loss = criterion(y_pred,y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br></pre></td></tr></table></figure><h1 id="评估模型并预测"><a href="#评估模型并预测" class="headerlink" title="评估模型并预测"></a>评估模型并预测</h1><h2 id="输出所有层次的-权重和偏置"><a href="#输出所有层次的-权重和偏置" class="headerlink" title="输出所有层次的 权重和偏置"></a>输出所有层次的 权重和偏置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># 输出各层  权重w 和 偏置b</span></span><br><span class="line"><span class="keyword">for</span> weight, bias <span class="keyword">in</span> FullLeanerModel.state_dict().items():  <span class="comment"># param is weight or bias(Tensor)</span></span><br><span class="line"><span class="built_in">print</span>( weight,bias)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/5c3c93e1544a4005873d7fabd4f88e51.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="各层次的权重和偏置值"></p><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_yuce = FullLeanerModel(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试样本的预测值为&quot;</span>,y_yuce.data,<span class="string">&quot;实际样本的标签值为&quot;</span>,y_test.data)</span><br></pre></td></tr></table></figure><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    numpy读取本地文件</span></span><br><span class="line"><span class="string">    delimiter 分割号</span></span><br><span class="line"><span class="string">    dtype 读取数据类型  一般机器学习类的都是float32 因为显卡一般他内核里面是按32位工作的</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;./dataset/diabetes.csv&#x27;</span>,delimiter=<span class="string">&quot;,&quot;</span>,dtype=np.float32)</span><br><span class="line"><span class="comment"># (759,9) 759行 9列</span></span><br><span class="line"><span class="built_in">print</span>(xy.shape)</span><br><span class="line"><span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">x_train = torch.from_numpy(xy[:-<span class="number">1</span>,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line">y_train = torch.from_numpy(xy[:-<span class="number">1</span>,[-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当测试样本 的 一组数据</span></span><br><span class="line">x_test = torch.from_numpy(xy[-<span class="number">1</span>:,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_test.shape)</span><br><span class="line">y_test = torch.from_numpy(xy[-<span class="number">1</span>:,[-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二部 定义模型</span></span><br><span class="line"><span class="comment"># 这个其实有点类似与 神经网络的结构了 每个线性层后面都加了一个sigmoid函数 做了次非线性变换</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullLeanerModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 定义多层线性模型的结构</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FullLeanerModel,self).__init__()</span><br><span class="line">        <span class="comment"># 第一个线性转换 将8维转换为6维</span></span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>,<span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>,<span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前馈 运行</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="comment"># 用一个变量比较 简单</span></span><br><span class="line">        x = self.sigmoid(self.linear1(x))</span><br><span class="line">        x = self.sigmoid(self.linear2(x))</span><br><span class="line">        x = self.sigmoid(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span>  x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">FullLeanerModel = FullLeanerModel()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 定义 损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(FullLeanerModel.parameters(),lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>):</span><br><span class="line">    y_pred = FullLeanerModel(x_train)</span><br><span class="line">    loss = criterion(y_pred,y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss.item())</span><br><span class="line">![在这里插入图片描述](https://img-blog.csdnimg.cn/f600a0b6fbe24b60b35a0f2bc26f9444.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16<span class="comment">#pic_center)</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五步 评估模型</span></span><br><span class="line">    <span class="comment"># 输出各层  权重w 和 偏置b</span></span><br><span class="line"><span class="keyword">for</span> weight, bias <span class="keyword">in</span> FullLeanerModel.state_dict().items():  <span class="comment"># param is weight or bias(Tensor)</span></span><br><span class="line">    <span class="built_in">print</span>( weight,bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_yuce = FullLeanerModel(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试样本的预测值为&quot;</span>,y_yuce.data,<span class="string">&quot;实际样本的标签值为&quot;</span>,y_test.data)</span><br></pre></td></tr></table></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="https://img-blog.csdnimg.cn/d4fac45272664d249443217900408b79.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="实验结果"><br>可以看到 最后一个样本模型预测概率是 0.6529 。&gt;0.5 我们可以推测其标签就是1，而实际标签也是1，所以这个模型预测结果目前看是正确的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 实现 多维特征的输入——糖尿病预测&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;整体的设计思路&lt;br&gt;大致的设计步骤 分为5步 如下所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/66ed2d3a9e914f</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="多维度特征" scheme="http://example.com/tags/%E5%A4%9A%E7%BB%B4%E5%BA%A6%E7%89%B9%E5%BE%81/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 实现 简易逻辑回归模型 —— 刘二</title>
    <link href="http://example.com/2021/09/12/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%AE%80%E6%98%93%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/"/>
    <id>http://example.com/2021/09/12/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%AE%80%E6%98%93%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/</id>
    <published>2021-09-12T03:30:01.000Z</published>
    <updated>2021-09-13T13:25:10.303Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 实现 简易逻辑回归模型 —— 刘二">TOC</a></p><p>用Pytorch 实现 简单的逻辑回归。整个流程图可以如下图所示：<br><img src="https://img-blog.csdnimg.cn/ed9e3efcda894426bc9e66c6c0c5d408.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_9,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="逻辑回归预测流程"></p><p>大致的设计步骤 分为5步 如下所示：<br><img src="https://img-blog.csdnimg.cn/66ed2d3a9e914f229e8c67a8248b1dee.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_9,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>第五步：是进行 评估模型并预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><h1 id="Step1：准备数据"><a href="#Step1：准备数据" class="headerlink" title="Step1：准备数据"></a>Step1：准备数据</h1><p><img src="https://img-blog.csdnimg.cn/91dc702ff784417993b5d0e5f63f75ed.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><h1 id="Step2：设计模型"><a href="#Step2：设计模型" class="headerlink" title="Step2：设计模型"></a>Step2：设计模型</h1><p>内涵的线性模型比较简单 为 $y=Wx+b$ 只有两个超参数 W 和 b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二步 设计模型</span></span><br><span class="line"><span class="comment"># 构建一个线性模型类 所有的模型类都必须继承torch.nn.Module</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 调用父类的构造 这步必须得有</span></span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        <span class="comment"># Linear 是一个模型类 这边实例化他给 linear</span></span><br><span class="line">        <span class="comment"># w 权重 = 1  b 偏置 = 1</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 线性模型之后 在外套sigmoid激活函数</span></span><br><span class="line">        y_pred = torch.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure><h2 id="torch-nn-Linear-in-features-out-features-bias-True-方法"><a href="#torch-nn-Linear-in-features-out-features-bias-True-方法" class="headerlink" title="torch.nn.Linear(in_features,out_features,bias=True) 方法"></a>torch.nn.Linear(in_features,out_features,bias=True) 方法</h2><p><img src="https://img-blog.csdnimg.cn/0e75527101244e56a7687acb4eaa9315.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><p>实例化这个模型为 model</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化这个模型</span></span><br><span class="line">model = LogisticRegressionModel()</span><br></pre></td></tr></table></figure><h1 id="Step3：构建损失函数和优化器"><a href="#Step3：构建损失函数和优化器" class="headerlink" title="Step3：构建损失函数和优化器"></a>Step3：构建损失函数和优化器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三步 构建损失函数和优化器</span></span><br><span class="line"><span class="comment"># BCELoss  Binary Cross Entropy</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>) <span class="comment"># size_average = True 的话 就 乘以 1/N  默认为true</span></span><br><span class="line"><span class="comment">#model.parameters() 可以找到模型所有需要训练的参数</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>) </span><br></pre></td></tr></table></figure><h2 id="torch-nn-BCELoss-size-average-False-方法"><a href="#torch-nn-BCELoss-size-average-False-方法" class="headerlink" title="torch.nn.BCELoss(size_average=False) 方法"></a>torch.nn.BCELoss(size_average=False) 方法</h2><p>用于创建一个 BCE 损失函数<br><img src="https://img-blog.csdnimg.cn/c901445a0aad460d8c7c50e0a9287cd8.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h2 id="torch-optim-SGD-…-方法"><a href="#torch-optim-SGD-…-方法" class="headerlink" title="torch.optim.SGD(…) 方法"></a>torch.optim.SGD(…) 方法</h2><p>优化器选择 SGD   可调整学习率<br>$w^{*} = w - α\frac{\partial L}{\partial W}$<br><img src="https://img-blog.csdnimg.cn/aab4fe33c3844ea386a7c14cc9350865.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="Step4-训练模型"><a href="#Step4-训练模型" class="headerlink" title="Step4: 训练模型"></a>Step4: 训练模型</h1><p> 前两步 就是正向传播 forward</p><ol><li>预测 标签</li><li>预测 与 实际 算出损失值</li><li>反向传播 backward 优化参数</li><li>更新参数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss.item() )</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br></pre></td></tr></table></figure><h1 id="Step5-评估模型并预测"><a href="#Step5-评估模型并预测" class="headerlink" title="Step5: 评估模型并预测"></a>Step5: 评估模型并预测</h1><p>这边没有准备 测试集及其标签</p><p>输出 超参数  权重w 和 偏置b<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出 超参数  权重w 和 偏置b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>,model.linear.weight.item() )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>,model.linear.bias.item())</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测 </span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x为4.0 预测的 y值为：&#x27;</span>,y_test.data)</span><br></pre></td></tr></table></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><font color="red"> 这边少了 怎么输出 准确率？ </font><br><img src="https://img-blog.csdnimg.cn/e9bb2b08573e4016bb049d6289513ec9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="可以画一下图"><a href="#可以画一下图" class="headerlink" title="可以画一下图"></a>可以画一下图</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">200</span>)</span><br><span class="line">x_t = torch.Tensor(x).view((<span class="number">200</span>, <span class="number">1</span>))</span><br><span class="line">y_t = model(x_t)</span><br><span class="line">y = y_t.data.numpy()</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">10</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>], c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability of Pass&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="实现源码"><a href="#实现源码" class="headerlink" title="实现源码"></a>实现源码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 第一步 载入数据</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步 设计模型</span></span><br><span class="line"><span class="comment"># 构建一个线性模型类 所有的模型类都必须继承torch.nn.Module</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 调用父类的构造 这步必须得有</span></span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        <span class="comment"># Linear 是一个模型类 这边实例化他给 linear</span></span><br><span class="line">        <span class="comment"># w 权重 = 1  b 偏置 = 1</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 线性模型之后 在外套sigmoid激活函数</span></span><br><span class="line">        y_pred = torch.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个模型</span></span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 构建损失函数和优化器</span></span><br><span class="line"><span class="comment"># BCELoss  Binary Cross Entropy</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>) <span class="comment"># size_average = True 的话 就 乘以 1/N  默认为true</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>) <span class="comment">#model.parameters() 可以找到</span></span><br><span class="line"><span class="comment"># 模型所有需要训练的参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss.item() )</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五步 评估模型</span></span><br><span class="line"><span class="comment"># 输出 超参数  权重w 和 偏置b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>,model.linear.weight.item() )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>,model.linear.bias.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x为4.0 预测的 y值为：&#x27;</span>,y_test.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">200</span>)</span><br><span class="line">x_t = torch.Tensor(x).view((<span class="number">200</span>, <span class="number">1</span>))</span><br><span class="line">y_t = model(x_t)</span><br><span class="line">y = y_t.data.numpy()</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">10</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>], c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability of Pass&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 实现 简易逻辑回归模型 —— 刘二&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;用Pytorch 实现 简单的逻辑回归。整个流程图可以如下图所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/ed9e3efcda</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="逻辑回归" scheme="http://example.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>神经网络与深度学习 邱锡鹏 学习笔记（机器学习）</title>
    <link href="http://example.com/2021/09/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E9%82%B1%E9%94%A1%E9%B9%8F%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%89/"/>
    <id>http://example.com/2021/09/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E9%82%B1%E9%94%A1%E9%B9%8F%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%89/</id>
    <published>2021-09-11T07:46:01.000Z</published>
    <updated>2021-09-13T04:58:39.371Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="神经网络与深度学习 邱锡鹏 学习笔记（机器学习）">TOC</a></p><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><p>机器学习看作一个从有限、高维、有噪声的数据上得到更一般性规律的泛化问题．<br><img src="https://img-blog.csdnimg.cn/b5f2ae1f8a044527984d6b7a4e1d2190.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="机器学习过程"></p><h2 id="机器学习的三个基本要素"><a href="#机器学习的三个基本要素" class="headerlink" title="机器学习的三个基本要素"></a>机器学习的三个基本要素</h2><p><img src="https://img-blog.csdnimg.cn/0e48590c034949c6917b8eeeb555ad30.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="机器学习三要素"></p><ul><li>模型</li><li>学习准则</li><li>优化算法</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>我们不知道 输入和输出是如何对应联系的，也就是数学函数究竟是什么。 我们假设一个可能的函数集合 $ℱ$ ，称为 假设空间 。 我们根据 观测 各函数在测试集 $𝒟$ 的表现，从中选择更理想的假设模型。<br>常用的假设空间分为 <strong>线性和非线性</strong> 两种</p><h2 id="学习准则"><a href="#学习准则" class="headerlink" title="学习准则"></a>学习准则</h2><p>令训练集 是由 $𝑁$ 个独立同分布的样本组成，即每个样本 (𝒙, 𝑦) ∈ 𝒳 × 𝒴 是从 𝒳  和 𝒴  的联合空间中按照某个未知分布 $𝑝_{r}(𝒙, 𝑦)$ 独立地随机产生的 。</p><p>学习准则包括：</p><ul><li>经验风险最小化</li><li>结构风险最小化</li><li>最大似然估计</li><li>最大后验估计</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>用来量化模型预测和真实标签之间的差异</p><h4 id="0-1损失函数"><a href="#0-1损失函数" class="headerlink" title="0-1损失函数"></a>0-1损失函数</h4><h4 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h4><p>经常用在预测标签 $𝑦$ 为实数值的任务中</p><script type="math/tex; mode=display">L(y,f(x;θ)) = \frac{1}{2}(y-f(x;θ))</script><p><strong>平方损失函数一般不适用于分类问题</strong> </p><h4 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h4><p>一般用于分类问题</p><script type="math/tex; mode=display">L(y,f(x;θ)) =-\sum_{c=1}^{C}y_{c}\log f_{c}(x;θ)</script><p>例如，对于一个三分类的问题，一个样本的标签向量为 $𝒚 = [0, 0, 1]^{T}$，模型预测的标签分布为 $f(x;θ) = [0.3, 0.3, 0.4]^{T}$。<br>则它们的交叉熵为 $−(0 × log(0.3) + 0 ×log(0.3) + 1 × log(0.4)) = − log(0.4)$</p><h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><blockquote><p><strong>过拟合</strong><br>模型在训练集上错误率 很低，但是在未知数据上错误率很高，这就是所谓的过拟合。<br>过拟合的 3种 方法： 参数在过拟合之前就停止更新；正则化Regularization；<br>dropout</p><p><strong>欠拟合</strong><br>模型不能很好地拟合训练数据，在训练集上的错误率比较高．欠拟合一般是由于模型能力不足造成的。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/0bb493818f134cfe924b74c3f5e9a772.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><h4 id="参数与超参数"><a href="#参数与超参数" class="headerlink" title="参数与超参数"></a>参数与超参数</h4><ul><li>参数：模型 𝑓(𝒙; 𝜃)中的 𝜃 称为模型的参数，可以通过优化算法进行学习</li><li>超参数：用来定义模型结构或优化策略的。 <font color="blue">常见的超参数有：聚类算法中的类别个数、梯度下降法中的步长、正则化项的系数、神经网络的层数、支持向量机中的核函数等 </font></li></ul><p>参数的优化一般都是由优化器优化，而超参数优化是机器学习的 一个经验性很强的技术 ，通常是按照人的经验设定，或者通过搜索的方法对一组超参数组合进行不断试错调整。</p><h4 id="批量梯度下降法"><a href="#批量梯度下降法" class="headerlink" title="批量梯度下降法"></a>批量梯度下降法</h4><p>缺点在于 局部最优问题<br>用下面的迭代公式来计算计算训练集 𝒟 上风险函数的最小值：<br><img src="https://img-blog.csdnimg.cn/4703fad601d345569a9950cc173f436d.png#pic_center" alt="批量梯度下降法"><br>𝛼 一般称为学习率（Learning Rate）．</p><blockquote><p>批量梯度下降法在  每次迭代时<strong>需要计算每个样本（也就是所有样本）</strong>上损失函数的梯度并求和。当训练集中的样本数量 𝑁 很大时，空间复杂度比较高，每次迭代的计算开销也很大．</p></blockquote><h4 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h4><p>缺点在于 局部最优问题，但相比于批量，更容易脱离局部最优</p><blockquote><p>为了减少每次迭代的计算复杂度，我们也可以在每次迭代时<strong>只采集一个样本</strong>，计算这个样本损失函数的梯度并更新参数，即随机梯度下降法。当经过足够次数的迭代时，随机梯度下降 也可以收敛到局部最优解。<br><img src="https://img-blog.csdnimg.cn/4770f90631bb4ef3aa53847f7242a206.png#pic_center" alt="随机梯度下降法"></p><h4 id="小批量梯度下降法"><a href="#小批量梯度下降法" class="headerlink" title="小批量梯度下降法"></a>小批量梯度下降法</h4><p><font color="red">现在 大规模的机器学习 常用这个。</font><br><strong>利用了计算机的并行计算能力</strong><br>每次迭代时，我们随机选取一小部分训练样本来计算梯度并更新参数，这样既可以兼顾随机梯度下降法的优点，也可以提高训练效率。<br><img src="https://img-blog.csdnimg.cn/36fa218d071f4a399d141fa6641232ae.png#pic_center" alt="小批量梯度下降法"></p><h1 id="机器学习的简单示例——线性回归"><a href="#机器学习的简单示例——线性回归" class="headerlink" title="机器学习的简单示例——线性回归"></a>机器学习的简单示例——线性回归</h1><p>自变量数量 为1时称为 <strong>简单回归</strong>， 自变量数量大于1时称为 <strong>多元回归</strong>．</p><script type="math/tex; mode=display">𝑓(𝒙; 𝒘) = 𝒘^{T}𝒙</script></blockquote><h2 id="不明白-先跳过"><a href="#不明白-先跳过" class="headerlink" title="不明白 先跳过"></a>不明白 先跳过</h2><h1 id="偏差-方差分解"><a href="#偏差-方差分解" class="headerlink" title="偏差- 方差分解"></a>偏差- 方差分解</h1><p>其用在，对如何对模型的拟合能力和复杂度之间取得一个良好的平衡，偏差与方差起这很好的分析和指导作用。</p><p>对于单个样本 𝒙，不同训练集 𝒟 得到模型 $𝑓_{D}(𝒙)$ 和最优模型 $𝑓^{*}(𝒙)$ 的期望  差距为：<br><img src="https://img-blog.csdnimg.cn/0568101873af46f9ac794e591d7cfb19.png#pic_center" alt="在这里插入图片描述"></p><ul><li>偏差：指一个模型在 <font color="red">不同训练集</font> 上的<strong>平均性能和最优模型的差异</strong>，可以用来衡量一个模型的<strong>拟合能力</strong></li><li>方差：一个模型在不同训练集上的差异 ，可以用来衡量一个模型是否容易过拟合 ．</li></ul><p><img src="https://img-blog.csdnimg.cn/99942f3b15814cacbb5dcfb5a53cb056.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_9,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="机器学习模型的四种偏差和方差组合情况"><br>每个图的中心点为<br>最优模型 $𝑓^{*}(𝒙)$，黑点为不同训练集𝐷 上得到的模型 $𝑓_{D}(𝒙)$</p><ul><li>高偏差<strong>低方差</strong>的情况，表示模型的泛化能力很好，但<strong>拟合能力不足</strong></li><li>低偏差<strong>高方差</strong>的情况，表示模型的<strong>拟合能力很好</strong>，但泛化能力比较差．当训练数据比较少时会导致过拟合</li></ul><p>总结的来看：<br>模型在<strong>训练集</strong>上的错误率比较高时，说明模型的<strong>拟合能力不够</strong>，<strong>偏差比较高</strong>。这种情况可以通过</p><ul><li>增加数据特征</li><li>提高模型复杂度</li><li>减小正则化系数</li></ul><p>模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高．这种情况可以通过</p><ul><li>降低模型复杂度</li><li>加大正则化系数</li><li>引入先验</li><li>此外，还有一种有效降低方差的方法为<strong>集成模型</strong>，即通过多个高方差模型的平均来降低方差．</li></ul><h1 id="学习算法分类"><a href="#学习算法分类" class="headerlink" title="学习算法分类"></a>学习算法分类</h1><p>按照训练样本提供的信息以及反馈方式的不同，将机器学习算法分为以下几类：</p><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>机器学习的特征 𝒙 和标签 𝑦 之间可以用 数学模型表示出来，并且训练集中每个样本都有标签。（数据集标签一般都需要由人工进行标注，成本很高）<br>根据标签的类型还可以分为：</p><ul><li>回归。 标签 𝑦 与 模型的输出 都是连续值</li><li>分类。 标签 𝑦 是离散的类别（符号）。这种模型又叫做 分类器 。 分类问题可以分为 二分类 多分类。</li><li>结构化学习。 特殊的分类问题。标签 𝒚 通常是结构化的对象，比如序列、树或图等。 由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将 𝒙 ,  𝒚 映射为该空间中的联合特征向量 𝜙(𝒙, 𝒚)，预测模型可以写为 <img src="https://img-blog.csdnimg.cn/e6f1ad9f44424166b35a7bf4def1dbb4.png#pic_center" alt="在这里插入图片描述"><br>其中 Gen(𝒙) 表示输入 𝒙 的所有可能的输出目标集合．计算 arg max 的过程也称为<strong>解码</strong>（Decoding）过程，一般通过<strong>动态规划</strong>的方法来计算。</li></ul><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p> 是指从不包含目标标签的训练样本中自动学习到一些有价值的信息。典型的无监督学习问题有 </p><ul><li>聚类</li><li>密度估计</li><li>特征学习</li><li>降维</li></ul><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p> 是一类通过交互来学习的机器学习算法．在强化学习中，智能体根据环境的状态做出一个动作，并得到即时或延时的奖励．智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报．</p><h2 id="弱监督和半监督"><a href="#弱监督和半监督" class="headerlink" title="弱监督和半监督"></a>弱监督和半监督</h2><p>弱监督学习和半监督学习的方法，希望从大规模的无标注数据中充分挖掘有用的信息，降低对标注样本数量的要求。</p><p>强化学习和监督学习的不同在于，强化学习不需要显式地以“输入/输出对”的方式给出训练样本，是一种在线的学习机制。</p><h1 id="数据的特征表示"><a href="#数据的特征表示" class="headerlink" title="数据的特征表示"></a>数据的特征表示</h1><p><img src="https://img-blog.csdnimg.cn/deb2a7eee01647089fa86ebbd5f6acd8.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><p>需要将这些不同类型的数据转换为向量表示 。</p><p>如何选取有效的特征，具体可分为两种：特征选择和特征抽取。 <strong>传统的是和预测模型的学习分离的。</strong></p><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>特征选择就是保留有用特征，移除冗余或无关的特征。<br>方法包括 子集搜索和 L1正则化 </p><h3 id="子集搜索"><a href="#子集搜索" class="headerlink" title="子集搜索"></a>子集搜索</h3><ul><li>过滤式方法：不依赖具体机器学习模型的特征选择方法。每次增加最有信息量的特征，或删除最没有信息量的特征。</li><li>包裹式方法（Wrapper Method）是使用后续机器学习模型的准确率作为评价来选择一个特征子集的方法．每次增加对后续机器学习模型最有用的特征，或删除对后续机器学习任务最无用的特征。</li></ul><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>由于 L1 正则化会导致稀疏特征，因此间接实现了特征选择．</p><h2 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h2><p> 构造一个新的特征空间，并将原始特征投影在新的空间中得到新的表示。 其方法分为 <strong>有监督和无监督</strong>两类。</p><ul><li><strong>监督</strong>的特征学习的目标是抽取对一个特定的预测任务最有用的特征，比如线性判别分析。</li><li><strong>无监督</strong>的特征学习和具体任务无关，其目标通常是减少冗余信息和噪声，比如<strong>主成分分析PCA</strong>和<strong>自编码器 AE</strong>。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://img-blog.csdnimg.cn/502589206ad64f6898d51126bd9233ba.png#pic_center" alt="传统的特征选择和特征抽取方法"><br>特征选择和特征抽取的优点是可以用较少的特征来表示原始特征中的大部分相关信息，去掉噪声信息，并进而提高计算效率和减小维度灾难。<br>对于很多没有正则化的模型，特征选择和特征抽取非常必要。 经过特征选择或特征抽取后，特征的数量一般会减少，因此特征选择和特征抽取。 也经常称为<strong>维数约减或降维</strong>。</p><h1 id="模型的评价指标"><a href="#模型的评价指标" class="headerlink" title="模型的评价指标"></a>模型的评价指标</h1><ul><li>准确率（Accuracy）</li><li>错误率（Error Rate）： 1 - 准确率</li><li>查准率（Precision）</li><li>查全率（Recall）</li><li>F值（F Measure）</li></ul><h2 id="如何理解-精确率、召回率和F值"><a href="#如何理解-精确率、召回率和F值" class="headerlink" title="如何理解 精确率、召回率和F值"></a>如何理解 精确率、召回率和F值</h2><p>模型在测试集上的结果可以分为以下四种情况：<br>比如我们现在 预测标签 C</p><ul><li>真正例（TP）：样本的预测与实际标签相同</li><li>假负例（FN）：样本实际标签为C，模型预测错了</li><li>假正例（FP）：样本实际标签不是C，但模型预测成C了</li><li>真负例（TN）：样本实际为其他类，模型也预测为其他类</li></ul><p><img src="https://img-blog.csdnimg.cn/e157318baff446ce9215cf7854120df2.png#pic_center" alt="在这里插入图片描述"></p><h3 id="查准率"><a href="#查准率" class="headerlink" title="查准率"></a>查准率</h3><p>类别 𝑐 的查准率  是所有预测为 类别C 的样本中 预测正确  的比例<br>精确率 $P_{C}$ 的计算 公式为：</p><script type="math/tex; mode=display">P_{C} = \frac{TP_{c}}{TP_{c}+FP_{c}}</script><h3 id="查全率"><a href="#查全率" class="headerlink" title="查全率"></a>查全率</h3><p>类别𝑐的查全率 是所有真实标签为 类别𝑐 的样本中预测正确的比例：</p><script type="math/tex; mode=display">R_{C} = \frac{TP_{c}}{TP_{c}+FN_{c}}</script><h3 id="F值"><a href="#F值" class="headerlink" title="F值"></a>F值</h3><p>F值（F Measure）是一个综合指标，为精确率和召回率的调和平均：</p><script type="math/tex; mode=display">F_{C} = \frac{(1+β^{2})×P_{C}×R_{C}}{β^{2}×P_{C}+R_{C}}</script><p>其中 𝛽 用于平衡精确率和召回率的重要性，一般取值为1．𝛽 = 1时的F值称为 <strong>F1 值</strong>，是精确率和召回率的调和平均．</p><h2 id="宏平均和微平均"><a href="#宏平均和微平均" class="headerlink" title="宏平均和微平均"></a>宏平均和微平均</h2><p>为了计算分类算法在<strong>所有类别上的总体查准率、查全率和 F1值</strong>，经常使用两种平均方法，分别称为 宏平均 和 微平均</p><ul><li>宏平均是每一类的性能指标的算术平均值</li><li>微平均是每一个样本的性能指标的算术平均值</li></ul><h1 id="理论和定理"><a href="#理论和定理" class="headerlink" title="理论和定理"></a>理论和定理</h1><h2 id="PAC学习理论："><a href="#PAC学习理论：" class="headerlink" title="PAC学习理论："></a>PAC学习理论：</h2><p>指该学习算法能够在多项式时间内从合理数量的训练数据中学习到一个近似正确的𝑓(𝒙)．</p><h2 id="没有免费午餐定理"><a href="#没有免费午餐定理" class="headerlink" title="没有免费午餐定理"></a>没有免费午餐定理</h2><p>没有免费午餐定理   就是不存在一种机器学习算法适合于任何领域或任务。</p><h2 id="奥卡姆剃刀原理"><a href="#奥卡姆剃刀原理" class="headerlink" title="奥卡姆剃刀原理"></a>奥卡姆剃刀原理</h2><p>简单的模型泛化能力更好．如果有两个性能相近的模型，我们应该选择<strong>更简单的模型</strong>。<br>因此，在机器学习的学习准则上，我们经常会引入参数正则化来限制模型能力，避免过拟合．</p><h2 id="丑小鸭定理"><a href="#丑小鸭定理" class="headerlink" title="丑小鸭定理"></a>丑小鸭定理</h2><p>世界上不存在相似性的客观标准，一切相似性的标准都是主观的</p><h2 id="归纳偏置"><a href="#归纳偏置" class="headerlink" title="归纳偏置"></a>归纳偏置</h2><p>预测模型前，先假设。<br>比如在最近邻分类器中，我们会假设在特征空间中，一个小的局部区域中的大部分样本同属一类。<br>在朴素贝叶斯分类器中，我们会假设每个特征的条件概率是互相独立的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;神经网络与深度学习 邱锡鹏 学习笔记（机器学习）&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;机器学习&quot;&gt;&lt;a href=&quot;#机器学习&quot; class=&quot;headerlink&quot; title=&quot;机器学习&quot;&gt;&lt;/a&gt;机器学习&lt;/h1&gt;&lt;p&gt;机器学习看作一个从有限</summary>
      
    
    
    
    <category term="深度学习基础-邱锡鹏" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%82%B1%E9%94%A1%E9%B9%8F/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="邱锡鹏" scheme="http://example.com/tags/%E9%82%B1%E9%94%A1%E9%B9%8F/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络与深度学习 学习笔记（第一章）</title>
    <link href="http://example.com/2021/09/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%89/"/>
    <id>http://example.com/2021/09/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%89/</id>
    <published>2021-09-10T08:42:01.000Z</published>
    <updated>2021-09-13T04:58:33.835Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="神经网络与深度学习 邱锡鹏 学习笔记（第一章）">TOC</a></p><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="浅层学习"><a href="#浅层学习" class="headerlink" title="浅层学习"></a>浅层学习</h2><blockquote><p>学习一个预测模型．一般需要首先将数据表示为一组特征（Feature），特征的表示形式可以是连续的数值、离散的符号或其他形式．然后将这些特征输入到预测模型，并输出预测结果．这类机器学习可以看作浅层学习（Shallow  Learning）</p></blockquote><h2 id="机器学习的步骤"><a href="#机器学习的步骤" class="headerlink" title="机器学习的步骤"></a>机器学习的步骤</h2><ol><li>数据预处理：对数据的原始形式进行初步的数据清理（比如去掉一些有缺失特征的样本，或去掉一些冗余的数据特征等）和加工（对数值特征进行缩放和归一化等），并构建成可用于训练机器学习模型的数据集．</li><li>特征提取：从数据的原始特征中提取一些对特定机器学习任务有用的高质量特征．比如在图像分类中提取边缘、尺度不变特征变换（Scale Invariant<br>Feature Transform，SIFT）特征，在文本分类中去除停用词等．</li><li>特征转换：对特征进行进一步的加工，比如降维和升维． 很多特征转换方法也都是机器学习方法．降维包括特征抽取（Feature Extraction）和特征选择（FeatureSelection）两种途径．常用的特征转换方法有主成分分析（Principal Components Analysis，PCA）、 线性判别分析（Linear Discriminant Analysis，LDA）等．</li><li>预测：机器学习的核心部分，学习一个函数并进行预测．</li></ol><p><img src="https://img-blog.csdnimg.cn/f3d73fb28f1342f48a6ca08638b8da68.png#pic_center" alt="传统机器学习的数据处理流程"><br>注：很多的机器学习问题变成了特征工程（Feature Engineering）问题．开发一个机器学习系统的主要工作量都消耗在了预处理、特征提取以及特征转换上。</p><h1 id="表示学习"><a href="#表示学习" class="headerlink" title="表示学习"></a>表示学习</h1><p>在表示学习中，有两个核心问题：</p><ul><li>一是“什么是一个好的表示”；即表示 需要包含更高层的语义信息</li><li>二是“如何学习到好的表示”．</li></ul><p>传统的特征学习一般是通过<font color="red">人为地设计一些准则</font>，然后根据这些准则来选取有效的特征。 所以 特征的学习是和最终预测模型的学习分开进行的，<font color="red">因此学习到的特征不一定可以提升最终模型的性能．</font></p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>将输入信息转换为有效的特征。<br>如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作表示学习（Representation Learning）．</p><h2 id="语义鸿沟"><a href="#语义鸿沟" class="headerlink" title="语义鸿沟"></a>语义鸿沟</h2><p>语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性。</p><blockquote><p>比如车，图片中每辆车的颜色和形状等属性都不尽相同，因此不同图片在像素级别上的表示（即底层特征）差异性也会非常大．但是我们理解这些图片是建立在比较抽象的高层语义概念上的</p></blockquote><h2 id="表示特征的方式"><a href="#表示特征的方式" class="headerlink" title="表示特征的方式"></a>表示特征的方式</h2><ol><li>局部表示：例如，one-hot向量 表示颜色。 缺点在于多个颜色就多个列或者行 </li><li>分布式表示：RGB 表示颜色</li></ol><p><img src="https://img-blog.csdnimg.cn/aa86242673364da09f8245d248914cbd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="局部表示和分布式表示示例"></p><h2 id="嵌入"><a href="#嵌入" class="headerlink" title="嵌入"></a>嵌入</h2><p>嵌入通常指将一个度量空间中的一些对象映射到另一个低维的度量空间中，并尽可能保持不同对象之间的拓扑关系．<font color="blue">比如自然语言中词的分布式表示，也经常叫作词嵌入 </font></p><p><font color="red"><strong>例如：3维one-hot向量空间和一个2维嵌入空间的对比</strong></font><br><img src="https://img-blog.csdnimg.cn/007e99163fd044348e7061ebebc1af33.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_14,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="one-hot向量空间与嵌入空间"><br>在低维的嵌入空间中，每个样本都不在坐标轴上，样本之间可以计算相似度．</p><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="什么是深度"><a href="#什么是深度" class="headerlink" title="什么是深度"></a>什么是深度</h2><p>“深度”是指原始数据进行非线性特征转换的次数</p><h2 id="深度学习的优点"><a href="#深度学习的优点" class="headerlink" title="深度学习的优点"></a>深度学习的优点</h2><p>深度学习，其主要目的是从数据中<font color="red">自动学习到有效的特征表示 </font><br>其抽象在 数据通过多层的特征转换，学习到的表示可以代替人工设计的特征，从而避免“特征工程”<br><img src="https://img-blog.csdnimg.cn/f678fb27e01c4d989d099cfa06afe038.png#pic_center" alt="深度学习的数据处理流程"><br>其是 一种 <font color="red"><strong>端到端的学习方式</strong></font>   在学习过程中<strong>不进行分模块或分阶段训练</strong>，直接优化任务的总体目标．在端到端学习中，一般不需要明确地给出不同模块或阶段的功能，中间过程不需要人为干预</p><h2 id="深度学习的关键问题"><a href="#深度学习的关键问题" class="headerlink" title="深度学习的关键问题"></a>深度学习的关键问题</h2><p>深度学习需要解决的关键问题是 <strong>贡献度分配问题</strong>，即一个系统中不同的 <strong>组件</strong> 或其 <strong>参数</strong> 对最终系统输出结果的贡献或影响 </p><p>目前，深度学习采用的模型主要是神经网络模型，其主要原因是神经网络模型可以使用 <strong>误差反向传播算法</strong> ，从而可以比较好地解决贡献度分配问题</p><h1 id="深度学习相关的学术会议"><a href="#深度学习相关的学术会议" class="headerlink" title="深度学习相关的学术会议"></a>深度学习相关的学术会议</h1><ul><li>国际表示学习会议  ICLR ：主要聚焦于深度学习</li><li>神经信息处理系统年会 NeurIPS ：交叉学科会议，但偏重于机器学习</li><li>国际机器学习会议 ICML：机器学习顶级会议</li><li>国际人工智能联合会议 IJCAI ：人工智能领域最顶尖的综合性会议</li><li>美国人工智能协会年会  AAAI ：人工智能领域的顶级会议</li></ul><h2 id="计算机视觉领域"><a href="#计算机视觉领域" class="headerlink" title="计算机视觉领域"></a>计算机视觉领域</h2><ul><li>计算机视觉与模式识别大会  CVPR</li><li>国际计算机视觉会议  ICCV</li></ul><h2 id="自然语言处理领域"><a href="#自然语言处理领域" class="headerlink" title="自然语言处理领域"></a>自然语言处理领域</h2><ul><li>计算语言学年会 ACL</li><li>自然语言处理实证方法大会  EMNLP</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;神经网络与深度学习 邱锡鹏 学习笔记（第一章）&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;机器学习&quot;&gt;&lt;a href=&quot;#机器学习&quot; class=&quot;headerlink&quot; title=&quot;机器学习&quot;&gt;&lt;/a&gt;机器学习&lt;/h1&gt;&lt;h2 id=&quot;浅层学习&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="深度学习基础-邱锡鹏" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%82%B1%E9%94%A1%E9%B9%8F/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="邱锡鹏" scheme="http://example.com/tags/%E9%82%B1%E9%94%A1%E9%B9%8F/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 实现 线性回归模型 —— 刘二</title>
    <link href="http://example.com/2021/09/08/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/"/>
    <id>http://example.com/2021/09/08/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/</id>
    <published>2021-09-08T02:40:01.000Z</published>
    <updated>2021-09-13T13:14:01.735Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 实现 线性回归模型 —— 刘二">TOC</a></p><p>用Pytorch 实现 线性模型。整个流程图可以如下图所示：<br><img src="https://img-blog.csdnimg.cn/0728aba4a28542838fdb11f7bf4a668c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>大致的设计步骤 分为5步 如下所示：<br><img src="https://img-blog.csdnimg.cn/e1aa1c61b89b4e46adebe9d44c61ff2b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>第五步：是进行 评估模型并预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><h1 id="Step1：准备数据"><a href="#Step1：准备数据" class="headerlink" title="Step1：准备数据"></a>Step1：准备数据</h1><p><img src="https://img-blog.csdnimg.cn/91dc702ff784417993b5d0e5f63f75ed.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br></pre></td></tr></table></figure><h1 id="Step2：设计模型"><a href="#Step2：设计模型" class="headerlink" title="Step2：设计模型"></a>Step2：设计模型</h1><p>这边的线性模型比较简单 为 $y=Wx+b$ 只有两个超参数 W 和 b<br><img src="https://img-blog.csdnimg.cn/51e148a09e1d452e8a4e9794e3c4035a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>要通过输入的维度和 输出的维度，才能明确 $W$ 和 $b$ 的维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个线性模型类 所有的模型类都必须继承torch.nn.Module</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"><span class="comment"># 构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 调用父类的构造 这步必须得有</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel,self).__init__()</span><br><span class="line">        <span class="comment"># Linear 是一个模型类 这边实例化他给 对象linear</span></span><br><span class="line">        <span class="comment"># w 权重 = 1  b 偏执 = 1</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 方法重写</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">    <span class="comment"># 预测值 </span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure><h2 id="torch-nn-Linear-in-features-out-features-bias-True-方法"><a href="#torch-nn-Linear-in-features-out-features-bias-True-方法" class="headerlink" title="torch.nn.Linear(in_features,out_features,bias=True) 方法"></a>torch.nn.Linear(in_features,out_features,bias=True) 方法</h2><p><img src="https://img-blog.csdnimg.cn/0e75527101244e56a7687acb4eaa9315.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><p>实例化这个模型为 model</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化这个模型</span></span><br><span class="line">model = LinearModel()</span><br></pre></td></tr></table></figure><h1 id="Step3：构建损失函数和优化器"><a href="#Step3：构建损失函数和优化器" class="headerlink" title="Step3：构建损失函数和优化器"></a>Step3：构建损失函数和优化器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数：mean squared error</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>) <span class="comment"># size_average = True 的话 就 乘以 1/N  默认为true</span></span><br><span class="line"><span class="comment"># model.parameters() 可以找到模型所有需要训练的参数   优化器：SGD</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>) </span><br></pre></td></tr></table></figure><h2 id="torch-nn-MSELoss-size-average-False-方法"><a href="#torch-nn-MSELoss-size-average-False-方法" class="headerlink" title="torch.nn.MSELoss(size_average=False) 方法"></a>torch.nn.MSELoss(size_average=False) 方法</h2><p>用于创建一个MSE损失函数<br><img src="https://img-blog.csdnimg.cn/4921387ab8ea4db6a6aaca5ed892fef5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h2 id="torch-optim-SGD-…-方法"><a href="#torch-optim-SGD-…-方法" class="headerlink" title="torch.optim.SGD(…) 方法"></a>torch.optim.SGD(…) 方法</h2><p>优化器选择 SGD   可调整学习率<br>$w^{*} = w - α\frac{\partial L}{\partial W}$<br><img src="https://img-blog.csdnimg.cn/aab4fe33c3844ea386a7c14cc9350865.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="Step4-训练模型"><a href="#Step4-训练模型" class="headerlink" title="Step4: 训练模型"></a>Step4: 训练模型</h1><p> 前两步 就是正向传播 forward</p><ol><li>预测 标签</li><li>预测 与 实际 算出损失值</li><li>反向传播 backward 优化参数</li><li>更新参数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"> <span class="comment">#数据跑 99次  range是不到那个数字的</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br></pre></td></tr></table></figure><h1 id="Step5-评估模型并预测"><a href="#Step5-评估模型并预测" class="headerlink" title="Step5: 评估模型并预测"></a>Step5: 评估模型并预测</h1><p>这边没有准备 测试集及其标签</p><p>输出 超参数  权重w 和 偏置b<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出 超参数  权重w 和 偏置b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>,model.linear.weight.item() )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>,model.linear.bias.item())</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测 </span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x为4.0 预测的 y值为：&#x27;</span>,y_test.data)</span><br></pre></td></tr></table></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><font color="red"> 这边少了 怎么输出 准确率？ </font><br><img src="https://img-blog.csdnimg.cn/c0013d6d060148b8ac02605d0fa0d163.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_15,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="实现源码"><a href="#实现源码" class="headerlink" title="实现源码"></a>实现源码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步 设计模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 调用父类的构造 这步必须得有</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel,self).__init__()</span><br><span class="line">        <span class="comment"># Linear 是一个模型类 这边实例化他给 linear</span></span><br><span class="line">        <span class="comment"># w 权重 = 1  b 偏执 = 1</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个模型</span></span><br><span class="line">model = LinearModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 构建损失函数和优化器</span></span><br><span class="line"><span class="comment"># mean squared error</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>) <span class="comment"># size_average = True 的话 就 乘以 1/N  默认为true</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>) <span class="comment">#model.parameters() 可以找到</span></span><br><span class="line"><span class="comment"># 模型所有需要训练的参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五步 评估模型</span></span><br><span class="line"><span class="comment"># 输出 超参数  权重w 和 偏置b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>,model.linear.weight.item() )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>,model.linear.bias.item())</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x为4.0 预测的 y值为：&#x27;</span>,y_test.data)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 实现 线性回归模型 —— 刘二&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;用Pytorch 实现 线性模型。整个流程图可以如下图所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/0728aba4a285428</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
  </entry>
  
  <entry>
    <title>Keras  RNN 实现 MNIST 手写数字识别</title>
    <link href="http://example.com/2021/09/04/Keras%20%20RNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    <id>http://example.com/2021/09/04/Keras%20%20RNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/</id>
    <published>2021-09-04T02:40:01.000Z</published>
    <updated>2021-09-03T16:51:56.305Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras  RNN 实现 MNIST 手写数字识别">TOC</a></p><p>我们就以 MNIST数据集的手写识别 为例子</p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers.recurrent <span class="keyword">import</span> SimpleRNN</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure><h1 id="载入MNIST-数据"><a href="#载入MNIST-数据" class="headerlink" title="载入MNIST 数据"></a>载入MNIST 数据</h1><p>该数据集一共有训练集 6w 张，测试集 1w 张<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_images,train_labels),(test_images,test_labels)= mnist.load_data()</span><br></pre></td></tr></table></figure></p><p>可以查看一下图像和标签  是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数字5</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签5</span></span><br><span class="line"><span class="built_in">print</span>(train_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 数字7</span></span><br><span class="line"><span class="built_in">print</span>(test_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签7</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以 打印 一下图片 看看是什么样子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(test_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/a745fc12bef94f93b93f80b2e89bb0d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="图片数据处理"><a href="#图片数据处理" class="headerlink" title="图片数据处理"></a>图片数据处理</h1><h2 id="查看图片原有-shape"><a href="#查看图片原有-shape" class="headerlink" title="查看图片原有 shape"></a>查看图片原有 shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_images (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images&#x27;</span>, train_images.shape)</span><br><span class="line"><span class="comment"># train_labels (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><h2 id="图片数据处理：归一化"><a href="#图片数据处理：归一化" class="headerlink" title="图片数据处理：归一化"></a>图片数据处理：归一化</h2><p>除以255.0 是为了归一化，使得元素点 全部变为在 0-1 之间<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment">#  (60000, 28, 28) </span></span><br><span class="line">train_images_scale = train_images/<span class="number">255.0</span></span><br><span class="line">test_images_scale = test_images/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_images变换后 (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images变换后&#x27;</span>, train_images_scale.shape)</span><br><span class="line"><span class="comment"># test_images变换后 (10000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_images变换后&#x27;</span>, test_images_scale.shape)</span><br></pre></td></tr></table></figure></p><h2 id="标签数据处理：转换成-one-hot-格式"><a href="#标签数据处理：转换成-one-hot-格式" class="headerlink" title="标签数据处理：转换成 one hot 格式"></a>标签数据处理：转换成 one hot 格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换 one hot 格式  共十个分类</span></span><br><span class="line"><span class="comment"># np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</span></span><br><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br><span class="line">test_labels_hot = np_utils.to_categorical(test_labels,num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_labels (60000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>,train_labels_hot.shape)</span><br><span class="line"><span class="comment"># test_labels (10000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_labels&#x27;</span>,test_labels_hot.shape)</span><br></pre></td></tr></table></figure><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><p>先定义 RNN 所需的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义rnn 的参数</span></span><br><span class="line"><span class="comment"># 数据长度 一行一共有28个元素</span></span><br><span class="line">input_size = <span class="number">28</span></span><br><span class="line"><span class="comment"># 序列长度 一共有28个序列 也就是28行</span></span><br><span class="line">time_steps = <span class="number">28</span></span><br><span class="line"><span class="comment"># 隐藏层cell个数</span></span><br><span class="line">cell_size = <span class="number">50</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>再定义 RNN 模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment">#循环神经网络</span></span><br><span class="line">model.add(SimpleRNN(</span><br><span class="line">    units=cell_size, <span class="comment">#输出</span></span><br><span class="line">    input_shape=(time_steps,input_size) <span class="comment">#输入</span></span><br><span class="line">))</span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line">model.add(Dense(input_dim=cell_size,units=<span class="number">10</span>,activation=<span class="string">&quot;softmax&quot;</span>))</span><br></pre></td></tr></table></figure><br>优化器使用 Adam， 损失函数 选择 交叉熵   并编译</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化器  10的 -4次方</span></span><br><span class="line">adam = Adam(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= adam,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p> 一共 60000张训练图片 按批次训练 一批次64张   一共6w/64 个批次<br>训练完一轮6w张，表示一个epoch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一共 60000张训练图片 按批次训练 一批次64张   </span></span><br><span class="line"><span class="comment"># 训练完一轮6w张，表示一个epoch</span></span><br><span class="line">model.fit(train_images_scale,train_labels_hot,batch_size=<span class="number">64</span>,epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/35cafae3dc0e431086f33cdf4a23dc52.png#pic_center" alt="在这里插入图片描述"></p><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br>也可以看一下 在训练集上的表现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(train_images_scale,train_labels_hot)</span><br></pre></td></tr></table></figure><p>下面是 结果（上训练集 下测试集）<br><img src="https://img-blog.csdnimg.cn/d93ee2873e72460e96c0c4a9a663985b.png#pic_center" alt="在这里插入图片描述"></p><h1 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h1><font color="red">**有问题 输入的 数据维度不对 不知道错哪了？？？**</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看下输入的形状</span></span><br><span class="line"><span class="comment"># # 预测数据</span></span><br><span class="line"><span class="built_in">print</span>(test_images_scale[<span class="number">0</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images_scale[<span class="number">0</span>]/<span class="number">255.0</span>)))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images_scale[<span class="number">0</span>]/<span class="number">255.0</span>))))</span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输入的图像 变换后形状<br><img src="https://img-blog.csdnimg.cn/c6e4d5008f9c416ba48bea577a86c238.png#pic_center" alt="在这里插入图片描述"><br>模型预测 数据 输出的概率分类结果<br><img src="https://img-blog.csdnimg.cn/ba872c35347041b592db07d82b429d35.png#pic_center" alt="在这里插入图片描述"><br>选出其中最大的 以及 实际图片标签 均为 数字7<br><img src="https://img-blog.csdnimg.cn/5a585701d439431aa1853283c8592b46.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras  RNN 实现 MNIST 手写数字识别&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们就以 MNIST数据集的手写识别 为例子&lt;/p&gt;
&lt;h1 id=&quot;导入需要的包&quot;&gt;&lt;a href=&quot;#导入需要的包&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
    <category term="RNN" scheme="http://example.com/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>Keras  CNN 实现 MNIST 手写数字识别</title>
    <link href="http://example.com/2021/09/03/Keras%20%20CNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    <id>http://example.com/2021/09/03/Keras%20%20CNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/</id>
    <published>2021-09-03T02:40:01.000Z</published>
    <updated>2021-09-03T16:49:33.510Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras  CNN 实现 MNIST 手写数字识别">TOC</a></p><p>我们就以 MNIST数据集的手写识别 为例子</p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Dropout,Convolution2D,MaxPooling2D,Flatten</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure><h1 id="载入MNIST-数据"><a href="#载入MNIST-数据" class="headerlink" title="载入MNIST 数据"></a>载入MNIST 数据</h1><p>该数据集一共有训练集 6w 张，测试集 1w 张<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_images,train_labels),(test_images,test_labels)= mnist.load_data()</span><br></pre></td></tr></table></figure></p><p>可以查看一下图像和标签  是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数字5</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签5</span></span><br><span class="line"><span class="built_in">print</span>(train_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 数字7</span></span><br><span class="line"><span class="built_in">print</span>(test_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签7</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以 打印 一下图片 看看是什么样子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(test_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/a745fc12bef94f93b93f80b2e89bb0d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="图片数据处理"><a href="#图片数据处理" class="headerlink" title="图片数据处理"></a>图片数据处理</h1><h2 id="查看图片原有-shape"><a href="#查看图片原有-shape" class="headerlink" title="查看图片原有 shape"></a>查看图片原有 shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_images (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images&#x27;</span>, train_images.shape)</span><br><span class="line"><span class="comment"># train_labels (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><h2 id="图片数据处理：变成四维-，并归一化"><a href="#图片数据处理：变成四维-，并归一化" class="headerlink" title="图片数据处理：变成四维 ，并归一化"></a>图片数据处理：变成四维 ，并归一化</h2><p>变维度的 -1 是个通配符，系统会自动完成应该变成多少<br>除以255.0 是为了归一化，使得元素点 全部变为在 0-1 之间<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment"># CNN 输入的是一张图片</span></span><br><span class="line"><span class="comment"># 将 (60000, 28, 28) -&gt; (60000, 28, 28, 1)  变成四维</span></span><br><span class="line"><span class="comment"># 第四个 1  表示的为深度   黑白图像为 1   彩色图像为 3</span></span><br><span class="line">train_images_scale = train_images.reshape(-<span class="number">1</span>, train_images.shape[<span class="number">1</span>] ,train_images.shape[<span class="number">2</span>],<span class="number">1</span>)/<span class="number">255.0</span></span><br><span class="line">test_images_scale = test_images.reshape(-<span class="number">1</span>,test_images.shape[<span class="number">1</span>], test_images.shape[<span class="number">2</span>],<span class="number">1</span>)/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_images变换后 (60000, 28, 28, 1)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images变换后&#x27;</span>, train_images_scale.shape)</span><br><span class="line"><span class="comment"># test_images变换后 (10000, 28, 28, 1)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_images变换后&#x27;</span>, test_images_scale.shape)</span><br></pre></td></tr></table></figure></p><h2 id="标签数据处理：转换成-one-hot-格式"><a href="#标签数据处理：转换成-one-hot-格式" class="headerlink" title="标签数据处理：转换成 one hot 格式"></a>标签数据处理：转换成 one hot 格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换 one hot 格式  共十个分类</span></span><br><span class="line"><span class="comment"># np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</span></span><br><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br><span class="line">test_labels_hot = np_utils.to_categorical(test_labels,num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_labels (60000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>,train_labels_hot.shape)</span><br><span class="line"><span class="comment"># test_labels (10000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_labels&#x27;</span>,test_labels_hot.shape)</span><br></pre></td></tr></table></figure><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><p>思路是 卷积 再池化 重复步骤  再压平给 全身神经网络<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment">#第一个卷积层</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    input shape 激入平面</span></span><br><span class="line"><span class="string">    filters 卷积核/过滤器 个数</span></span><br><span class="line"><span class="string">    kernel_size 卷积窗口大小</span></span><br><span class="line"><span class="string">    strides 步长</span></span><br><span class="line"><span class="string">    padding（边界填充）padding方式 same/valid</span></span><br><span class="line"><span class="string">    activation 激活函数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 用same 保持整个还是 28 × 28</span></span><br><span class="line">model.add(Convolution2D(</span><br><span class="line">    input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>),filters=<span class="number">32</span>,kernel_size=<span class="number">5</span>,strides=<span class="number">1</span>,padding=<span class="string">&#x27;same&#x27;</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment">#第一个池化层</span></span><br><span class="line">model.add(MaxPooling2D(</span><br><span class="line">    pool_size=<span class="number">2</span>,strides=<span class="number">2</span>,padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line"><span class="comment">#第二卷积层</span></span><br><span class="line">model.add(Convolution2D(<span class="number">64</span>,<span class="number">5</span>,strides=<span class="number">1</span>,padding=<span class="string">&#x27;same&#x27;</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment">#第二个池化层</span></span><br><span class="line">model.add(MaxPooling2D(</span><br><span class="line">    pool_size=<span class="number">2</span>,strides=<span class="number">2</span>,padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#把第二个池化层的前出扁平化为1维</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line"><span class="comment">#第一个全连接层</span></span><br><span class="line">model.add(Dense(units=<span class="number">1024</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment">#Dropout</span></span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line"><span class="comment">#第二个全连接层</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">1024</span>,activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure></p><p>优化器使用 Adam， 损失函数 选择 交叉熵   并编译</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化器  10的 -4次方</span></span><br><span class="line">adam = Adam(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= adam,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p> 一共 60000张训练图片 按批次训练 一批次64张   一共6w/64 个批次<br>训练完一轮6w张，表示一个epoch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一共 60000张训练图片 按批次训练 一批次64张   </span></span><br><span class="line"><span class="comment"># 训练完一轮6w张，表示一个epoch</span></span><br><span class="line">model.fit(train_images_scale,train_labels_hot,batch_size=<span class="number">64</span>,epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/35cafae3dc0e431086f33cdf4a23dc52.png#pic_center" alt="在这里插入图片描述"><br>上图只跑了一次 epoch，因为笔记本太慢了<br><strong>注意： 最好用 GPU 来跑 ，不然笔记本非常的慢</strong></p><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br>也可以看一下 在训练集上的表现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(train_images_scale,train_labels_hot)</span><br></pre></td></tr></table></figure><p>下面是 结果（上训练集 下测试集）<br><img src="https://img-blog.csdnimg.cn/d93ee2873e72460e96c0c4a9a663985b.png#pic_center" alt="在这里插入图片描述"></p><h1 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看下输入的形状</span></span><br><span class="line"><span class="built_in">print</span>(test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>))))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))))</span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输入的图像 变换后形状<br><img src="https://img-blog.csdnimg.cn/c6e4d5008f9c416ba48bea577a86c238.png#pic_center" alt="在这里插入图片描述"><br>模型预测 数据 输出的概率分类结果<br><img src="https://img-blog.csdnimg.cn/ba872c35347041b592db07d82b429d35.png#pic_center" alt="在这里插入图片描述"><br>选出其中最大的 以及 实际图片标签 均为 数字7<br><img src="https://img-blog.csdnimg.cn/5a585701d439431aa1853283c8592b46.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras  CNN 实现 MNIST 手写数字识别&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们就以 MNIST数据集的手写识别 为例子&lt;/p&gt;
&lt;h1 id=&quot;导入需要的包&quot;&gt;&lt;a href=&quot;#导入需要的包&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
    <category term="CNN" scheme="http://example.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>Keras MNIST 过拟合问题解决：Dropout 与 正则化</title>
    <link href="http://example.com/2021/09/02/Keras%20MNIST%20%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%EF%BC%9ADropout%20%E4%B8%8E%20%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <id>http://example.com/2021/09/02/Keras%20MNIST%20%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%EF%BC%9ADropout%20%E4%B8%8E%20%E6%AD%A3%E5%88%99%E5%8C%96/</id>
    <published>2021-09-02T02:40:01.000Z</published>
    <updated>2021-09-02T16:09:01.678Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras MNIST 过拟合问题解决：Dropout 与 正则化">TOC</a></p><p>我们就以 MNIST数据集的手写识别 为例子<br>做 过拟合问题的应用  包括 Dropout 和 正则化</p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><h2 id="使用Dropout-需要导入的包"><a href="#使用Dropout-需要导入的包" class="headerlink" title="使用Dropout 需要导入的包"></a>使用Dropout 需要导入的包</h2><p>需要 导入 另一个包 keras.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure></p><h2 id="使用-正则化-需要导入的包"><a href="#使用-正则化-需要导入的包" class="headerlink" title="使用 正则化  需要导入的包"></a>使用 正则化  需要导入的包</h2><p>layers 层中 引入 keras.regularizers   中  l2 范式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.regularizers <span class="keyword">import</span> l2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure></p><h1 id="载入MNIST-数据"><a href="#载入MNIST-数据" class="headerlink" title="载入MNIST 数据"></a>载入MNIST 数据</h1><p>该数据集一共有训练集 6w 张，测试集 1w 张<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_images,train_labels),(test_images,test_labels)= mnist.load_data()</span><br></pre></td></tr></table></figure></p><p>可以查看一下图像和标签  是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数字5</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签5</span></span><br><span class="line"><span class="built_in">print</span>(train_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 数字7</span></span><br><span class="line"><span class="built_in">print</span>(test_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签7</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以 打印 一下图片 看看是什么样子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(test_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/a745fc12bef94f93b93f80b2e89bb0d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="图片数据处理"><a href="#图片数据处理" class="headerlink" title="图片数据处理"></a>图片数据处理</h1><h2 id="查看图片原有-shape"><a href="#查看图片原有-shape" class="headerlink" title="查看图片原有 shape"></a>查看图片原有 shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_images (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images&#x27;</span>, train_images.shape)</span><br><span class="line"><span class="comment"># train_labels (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><h2 id="图片数据处理：将图片压平"><a href="#图片数据处理：将图片压平" class="headerlink" title="图片数据处理：将图片压平"></a>图片数据处理：将图片压平</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment"># 将 (60000, 28, 28) -&gt; (60000, 784)  压平图片</span></span><br><span class="line">train_images_scale = train_images.reshape(train_images.shape[<span class="number">0</span>], train_images.shape[<span class="number">1</span>] * train_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line">test_images_scale = test_images.reshape(test_images.shape[<span class="number">0</span>], test_images.shape[<span class="number">1</span>] * test_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_images变换后 (60000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images变换后&#x27;</span>, train_images_scale.shape)</span><br><span class="line"><span class="comment"># test_images变换后 (10000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_images变换后&#x27;</span>, test_images_scale.shape)</span><br></pre></td></tr></table></figure><h2 id="标签数据处理：转换成-one-hot-格式"><a href="#标签数据处理：转换成-one-hot-格式" class="headerlink" title="标签数据处理：转换成 one hot 格式"></a>标签数据处理：转换成 one hot 格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换 one hot 格式  共十个分类</span></span><br><span class="line"><span class="comment"># np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</span></span><br><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br><span class="line">test_labels_hot = np_utils.to_categorical(test_labels,num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_labels (60000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>,train_labels_hot.shape)</span><br><span class="line"><span class="comment"># test_labels (10000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_labels&#x27;</span>,test_labels_hot.shape)</span><br></pre></td></tr></table></figure><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><h2 id="添加-Dropout"><a href="#添加-Dropout" class="headerlink" title="添加 Dropout"></a>添加 Dropout</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 输入压平图像 维度为784   输出为 10分类</span></span><br><span class="line"><span class="comment"># 加个隐层</span></span><br><span class="line">model.add(Dense(units=<span class="number">200</span>,input_dim=<span class="number">784</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;tanh&quot;</span>))</span><br><span class="line"><span class="comment"># 上层40%的神经元不工作</span></span><br><span class="line">model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">100</span>,input_dim=<span class="number">200</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;tanh&quot;</span>))</span><br><span class="line"><span class="comment"># 上层40%的神经元不工作</span></span><br><span class="line">model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">100</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;softmax&quot;</span>))</span><br></pre></td></tr></table></figure><p>优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新定义 sgd 优化器  加速一下学习率</span></span><br><span class="line">sgd = SGD(learning_rate=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="添加-正则化项"><a href="#添加-正则化项" class="headerlink" title="添加 正则化项"></a>添加 正则化项</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 输入压平图像 维度为784   输出为 10分类</span></span><br><span class="line"><span class="comment"># 加个隐层</span></span><br><span class="line">model.add(Dense(units=<span class="number">200</span>,input_dim=<span class="number">784</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;tanh&quot;</span>,kernel_initializer=l2(<span class="number">0.003</span>)))</span><br><span class="line">model.add(Dense(units=<span class="number">100</span>,input_dim=<span class="number">200</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;tanh&quot;</span>,kernel_initializer=l2(<span class="number">0.003</span>)))</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">100</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;softmax&quot;</span>,kernel_initializer=l2(<span class="number">0.003</span>)))</span><br></pre></td></tr></table></figure><p>优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新定义 sgd 优化器  加速一下学习率</span></span><br><span class="line">sgd = SGD(learning_rate=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p> 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次<br>训练完一轮6w张，表示一个epoch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次</span></span><br><span class="line"><span class="comment"># 训练完一轮6w张，表示一个epoch</span></span><br><span class="line">model.fit(train_images_scale,train_labels_hot,batch_size=<span class="number">32</span>,epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/576f033299d04330a371c33eb6a2abac.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="训练过程"></p><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><h2 id="Dropout的结果"><a href="#Dropout的结果" class="headerlink" title="Dropout的结果"></a>Dropout的结果</h2><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br>也可以看一下 在训练集上的表现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(train_images_scale,train_labels_hot)</span><br></pre></td></tr></table></figure><p>下面是 结果（上训练集 下测试集）<br><img src="https://img-blog.csdnimg.cn/7c46786df7d74ef383b14a1627e0cf9d.png#pic_center" alt="在这里插入图片描述"></p><h2 id="正则化的结果"><a href="#正则化的结果" class="headerlink" title="正则化的结果"></a>正则化的结果</h2><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br>也可以看一下 在训练集上的表现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(train_images_scale,train_labels_hot)</span><br></pre></td></tr></table></figure><p>下面是 结果（上训练集 下测试集）</p><p><font color="red"><strong>出现了错误 不知道是哪里的问题哎？</strong></font><br><img src="https://img-blog.csdnimg.cn/44c6138733e4436ba6d847fec0f88cfd.png#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: __call__() got an unexpected keyword argument <span class="string">&#x27;dtype&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/e71199da4cfb4c2aaf69d39fc4dacfd3.png#pic_center" alt="在这里插入图片描述"></p><h1 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看下输入的形状</span></span><br><span class="line"><span class="built_in">print</span>(test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>).shape)</span><br><span class="line"><span class="comment"># 模型预测  输出每个分类的 概率</span></span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>))))</span><br><span class="line"><span class="comment"># 选取最大的那个 就是预测的标签</span></span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>)))))</span><br><span class="line"><span class="comment"># 实际该图片的 标签</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输入的图像 变换后形状<br><img src="https://img-blog.csdnimg.cn/4302df2ba0e44c04897aaf00f6955dad.png#pic_center" alt="在这里插入图片描述"><br>模型预测 数据 输出的概率分类结果<br><img src="https://img-blog.csdnimg.cn/3ffa135e578d486c8404588781678e0c.png#pic_center" alt="在这里插入图片描述"><br>选出其中最大的 以及 实际图片标签 均为 数字7<br><img src="https://img-blog.csdnimg.cn/5a585701d439431aa1853283c8592b46.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras MNIST 过拟合问题解决：Dropout 与 正则化&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们就以 MNIST数据集的手写识别 为例子&lt;br&gt;做 过拟合问题的应用  包括 Dropout 和 正则化&lt;/p&gt;
&lt;h1 id=&quot;导入需要的包&quot;&gt;&lt;a</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>Keras  MNIST（数字识别）数据集分类（普通全神经网络）</title>
    <link href="http://example.com/2021/08/30/Keras%20%20MNIST%EF%BC%88%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB%EF%BC%88%E6%99%AE%E9%80%9A%E5%85%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89/"/>
    <id>http://example.com/2021/08/30/Keras%20%20MNIST%EF%BC%88%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB%EF%BC%88%E6%99%AE%E9%80%9A%E5%85%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89/</id>
    <published>2021-08-30T02:40:01.000Z</published>
    <updated>2021-08-30T02:42:06.929Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras  MNIST（数字识别）数据集分类">TOC</a></p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure><h1 id="载入MNIST-数据"><a href="#载入MNIST-数据" class="headerlink" title="载入MNIST 数据"></a>载入MNIST 数据</h1><p>该数据集一共有训练集 6w 张，测试集 1w 张<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_images,train_labels),(test_images,test_labels)= mnist.load_data()</span><br></pre></td></tr></table></figure></p><p>可以查看一下图像和标签  是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数字5</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签5</span></span><br><span class="line"><span class="built_in">print</span>(train_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 数字7</span></span><br><span class="line"><span class="built_in">print</span>(test_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签7</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以 打印 一下图片 看看是什么样子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(test_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/a745fc12bef94f93b93f80b2e89bb0d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="图片数据处理"><a href="#图片数据处理" class="headerlink" title="图片数据处理"></a>图片数据处理</h1><h2 id="查看图片原有-shape"><a href="#查看图片原有-shape" class="headerlink" title="查看图片原有 shape"></a>查看图片原有 shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_images (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images&#x27;</span>, train_images.shape)</span><br><span class="line"><span class="comment"># train_labels (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><h2 id="图片数据处理：将图片压平"><a href="#图片数据处理：将图片压平" class="headerlink" title="图片数据处理：将图片压平"></a>图片数据处理：将图片压平</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment"># 将 (60000, 28, 28) -&gt; (60000, 784)  压平图片</span></span><br><span class="line">train_images_scale = train_images.reshape(train_images.shape[<span class="number">0</span>], train_images.shape[<span class="number">1</span>] * train_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line">test_images_scale = test_images.reshape(test_images.shape[<span class="number">0</span>], test_images.shape[<span class="number">1</span>] * test_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_images变换后 (60000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images变换后&#x27;</span>, train_images_scale.shape)</span><br><span class="line"><span class="comment"># test_images变换后 (10000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_images变换后&#x27;</span>, test_images_scale.shape)</span><br></pre></td></tr></table></figure><h2 id="标签数据处理：转换成-one-hot-格式"><a href="#标签数据处理：转换成-one-hot-格式" class="headerlink" title="标签数据处理：转换成 one hot 格式"></a>标签数据处理：转换成 one hot 格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换 one hot 格式  共十个分类</span></span><br><span class="line"><span class="comment"># np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</span></span><br><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br><span class="line">test_labels_hot = np_utils.to_categorical(test_labels,num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_labels (60000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>,train_labels_hot.shape)</span><br><span class="line"><span class="comment"># test_labels (10000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_labels&#x27;</span>,test_labels_hot.shape)</span><br></pre></td></tr></table></figure><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><p>只有输入和输出层， 输入压平图像 维度为784   输出为 10分类<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 输入压平图像 维度为784   输出为 10分类</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">784</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;softmax&quot;</span>))</span><br></pre></td></tr></table></figure><br>优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新定义 sgd 优化器  加速一下学习率</span></span><br><span class="line">sgd = SGD(learning_rate=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p> 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次<br>训练完一轮6w张，表示一个epoch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次</span></span><br><span class="line"><span class="comment"># 训练完一轮6w张，表示一个epoch</span></span><br><span class="line">model.fit(train_images_scale,train_labels_hot,batch_size=<span class="number">32</span>,epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/576f033299d04330a371c33eb6a2abac.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="训练过程"></p><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/eb52f38d3e944137a3ade6e5b9d1e1a9.png#pic_center" alt="在这里插入图片描述"></p><h1 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看下输入的形状</span></span><br><span class="line"><span class="built_in">print</span>(test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>).shape)</span><br><span class="line"><span class="comment"># 模型预测  输出每个分类的 概率</span></span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>))))</span><br><span class="line"><span class="comment"># 选取最大的那个 就是预测的标签</span></span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>)))))</span><br><span class="line"><span class="comment"># 实际该图片的 标签</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输入的图像 变换后形状<br><img src="https://img-blog.csdnimg.cn/4302df2ba0e44c04897aaf00f6955dad.png#pic_center" alt="在这里插入图片描述"><br>模型预测 数据 输出的概率分类结果<br><img src="https://img-blog.csdnimg.cn/3ffa135e578d486c8404588781678e0c.png#pic_center" alt="在这里插入图片描述"><br>选出其中最大的 以及 实际图片标签 均为 数字7<br><img src="https://img-blog.csdnimg.cn/5a585701d439431aa1853283c8592b46.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras  MNIST（数字识别）数据集分类&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;导入需要的包&quot;&gt;&lt;a href=&quot;#导入需要的包&quot; class=&quot;headerlink&quot; title=&quot;导入需要的包&quot;&gt;&lt;/a&gt;导入需要的包&lt;/h1&gt;&lt;p&gt;首先导入</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>keras 包方法集合</title>
    <link href="http://example.com/2021/08/29/keras%20%E5%8C%85%E6%96%B9%E6%B3%95%E9%9B%86%E5%90%88/"/>
    <id>http://example.com/2021/08/29/keras%20%E5%8C%85%E6%96%B9%E6%B3%95%E9%9B%86%E5%90%88/</id>
    <published>2021-08-29T02:18:01.000Z</published>
    <updated>2021-08-30T02:55:17.890Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="keras 包方法集合">TOC</a></p><h1 id="keras-util-库-np-utils"><a href="#keras-util-库-np-utils" class="headerlink" title="keras.util 库  np_utils"></a>keras.util 库  np_utils</h1><h2 id="np-utils-to-categorical"><a href="#np-utils-to-categorical" class="headerlink" title="np_utils.to_categorical"></a>np_utils.to_categorical</h2><p>np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>如将 $[1,2,3,……4]$ 转化成：<br><img src="https://img-blog.csdnimg.cn/379535ca64bc4e5987673ecd12af0c75.png#pic_center" alt="在这里插入图片描述"><br>这样的形态。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;keras 包方法集合&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;keras-util-库-np-utils&quot;&gt;&lt;a href=&quot;#keras-util-库-np-utils&quot; class=&quot;headerlink&quot; title=&quot;keras.util 库</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>各类 优化器(调参工具) 详解与选择</title>
    <link href="http://example.com/2021/08/28/%E5%90%84%E7%B1%BB%20%E4%BC%98%E5%8C%96%E5%99%A8(%E8%B0%83%E5%8F%82%E5%B7%A5%E5%85%B7)%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://example.com/2021/08/28/%E5%90%84%E7%B1%BB%20%E4%BC%98%E5%8C%96%E5%99%A8(%E8%B0%83%E5%8F%82%E5%B7%A5%E5%85%B7)%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2021-08-28T02:40:01.000Z</published>
    <updated>2021-08-30T02:52:24.871Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="各类 优化器 详解与选择">TOC</a></p><h1 id="sgd"><a href="#sgd" class="headerlink" title="sgd"></a>sgd</h1><h1 id="adam"><a href="#adam" class="headerlink" title="adam"></a>adam</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;各类 优化器 详解与选择&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;sgd&quot;&gt;&lt;a href=&quot;#sgd&quot; class=&quot;headerlink&quot; title=&quot;sgd&quot;&gt;&lt;/a&gt;sgd&lt;/h1&gt;&lt;h1 id=&quot;adam&quot;&gt;&lt;a href=&quot;#adam&quot; </summary>
      
    
    
    
    <category term="优化器" scheme="http://example.com/categories/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    
    
    <category term="优化器" scheme="http://example.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Keras 构建 线性模型和非线性模型</title>
    <link href="http://example.com/2021/08/24/Keras%20%E6%9E%84%E5%BB%BA%20%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2021/08/24/Keras%20%E6%9E%84%E5%BB%BA%20%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-08-24T09:22:01.000Z</published>
    <updated>2021-08-24T09:28:44.809Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras 构建 线性模型和非线性模型">TOC</a></p><h1 id="预测线性模型"><a href="#预测线性模型" class="headerlink" title="预测线性模型"></a>预测线性模型</h1><p>使用的数据 是我们随机生成的<br>、<br>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># matplotlib.use(&#x27;TkAgg&#x27;)</span></span><br><span class="line"><span class="comment"># print(matplotlib.get_backend())</span></span><br><span class="line"><span class="comment"># Sequential按顺序构成的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="comment"># Dense全连接层</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br></pre></td></tr></table></figure><p>先准备我们需要的数据。随机生成100个随机值x，并随机产生100个噪声值。我们按 $y=0.1x+0.2$ 的公式，得到对应的y标签值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用numpy 生成100个 随机点</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 测试集，但其实都是随机的 用x_data 也可以</span></span><br><span class="line">x_pre = np.random.rand(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 噪声 使得每个点不是 均匀在一条直线上</span></span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.01</span>,x_data.shape)</span><br><span class="line">y_data = x_data * <span class="number">0.1</span> + <span class="number">0.2</span> + noise</span><br></pre></td></tr></table></figure><p>可以将 100个点的分布图画出。 注意图的显示可能有问题，自行解决一下哦。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><blockquote><p>Q：为什么我们要引入 噪声呢？<br>答：引入噪声可以让我们的数据更加的离散分布 在 我们设计的线性模型上。 使得假设的数据更加的合理。 如下图所示 。 <font color="orange">橙色</font>的是我们设定的线性模型，<font color="blue">蓝色</font>的是 加入噪声以后的数据分布<br><img src="https://img-blog.csdnimg.cn/92fd1e153f2845aba178b80a7845b8cb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><p>使用 keras 中的 Sequential （顺序构成的模型） 构建模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个顺序模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 在模型中添加一个全连接层</span></span><br><span class="line"><span class="comment"># units 输出的维度</span></span><br><span class="line"><span class="comment"># input_dim 输入的维度</span></span><br><span class="line">model.add(Dense(units=<span class="number">1</span>,input_dim=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># sgd 随机梯度下降法</span></span><br><span class="line"><span class="comment"># mse Mean Squared Error 均方误差</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;sgd&#x27;</span>,loss=<span class="string">&#x27;mse&#x27;</span>)</span><br></pre></td></tr></table></figure><p>之后我们就按照批次训练。 共训练3001个批次。有两种写法。<br>方法一：<br>用一个循环体，循环3001次； 每500次 打印一次 损失值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练3001个批次</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3001</span>):</span><br><span class="line">    <span class="comment">#每次训练一个批次</span></span><br><span class="line">    cost = model.train_on_batch(x_data,y_data)</span><br><span class="line">    <span class="comment"># 每500个 batch 打印一次 cost值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;cost:&quot;</span>,cost)</span><br></pre></td></tr></table></figure><br>方法二： 直接使用 model.fit () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x_data,y_data,epochs=<span class="number">3001</span>)</span><br></pre></td></tr></table></figure><p> 可以查看 参数值 W （权重）和 b（偏置值）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W,b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;W：&#x27;</span>,W,<span class="string">&#x27;b:&#x27;</span>,b)</span><br></pre></td></tr></table></figure><p>预测 测试集的 结果 使用 model.predict () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集 输入网络中，得到预测值 y_pred</span></span><br><span class="line">y_pred = model.predict(x_pre)</span><br></pre></td></tr></table></figure><p>可以再 把预测的 图打出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_pre,y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>我们的训练 使用方法二 <strong>结果如图所示：</strong><br><img src="https://img-blog.csdnimg.cn/45c3db30524a45a48e919881f6c36710.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="预测非线性模型"><a href="#预测非线性模型" class="headerlink" title="预测非线性模型"></a>预测非线性模型</h1><p>使用的数据 也是我们随机生成的</p><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便） 注意SGD 需要 tensorflow.keras.optimizers 导入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="comment"># Sequential按顺序构成的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="comment"># Dense全连接层</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Activation</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br></pre></td></tr></table></figure><p>先准备我们需要的数据。用等差数列生成200个值x，并随机产生200个噪声值。我们按 $y=x^{2}$ 的公式，得到对应的y标签值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用numpy 生成 200 个随机点</span></span><br><span class="line">x_data = np.linspace(-<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">x_pre = np.linspace(-<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">200</span>)</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.02</span>,x_data.shape)</span><br><span class="line">y_data = np.square(x_data) + noise</span><br></pre></td></tr></table></figure><p>可以将 200个点的分布图画出。 注意图的显示可能有问题，自行解决一下哦。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>使用 keras 中的 Sequential （顺序构成的模型） 构建模型。 <font color="red">与线性模型的区别在，我们需要 增加激活函数，并且增加一个 中间层（含有10个神经元）。</font>  <font color="blue">并且增加一点 sgd 的学习率，不然学习度太慢，需要的训练次数就会非常大。</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个顺序模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 在模型中添加一个全连接层</span></span><br><span class="line"><span class="comment"># units 输出的维度  维度就是神经元个数</span></span><br><span class="line"><span class="comment"># input_dim 输入的维度</span></span><br><span class="line"><span class="comment"># 需要的神经模型为 1-10-1</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">1</span>,activation=<span class="string">&#x27;tanh&#x27;</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">1</span>,input_dim=<span class="number">10</span>,activation=<span class="string">&#x27;tanh&#x27;</span>))</span><br><span class="line"><span class="comment"># sgd 随机梯度下降法</span></span><br><span class="line"><span class="comment"># mse Mean Squared Error 均方误差</span></span><br><span class="line"><span class="comment"># sgd 的学习率太小 训练次数可能非常多</span></span><br><span class="line"><span class="comment"># 需要修改一下 sgd的学习率</span></span><br><span class="line">sgd = SGD(lr=<span class="number">0.3</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;mse&#x27;</span>)</span><br></pre></td></tr></table></figure><p>之后我们就按照批次训练。 共训练3001个批次。有两种写法。<br>方法一：<br>用一个循环体，循环3001次； 每500次 打印一次 损失值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练3001个批次</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3001</span>):</span><br><span class="line">    <span class="comment">#每次训练一个批次</span></span><br><span class="line">    cost = model.train_on_batch(x_data,y_data)</span><br><span class="line">    <span class="comment"># 每500个 batch 打印一次 cost值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;cost:&quot;</span>,cost)</span><br></pre></td></tr></table></figure><br>方法二： 直接使用 model.fit () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x_data,y_data,epochs=<span class="number">3001</span>)</span><br></pre></td></tr></table></figure><p> 可以查看 参数值 W （权重）和 b（偏置值）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W,b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;W：&#x27;</span>,W,<span class="string">&#x27;b:&#x27;</span>,b)</span><br></pre></td></tr></table></figure><p>预测 测试集的 结果 使用 model.predict () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集 输入网络中，得到预测值 y_pred</span></span><br><span class="line">y_pred = model.predict(x_pre)</span><br></pre></td></tr></table></figure><p>可以再 把预测的 图打出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_pre,y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>我们的训练 使用方法二 <strong>结果如图所示：</strong><br><img src="https://img-blog.csdnimg.cn/79b89f1dec6e4327a2679c2c9882d6c7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras 构建 线性模型和非线性模型&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;预测线性模型&quot;&gt;&lt;a href=&quot;#预测线性模型&quot; class=&quot;headerlink&quot; title=&quot;预测线性模型&quot;&gt;&lt;/a&gt;预测线性模型&lt;/h1&gt;&lt;p&gt;使用的数据 是我</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
    <category term="线性模型" scheme="http://example.com/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="非线性模型" scheme="http://example.com/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>各类 激活函数 详解与选择</title>
    <link href="http://example.com/2021/08/24/%E5%90%84%E7%B1%BB%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://example.com/2021/08/24/%E5%90%84%E7%B1%BB%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2021-08-24T09:22:01.000Z</published>
    <updated>2021-08-24T09:28:29.357Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="各类 激活函数 详解与选择">TOC</a></p><h1 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h1><h1 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h1><h1 id="sigmiod"><a href="#sigmiod" class="headerlink" title="sigmiod"></a>sigmiod</h1><h1 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;各类 激活函数 详解与选择&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Relu&quot;&gt;&lt;a href=&quot;#Relu&quot; class=&quot;headerlink&quot; title=&quot;Relu&quot;&gt;&lt;/a&gt;Relu&lt;/h1&gt;&lt;h1 id=&quot;tanh&quot;&gt;&lt;a href=&quot;#t</summary>
      
    
    
    
    <category term="损失函数与激活函数" scheme="http://example.com/categories/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
    
    <category term="激活函数" scheme="http://example.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>各类 损失函数 详解与选择</title>
    <link href="http://example.com/2021/08/23/%E5%90%84%E7%B1%BB%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://example.com/2021/08/23/%E5%90%84%E7%B1%BB%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2021-08-23T09:22:01.000Z</published>
    <updated>2021-08-24T09:28:21.357Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="各类 损失函数 详解与选择">TOC</a></p><h1 id="mean-squared-error-均方误差"><a href="#mean-squared-error-均方误差" class="headerlink" title="mean_squared_error 均方误差"></a>mean_squared_error 均方误差</h1><p><img src="https://img-blog.csdnimg.cn/a55e390e62dd4e0b8bd1c9ff5cfb7826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;各类 损失函数 详解与选择&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;mean-squared-error-均方误差&quot;&gt;&lt;a href=&quot;#mean-squared-error-均方误差&quot; class=&quot;headerlink&quot; title=&quot;mean_s</summary>
      
    
    
    
    <category term="损失函数与激活函数" scheme="http://example.com/categories/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
    
    <category term="激活函数" scheme="http://example.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow 代码学习</title>
    <link href="http://example.com/2021/08/22/Tensorflow%20%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/"/>
    <id>http://example.com/2021/08/22/Tensorflow%20%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-08-22T06:05:01.000Z</published>
    <updated>2021-08-24T10:26:08.692Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Tensorflow 代码学习">TOC</a></p><p>机器学习的整体思路为：</p><p><img src="https://img-blog.csdnimg.cn/d73975270b58452892a3cd9133defd42.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="用TensorFlow-预测线性模型"><a href="#用TensorFlow-预测线性模型" class="headerlink" title="用TensorFlow 预测线性模型"></a>用TensorFlow 预测线性模型</h1><p>我们以这个做最简单的栗子，题目描述如下图所示：<br><img src="https://img-blog.csdnimg.cn/e862ba592668447dadbb087a5a43ef87.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="问题描述"><br>这就是一个 最简单的数据模型。</p><p>首先我们要引入需要的包，这边使用的是 keras API包<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><br>先构建模型，构建的为一层的神经网络，输入只有一个变量x<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([keras.layers.Dense(units=<span class="number">1</span>,input_shape=[<span class="number">1</span>])])</span><br></pre></td></tr></table></figure><br>再设置 优化器 和 损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;sgd&#x27;</span>,loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)</span><br></pre></td></tr></table></figure><br>准备训练数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xs = np.array([-<span class="number">1.0</span>,<span class="number">0.0</span>,<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>],dtype=<span class="built_in">float</span>)</span><br><span class="line">ys = np.array([-<span class="number">3.0</span>,-<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">3.0</span>,<span class="number">5.0</span>,<span class="number">7.0</span>],dtype=<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure><br>训练模型 迭代500次<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(xs,ys,epochs=<span class="number">500</span>)</span><br></pre></td></tr></table></figure><br>使用模型，对一个 测试集 x 进行预测 y 并输出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.predict([<span class="number">10.0</span>]))</span><br></pre></td></tr></table></figure></p><p>总体代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#构建模型</span></span><br><span class="line"><span class="comment">#构建一层的神经网络，并且输入只有1个值</span></span><br><span class="line">model = keras.Sequential([keras.layers.Dense(units=<span class="number">1</span>,input_shape=[<span class="number">1</span>])])</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;sgd&#x27;</span>,loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#准备训练数据</span></span><br><span class="line">xs = np.array([-<span class="number">1.0</span>,<span class="number">0.0</span>,<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>],dtype=<span class="built_in">float</span>)</span><br><span class="line">ys = np.array([-<span class="number">3.0</span>,-<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">3.0</span>,<span class="number">5.0</span>,<span class="number">7.0</span>],dtype=<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">model.fit(xs,ys,epochs=<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用模型，对一个x 进行预测 y</span></span><br><span class="line"><span class="built_in">print</span>(model.predict([<span class="number">10.0</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>结果如下所示：<br><img src="https://img-blog.csdnimg.cn/e82d07bccb7441059c354a3bde71f0e8.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="实验结果"><br>预测的结果是 18.977886   按照 正确的数学模型 $y = 2x - 1$ 结果应该是 19 。但深度学习不可能完美预测这个值，只能是近似。</p><h2 id="需要理解的几个点"><a href="#需要理解的几个点" class="headerlink" title="需要理解的几个点"></a>需要理解的几个点</h2><blockquote><p>Q1 ：请问 <code>model.fit(xs,ys,epochs=500)</code> 的 epochs 500 是什么意思？<br>就是针对同一批数据，利用各类算法（比如梯度下降算法），优化训练的次数，理论上训练次数越多，损失函数越小，准确度越高。</p><p>Q2：如何看待这个模型是不是正确的？ 第一，需要看输出的 loss 是不是越来越小 ，accuracy 越来越高。 如果loss<br>不是越来越小，那就说明有问题 第二，你可以看看在测试集 上表现怎么样。</p><p>Q3：请问 这个 epochs 越多越好么？ 当然不是，正常来说 模型在测试集上的表现 是不如训练集的。 要选取一个合适的 epochs 值，不然会出现 过拟合的现象。<img src="https://img-blog.csdnimg.cn/56bdd71ef3784049ad7ddd4003317a7e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>  <strong>注意：过拟合 是在测试集上的概念。是训练集上表现不错，但测试集表现不尽人意，叫做过拟合</strong></p></blockquote><h1 id="用TensorFlow-做全神经网络的图像识别分类"><a href="#用TensorFlow-做全神经网络的图像识别分类" class="headerlink" title="用TensorFlow 做全神经网络的图像识别分类"></a>用TensorFlow 做全神经网络的图像识别分类</h1><p>对 Fashion MNIST 进行图像分类 类别包括<br>0 T-shirt/top(体恤) 1 Trouser(裤子) 2 Pullover(套头衫) 3 Dress(连衣裙) 4 Coat(外套) 5 Sandal(凉鞋) 6 Shirt(衬衫) 7 Sneaker(运动鞋) 8 Bag(袋子) 9 Ankle boot(短靴）<br><img src="https://img-blog.csdnimg.cn/fd0a80f2af654b2d83c71cef332f3e53.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="总体代码"><a href="#总体代码" class="headerlink" title="总体代码"></a>总体代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">fashion_mnist = keras.datasets.fashion_mnist</span><br><span class="line">(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data()</span><br><span class="line"><span class="comment">#具体值 每一个数字都是灰度值</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#可以可视化的 查看其中的图片</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建一个全连接的神经网络</span></span><br><span class="line">model = keras.Sequential()</span><br><span class="line"><span class="comment">#输入层</span></span><br><span class="line">model.add(keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line"><span class="comment">#中间层 128个神经元 激活函数使用relu</span></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">128</span>,activation=tf.nn.relu))</span><br><span class="line"><span class="comment">#输出层 10个神经元 激活函数使用softmax</span></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>,activation=tf.nn.softmax))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中间层  参数共有 100480个</span></span><br><span class="line"><span class="comment">#为 784 × 128 = 100352  还要再加上 每个神经元都有的 bias 100352+128=100480</span></span><br><span class="line"><span class="comment"># 输出层  参数共有 1290个</span></span><br><span class="line"><span class="comment"># 同理 为 128 × 10 + 10 = 1290</span></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer 优化器   loss：损失函数</span></span><br><span class="line"><span class="comment"># 当标签是除了0，1以外有其他数字的  用sparse_categorical_crossentropy</span></span><br><span class="line"><span class="comment">#为 one-hot  只有一个1 如： [0,0,0,1]用 categorical_crossentropy</span></span><br><span class="line">train_images_scaled = train_images/<span class="number">255</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.optimizers.Adam(),loss=tf.losses.sparse_categorical_crossentropy,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">model.fit(train_images_scaled,train_labels,epochs=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">test_images_scaled = test_images/<span class="number">255</span></span><br><span class="line"><span class="comment"># # 输出 loss 和 accuracy</span></span><br><span class="line"><span class="comment"># model.evaluate(test_images_scaled,test_labels)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(np.shape(test_images[<span class="number">0</span>]/<span class="number">255</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要满足输入的维度, 并从</span></span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images[<span class="number">0</span>]/<span class="number">255</span>).reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images[<span class="number">0</span>]/<span class="number">255</span>).reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>))))</span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>先引入 需要的包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br></pre></td></tr></table></figure><h2 id="加载Fashion-MNIST数据集"><a href="#加载Fashion-MNIST数据集" class="headerlink" title="加载Fashion MNIST数据集"></a>加载Fashion MNIST数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fashion_mnist = keras.datasets.fashion_mnist</span><br><span class="line">(train_images,train_labels),(test_images,test_labels) = fashion_mnist.load_data()</span><br></pre></td></tr></table></figure><p>可以查看一下 图片内容是什么  是个 28 × 28 的二维数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#具体值 每一个数字都是灰度值</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以可视化的 查看一下 这张图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可以可视化的 查看其中的图片</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/c40c76a3833444aa9b950977e09d3a95.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="构造神经元网络模型"><a href="#构造神经元网络模型" class="headerlink" title="构造神经元网络模型"></a>构造神经元网络模型</h2><p>有两种表达方式<br>方式一 ：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建一个全连接的神经网络</span></span><br><span class="line">model = keras.Sequential()</span><br><span class="line"><span class="comment">#输入层</span></span><br><span class="line">model.add(keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line"><span class="comment">#中间层 128个神经元 激活函数使用relu</span></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">128</span>,activation=tf.nn.relu))</span><br><span class="line"><span class="comment">#输出层 10个神经元 激活函数使用softmax</span></span><br><span class="line">model.add(keras.layers.Dense(<span class="number">10</span>,activation=tf.nn.softmax))</span><br></pre></td></tr></table></figure><br>方式二 ： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model=tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">512</span>, activation=tf.nn.relu),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax)</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>可以 用summary 函数  查看各层的信息 包括参数等;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/4329a84b31e948dc82268215a010427c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>Q： 中间层参数 100480 怎么来的？</strong><br> 中间层  参数共有 100480个  ：为 784 × 128 = 100352  还要再加上 每个神经元都有的 bias 100352+128=100480<br> 输出层  参数共有 1290个 ： 同理 为 128 × 10 + 10 = 1290</p></blockquote><h2 id="归一化与训练数据"><a href="#归一化与训练数据" class="headerlink" title="归一化与训练数据"></a>归一化与训练数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_images_scaled = train_images/<span class="number">255</span>  <span class="comment">#归一化</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.optimizers.Adam(),loss=tf.losses.sparse_categorical_crossentropy,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">model.fit(train_images_scaled,train_labels,epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>Q：为什么只对 1875个 进行训练？ epoch 5 的 含义是什么？</strong><img src="https://img-blog.csdnimg.cn/152d19d34215451c90b946d3d3bcbf7e.png#pic_center" alt="在这里插入图片描述"><br> 训练没有问题。正在对1875批次（每批次32张图像）而不是1875张图像进行模型训练。     1875 × 32 = 60000张图像</p></blockquote><h2 id="评估模型-与-测试数据"><a href="#评估模型-与-测试数据" class="headerlink" title="评估模型 与 测试数据"></a>评估模型 与 测试数据</h2><p>评估模型的 loss 和 accuracy 使用 <font color="red">evaluate (测试集全体数据，测试集全体标签) 方法</font><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test_images_scaled = test_images/<span class="number">255</span></span><br><span class="line"><span class="comment"># # 输出 loss 和 accuracy</span></span><br><span class="line">model.evaluate(test_images_scaled,test_labels)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>如果要对 测试集图像 进行预测 需要使用<font color="red"> predict (测试集数据) 方法 </font>  输出最后的输出内容为 10维向量（因为一共0-9 10个分类 输出层已经设定好了） 然后再用 numpy的<font color="red">  argmax  </font> 取得向量中 值最大的那个 就是对应 预测的标签。<br><strong>要注意输入的维度 必须要与 输入层设定的维度 保持一致</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要满足输入的维度, 并从</span></span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images[<span class="number">0</span>]/<span class="number">255</span>).reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line"><span class="comment">#输出预测的标签</span></span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images[<span class="number">0</span>]/<span class="number">255</span>).reshape(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>))))</span><br><span class="line"><span class="comment">#对比一下 真实的标签是什么</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><h2 id="可以设定自动终止训练"><a href="#可以设定自动终止训练" class="headerlink" title="可以设定自动终止训练"></a>可以设定自动终止训练</h2><p>当损失值 小于 0.4 就终止 批次训练<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myCallback</span>(<span class="params">tf.keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self,epoch,logs=&#123;&#125;</span>):</span></span><br><span class="line">        <span class="keyword">if</span>(logs. get(<span class="string">&#x27;loss&#x27;</span>)&lt; <span class="number">0.4</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\ nLoss is low so cancelling training!&quot;</span>)</span><br><span class="line">            self.model.stop_training=<span class="literal">True</span></span><br><span class="line">            <span class="built_in">print</span>(model.predict((test_images[<span class="number">0</span>] / <span class="number">255</span>).reshape(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">            <span class="built_in">print</span>(np.argmax(model.predict((test_images[<span class="number">0</span>] / <span class="number">255</span>).reshape(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))))</span><br><span class="line">            <span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br><span class="line">callbacks=myCallback()</span><br><span class="line">mnist=tf.keras.datasets.fashion_mnist</span><br><span class="line">(training_images, training_labels),(test_images, test_labels)=mnist.load_data()</span><br><span class="line">training_images_scaled=training_images/<span class="number">255.0</span></span><br><span class="line">test_images_scaled=test_images/<span class="number">255.0</span></span><br><span class="line">model=tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">512</span>, activation=tf.nn.relu),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax)</span><br><span class="line">])</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">model.fit(training_images_scaled, training_labels, epochs=<span class="number">5</span>, callbacks=[callbacks])</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Tensorflow 代码学习&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;机器学习的整体思路为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/d73975270b58452892a3cd9133defd42.png?x</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>Numpy、Pandas、Matplotlib  常用代码</title>
    <link href="http://example.com/2021/08/21/Numpy%E3%80%81Pandas%E3%80%81Matplotlib%20%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/"/>
    <id>http://example.com/2021/08/21/Numpy%E3%80%81Pandas%E3%80%81Matplotlib%20%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/</id>
    <published>2021-08-21T02:18:01.000Z</published>
    <updated>2021-08-24T09:29:06.390Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Numpy、Pandas、Matplotlib  常用代码">TOC</a></p><h1 id="Numpy-常用代码"><a href="#Numpy-常用代码" class="headerlink" title="Numpy 常用代码"></a>Numpy 常用代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数组</span></span><br><span class="line">n = numpy.arange(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数组，并做2行3列的分隔</span></span><br><span class="line">m = numpy.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据，分隔成3位数组</span></span><br><span class="line">t = numpy.arange(<span class="number">27</span>).reshape(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(t)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载文本，为int方式</span></span><br><span class="line">tx1 = numpy.loadtxt(<span class="string">&quot;numpy.txt&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>, dtype=<span class="string">&quot;int&quot;</span>)</span><br><span class="line"><span class="comment"># 横列替换</span></span><br><span class="line">tx2 = numpy.loadtxt(<span class="string">&quot;numpy.txt&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>, dtype=<span class="string">&quot;int&quot;</span>, unpack=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(tx1)</span><br><span class="line"><span class="built_in">print</span>(tx2)</span><br><span class="line"><span class="comment"># 1:2横截取，[1,2]为选取</span></span><br><span class="line">tx3 = tx1[<span class="number">1</span>:<span class="number">2</span>,[<span class="number">1</span>,<span class="number">2</span>]]</span><br><span class="line"><span class="built_in">print</span>(tx3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 竖拼接</span></span><br><span class="line">tx4 = numpy.vstack((tx1, tx2))</span><br><span class="line"><span class="built_in">print</span>(tx4)</span><br><span class="line"><span class="comment"># 横拼接</span></span><br><span class="line">tx5 = numpy.hstack((tx1, tx2))</span><br><span class="line"><span class="built_in">print</span>(tx5)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">20</span>)</span><br></pre></td></tr></table></figure><h2 id="函数简介"><a href="#函数简介" class="headerlink" title="函数简介"></a>函数简介</h2><h3 id="arrange-函数：用于创建数值范围并返回数组对象"><a href="#arrange-函数：用于创建数值范围并返回数组对象" class="headerlink" title="arrange 函数：用于创建数值范围并返回数组对象"></a>arrange 函数：用于创建数值范围并返回数组对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numpy.arrange([<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>],dtype=numpy.int6或dtype=<span class="string">&#x27;i8&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="linspace-函数：-用于创建等差数组"><a href="#linspace-函数：-用于创建等差数组" class="headerlink" title="linspace 函数： 用于创建等差数组"></a>linspace 函数： 用于创建等差数组</h3><p>numpy.linspace(start,stop,num,endpoint,retstep,dtype)</p><p>dtype：默认为 float64<br>num：设置生成的元素个数<br>endpoint：设置是否包含结束值（stop），False为不包含，<strong>默认为True</strong><br>retstep：设置是否返回步长（即公差），False表示返回，<strong>默认为False</strong>。当值为 True时，返回值为 二元组，包括数组与步长。</p><h3 id="logspace-函数：-用于创建等比数组"><a href="#logspace-函数：-用于创建等比数组" class="headerlink" title="logspace 函数： 用于创建等比数组"></a>logspace 函数： 用于创建等比数组</h3><p>numpy.logspace(start,stop,num,endpoint,base,dtype)</p><p>start：开始值，值为$base^{start}$    =》 base为底的 start次幂<br>stop：结束值，值为$base^{stop}$    =》base为底的 stop次幂<br>base：底数<br>dtype：默认数据类型 float64<br>endpoint：True为包含结束值，默认为True</p><h2 id="numpy-练习题一"><a href="#numpy-练习题一" class="headerlink" title="numpy 练习题一"></a>numpy 练习题一</h2><p><strong>numpy 的基本用法</strong><br>1.导入numpy库<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><br>2.建立一个一维数组 a 初始化为[4,5,6],<br>(1)输出a 的类型（type）<br>(2)输出a的各维度的大小（shape）<br>(3)输出 a的第一个元素（值为4）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span>(a.dtype)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><br>3.建立一个二维数组 b,初始化为 [ [4, 5, 6],[1, 2, 3]]<br> (1)输出各维度的大小（shape）<br> (2)输出 b(0,0)，b(0,1),b(1,1) 这三个元素（对应值分别为4,5,2）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">b = np.array([[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">0</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">1</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(b.shape)</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">0</span>][<span class="number">0</span>],b[<span class="number">0</span>][<span class="number">1</span>],b[<span class="number">1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure></p><p>4  (1)建立一个全0矩阵 a, 大小为 3x3; 类型为整型（提示: dtype = int）(2)建立一个全1矩阵b,大小为4x5; (3)建立一个单位矩阵c ,大小为4x4; (4)生成一个随机数矩阵d,大小为 3x2.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">c = np.zeros([<span class="number">3</span>,<span class="number">3</span>],dtype=<span class="built_in">int</span>)</span><br><span class="line">d = np.ones([<span class="number">4</span>,<span class="number">5</span>],dtype=<span class="built_in">int</span>)</span><br><span class="line">e = np.identity(<span class="number">4</span>,dtype=<span class="built_in">int</span>)</span><br><span class="line">f = np.random.rand(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"><span class="built_in">print</span>(f)</span><br></pre></td></tr></table></figure><br>5  建立一个数组 a,(值为[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]] ) ,(1)打印a; (2)输出 下标为(2,3),(0,0) 这两个数组元素的值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">2</span>][<span class="number">3</span>],a[<span class="number">0</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure><br>6.把上一题的 a数组的 0到1行 2到3列，放到b里面去，（此处不需要从新建立a,直接调用即可）(1),输出b;(2) 输出b 的（0,0）这个元素的值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b = a[<span class="number">0</span>:<span class="number">2</span>,<span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(b[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"> <span class="comment">#？？？ 用ndarray不会</span></span><br></pre></td></tr></table></figure><br>7 把第5题中数组a的最后两行所有元素放到 c中，（提示： a[1:2, :]）(1)输出 c ; (2) 输出 c 中第一行的最后一个元素（提示，使用 -1 表示最后一个元素）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">c = a[<span class="number">1</span>:]</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(c[<span class="number">0</span>][-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><br>8.建立数组a,初始化a为[[1, 2], [3, 4], [5, 6]]，输出 （0,0）（1,1）（2,0）这三个元素（提示： 使用 print(a[[0, 1, 2], [0, 1, 0]]) ）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment">#花式索引 第一种</span></span><br><span class="line"><span class="built_in">print</span>(a[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br></pre></td></tr></table></figure><br>9.建立矩阵a ,初始化为[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]，输出(0,0),(1,2),(2,0),(3,1) (提示使用 b = np.array([0, 2, 0, 1]) print(a[np.arange(4), b]))<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#花式索引 第二种</span></span><br><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b = np.array([<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(a[np.arange(<span class="number">4</span>),b])</span><br></pre></td></tr></table></figure><br>10.对9 中输出的那四个元素，每个都加上10，然后重新输出矩阵a.(提示： a[np.arange(4), b] += 10 ）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数组广播</span></span><br><span class="line"><span class="built_in">print</span>(a[np.arange(<span class="number">4</span>),b]+<span class="number">10</span>)</span><br></pre></td></tr></table></figure><br><strong>numpy 的 array 数学操作</strong></p><ol><li>执行 x = np.array([1, 2])，然后输出 x 的数据类型<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>12.执行 x = np.array([1.0, 2.0]) ，然后输出 x 的数据类类型<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line"><span class="built_in">print</span>(x.dtype)</span><br></pre></td></tr></table></figure>13.执行 x = np.array([[1, 2], [3, 4]], dtype=np.float64) ，y = np.array([[5, 6], [7, 8]], dtype=np.float64)，然后输出 x+y ,和 np.add(x,y)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x+y)</span><br><span class="line"><span class="built_in">print</span>(np.add(x,y))</span><br><span class="line"><span class="comment">#总结：在numpy中，add和“+”是一样的</span></span><br></pre></td></tr></table></figure>14  利用 13题目中的x,y 输出 x-y 和 np.subtract(x,y)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x-y)</span><br><span class="line"><span class="built_in">print</span>(np.subtract(x,y))</span><br><span class="line"><span class="comment">#总结：在numpy中，subtract和“-”是一样的</span></span><br></pre></td></tr></table></figure>15  利用13题目中的x，y 输出 x*y ,和 np.multiply(x, y) 还有 np.dot(x,y),比较差异。然后自己换一个不是方阵的试试。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.multiply(x,y))</span><br><span class="line"><span class="built_in">print</span>(np.dot(x,y))</span><br><span class="line"><span class="built_in">print</span>(x*y)</span><br><span class="line"></span><br><span class="line"><span class="comment">##总结：np.multiply()：数组和矩阵对应位置相乘，输出与相乘数组/矩阵大小一致。</span></span><br><span class="line"><span class="comment"># np.dot():执行矩阵乘法运算，若秩为1，则执行对应位置相乘再相加。</span></span><br><span class="line"><span class="comment"># *：对array执行对应位置相乘</span></span><br></pre></td></tr></table></figure>16 利用13题目中的x,y,输出 x / y .(提示 ： 使用函数 np.divide())<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.divide(x,y))</span><br><span class="line"><span class="built_in">print</span>(x/y)</span><br><span class="line"><span class="comment">## np.divide()与 / 效果相同</span></span><br></pre></td></tr></table></figure>17 利用13题目中的x,输出 x的 开方。(提示： 使用函数 np.sqrt() )<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.sqrt(x))</span><br></pre></td></tr></table></figure>18.利用13题目中的x,y ,执行 print(x.dot(y)) 和 print(np.dot(x,y))<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x.dot(y))</span><br><span class="line"><span class="built_in">print</span>(np.dot(x,y))</span><br><span class="line"><span class="comment">##总结：二维数组矩阵之间dot函数运算得到的乘积是矩阵乘积，一维数组是两个向量的内积</span></span><br></pre></td></tr></table></figure>19.利用13题目中的 x,进行求和。提示：输出三种求和<br>(1)print(np.sum(x)): (2)print(np.sum(x，axis =0 )); (3)print(np.sum(x,axis = 1))<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(x))</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(x,axis=<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">sum</span>(x,axis=<span class="number">1</span>))</span><br><span class="line"><span class="comment">##总结：axis为0是压缩行,即将每一列的元素相加,将矩阵压缩为一行</span></span><br><span class="line"><span class="comment">## axis为1是压缩列,即将每一行的元素相加,将矩阵压缩为一列，再转置</span></span><br></pre></td></tr></table></figure>20.利用13题目中的 x,进行求平均数（提示：输出三种平均数(1)print(np.mean(x)) (2)print(np.mean(x,axis = 0))(3) print(np.mean(x,axis =1))）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.mean(x))</span><br><span class="line"><span class="built_in">print</span>(np.mean(x,axis = <span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(np.mean(x,axis =<span class="number">1</span>))</span><br><span class="line"><span class="comment">##总结：axis为0是压缩行,即将每一列的元素相加,将矩阵压缩为一行,再取平均值</span></span><br><span class="line"><span class="comment">## axis为1是压缩列,即将每一行的元素相加,将矩阵压缩为一列，再转置，再取平均值</span></span><br></pre></td></tr></table></figure>21.利用13题目中的x，对x 进行矩阵转置，然后输出转置后的结果，（提示： x.T 表示对 x 的转置）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x.T)</span><br></pre></td></tr></table></figure>22.利用13题目中的x,求e的指数（提示： 函数 np.exp()）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(np.exp(x))</span><br></pre></td></tr></table></figure>23.利用13题目中的 x,求值最大的下标（提示(1)print(np.argmax(x)) ,(2) print(np.argmax(x, axis =0))(3)print(np.argmax(x),axis =1))<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>,<span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>,<span class="number">5</span>]], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(np.argmax(x))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(x, axis =<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(x,axis =<span class="number">1</span>))</span><br><span class="line"><span class="comment">##总结： numpy.argmax(array, axis) 用于返回一个numpy数组中最大值的索引值。</span></span><br><span class="line"><span class="comment"># axis=0则竖着看，当axis=0，是在列中比较，选出最大的 行 索引</span></span><br><span class="line"><span class="comment"># axis=1则横着看, 当axis=1，是在行中比较，选出最大的 列 索引</span></span><br></pre></td></tr></table></figure>24,画图，y=x*x 其中 x = np.arange(0, 100, 0.1) （提示这里用到 matplotlib.pyplot 库）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x=np.arange(<span class="number">0</span>,<span class="number">100</span>,<span class="number">0.1</span>)</span><br><span class="line">y=x*x</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>25.画图。画正弦函数和余弦函数， x = np.arange(0, 3 * np.pi, 0.1)(提示：这里用到 np.sin() np.cos() 函数和 matplotlib.pyplot 库)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x=np.arange(<span class="number">0</span>, <span class="number">3</span>*np.pi, <span class="number">0.1</span>)</span><br><span class="line">y1=np.sin(x)</span><br><span class="line">y2=np.cos(x)</span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><h1 id="Pandas-常用代码"><a href="#Pandas-常用代码" class="headerlink" title="Pandas 常用代码"></a>Pandas 常用代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文件</span></span><br><span class="line">df = pandas.read_csv(<span class="string">&quot;BeijingPM20100101_20151231.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 展示</span></span><br><span class="line"><span class="comment"># print(df.head())</span></span><br><span class="line"><span class="comment"># print(df.info())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接时间</span></span><br><span class="line">period = pandas.PeriodIndex(year=df[<span class="string">&quot;year&quot;</span>], month=df[<span class="string">&quot;month&quot;</span>], day=df[<span class="string">&quot;day&quot;</span>], hour=df[<span class="string">&quot;hour&quot;</span>], freq=<span class="string">&quot;H&quot;</span>)</span><br><span class="line"><span class="comment"># 将时间数据赋值</span></span><br><span class="line">df[<span class="string">&quot;dataTime&quot;</span>] = period</span><br><span class="line"><span class="comment"># 设置索引</span></span><br><span class="line">df.set_index(<span class="string">&quot;dataTime&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># # print(period)</span></span><br><span class="line"><span class="comment"># print(df.head())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过月份统计</span></span><br><span class="line">df = df.resample(<span class="string">&quot;M&quot;</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># (统计)缺失</span></span><br><span class="line">data = df[<span class="string">&quot;PM_US Post&quot;</span>].dropna()</span><br><span class="line"></span><br><span class="line"><span class="comment"># pylot展示</span></span><br><span class="line">x = data.index</span><br><span class="line">y = data.values</span><br><span class="line"></span><br><span class="line">pyplot.figure(figsize=(<span class="number">20</span>, <span class="number">8</span>), dpi=<span class="number">80</span>)</span><br><span class="line">pyplot.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(x)), y)</span><br><span class="line">pyplot.xticks(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(x), <span class="number">3</span>), x[::<span class="number">3</span>])</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><h1 id="Matplotlib-常用代码"><a href="#Matplotlib-常用代码" class="headerlink" title="Matplotlib 常用代码"></a>Matplotlib 常用代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">7</span>]</span><br><span class="line">y_1 = [<span class="number">10</span>, <span class="number">15</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">15</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">17</span>]</span><br><span class="line">y_2 = [<span class="number">17</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">19</span>, <span class="number">17</span>, <span class="number">13</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">15</span>, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">pyplot.figure(figsize=(<span class="number">20</span>, <span class="number">12</span>), dpi=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整字体</span></span><br><span class="line">matplotlib.rc(<span class="string">&quot;font&quot;</span>, family=<span class="string">&quot;MicroSoft YaHei&quot;</span>,weight=<span class="string">&quot;bold&quot;</span>, size=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变刻度</span></span><br><span class="line"><span class="comment"># pyplot.xticks([ i + 1 for i in range(max(x))], [ &quot;time&quot; + str(i + 1) for i in range(max(x))], rotation=45)</span></span><br><span class="line"><span class="comment"># 第一个参数x轴 第二个展示的内容 rotation 旋转</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 描述</span></span><br><span class="line">pyplot.xlabel(<span class="string">&quot;时间&quot;</span>)</span><br><span class="line">pyplot.ylabel(<span class="string">&quot;温度&quot;</span>)</span><br><span class="line">pyplot.title(<span class="string">&quot;折线图&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 折线图</span></span><br><span class="line">pyplot.plot(x, y_1)</span><br><span class="line"><span class="comment"># pyplot.plot(x, y_2)</span></span><br><span class="line"><span class="comment"># 散点图</span></span><br><span class="line"><span class="comment"># pyplot.scatter(x, y_1)</span></span><br><span class="line"><span class="comment"># pyplot.scatter(x, y_2)</span></span><br><span class="line"><span class="comment"># 柱状图</span></span><br><span class="line"><span class="comment"># pyplot.bar(x, y_1)</span></span><br><span class="line"><span class="comment"># pyplot.bar(x, y_2)</span></span><br><span class="line"><span class="comment"># 横版柱状图</span></span><br><span class="line"><span class="comment"># pyplot.barh(range(len(x)), y_1, height=0.3)</span></span><br><span class="line"><span class="comment"># pyplot.barh(range(len(x)), y_2, height=0.3)</span></span><br><span class="line"><span class="comment"># 直方图</span></span><br><span class="line"><span class="comment"># pyplot.hist(x, (max(x)-min(x))//1)</span></span><br><span class="line">pyplot.xticks(<span class="built_in">range</span>(<span class="built_in">min</span>(x), <span class="built_in">max</span>(x) + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># pyplot.grid()</span></span><br><span class="line"><span class="comment"># 保存图片</span></span><br><span class="line"><span class="comment"># pyplot.savefig(&quot;link.png&quot;)</span></span><br><span class="line"></span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Numpy、Pandas、Matplotlib  常用代码&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Numpy-常用代码&quot;&gt;&lt;a href=&quot;#Numpy-常用代码&quot; class=&quot;headerlink&quot; title=&quot;Numpy 常用代码&quot;&gt;&lt;/a&gt;</summary>
      
    
    
    
    <category term="python框架" scheme="http://example.com/categories/python%E6%A1%86%E6%9E%B6/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Numpy" scheme="http://example.com/tags/Numpy/"/>
    
    <category term="Pandas" scheme="http://example.com/tags/Pandas/"/>
    
    <category term="Matplotlib" scheme="http://example.com/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>无监督学习（Unsupervised Learning）之 聚类与降维</title>
    <link href="http://example.com/2021/08/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Unsupervised%20Learning%EF%BC%89%E4%B9%8B%20%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/"/>
    <id>http://example.com/2021/08/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Unsupervised%20Learning%EF%BC%89%E4%B9%8B%20%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/</id>
    <published>2021-08-19T14:36:01.000Z</published>
    <updated>2021-08-20T10:39:03.082Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="无监督学习 Unsupervised  Learning">TOC</a></p><p><img src="https://img-blog.csdnimg.cn/9d69260c8d1442429d24ca7190dd4e5f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>总结 无监督学习 的要点：<br>1、无监督学习的概念</p><pre><code> - 什么叫无监督学习（输入都是无label的数据，没有训练集之说，也就是**只能从一些无label的数据中自己寻找规律**） - 无监督学习的两大任务：“化繁为简”（聚类、降维）、“无中生有”</code></pre><p>2、聚类Clustering（K-means、HAC）<br>3、降维Dimension Reduction（PCA）</p></blockquote><h1 id="无监督学习的具体分类？"><a href="#无监督学习的具体分类？" class="headerlink" title="无监督学习的具体分类？"></a>无监督学习的具体分类？</h1><p><img src="https://img-blog.csdnimg.cn/a3a3f30810614f508add1fd431cb5047.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="无监督学习的分类"></p><ul><li>化繁为简：找一个函数，将本来复杂的输入，变成比较简单的输出。<strong>比如找一个函数，可以把所有的树都变成抽象的树</strong>。因此我们拥有一大堆各种不同的图像的数据，但不知它的 output 长什么样子。</li><li>无中生有：找一个函数，随机给它一个input（比如一个数字1），然后output一棵树，输入数字2，output另外一棵树，输入3，又是另外一棵树。<strong>输入一个随机数，就自动画一张图出来，不同的数画出来的图不一样</strong>。这个任务里面，要找的可以画图的函数，只有output没有input。只有一大堆的图像，但是不知道输入什么数字才可以得到这些图像。</li></ul><h2 id="化繁为简包括-聚类"><a href="#化繁为简包括-聚类" class="headerlink" title="化繁为简包括 聚类"></a>化繁为简包括 聚类</h2><p><img src="https://img-blog.csdnimg.cn/436d9bf4a5a94f78b4ed65378066bcec.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="K-means聚类"></p><blockquote><p><strong>Q：什么是 聚类？</strong></p><p><font color="red"><strong>假设做图像的聚类，现在有一大堆的图像，然后把它们分成各类</strong>。</font>如上图左边的图像都属于 簇1，右边的图像都属于 簇2，上方的图像都属于 簇3。<strong>这就像给图像贴标签，把类似的图像，都用同一个簇表示</strong>，就做到化繁为简这件事情。</p><p><strong>Q：聚类的注意点是什么？</strong><br> 是 这些数据到底有多少个簇！ 这和神经网络需要设计几层一样，是需要算法工程师的个人经验的。</p><p><font color="red"><strong>这个簇不能太多也不能太少。</strong> </font>比如多到说9张图像9个簇，那聚类就没有意义，直接每个图像一个簇就好了，或者说全部图像都是一个簇，也跟没有做一样。</p></blockquote><p>聚类方法最常用的就是K-means，有一大堆未标注数据 $x^{1}$ 到 $x^{n}$ ，每一个 $x$ 代表一张图像，做成 K 个簇。</p><h3 id="K-means聚类算法怎么做？"><a href="#K-means聚类算法怎么做？" class="headerlink" title="K-means聚类算法怎么做？"></a>K-means聚类算法怎么做？</h3><p>先找簇的中心，假如每一个对象都用一个向量表示，有 K 个簇就需要 $c^{1}$  到 $c^{K}$  个中心。可以从训练数据里<strong>随机找 K个对象出来作为初始化中心。</strong><br>而后对所有数据，决定属于哪一个簇。假设 $x^{n}$  和 $c^{i}$  最接近，那么 $x^{n}$ 就属于 $c^{i}$ ，用 $b_{n}^{i}$ 表示。然后更新簇，所有属于 $c^{i}$ 的数据做平均，就是第 $i$ 个簇新的中心，更新要反复进行。</p><blockquote><p><strong>Q：为什么 是从数据集中挑选 K 个样本做初始化 簇中心？</strong><br><strong>答</strong>：之所以从数据集挑选K个样本做初始化簇中心，有一个很重要的原因是，如果是纯粹随机的（不从数据集里挑），那很可能在第一次分配这个簇中心的时候，没有任何一个样本跟这个中心很像，也可以说这个簇没有任何样本，再次更新就会出错。</p></blockquote><p>K-means 用更简单的话来说：<br>其算法思想大致为：先从样本集中随机选取 K 个样本作为簇中心，并计算所有样本与这 K 个“簇中心”的距离，对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中，对于新的簇计算各个簇的新的“簇中心”。循环反复。</p><blockquote><p><strong>Q：总结一下 K-means 算法的 主要流程</strong></p><ol><li>簇个数 K 的选择</li><li>初始化簇中心（可以从你的train data里面随机找K个x出来，就是你的k个center）</li><li>while（收敛——聚类结果不再变化）<br>　　{<br>　　　　 各个样本点到“簇中心”的距离 ；<br>  　　 根据新划分的簇，更新“簇中心”（求均值）;<br>　　}</li></ol></blockquote><h3 id="层次凝聚聚类算法（HAC）怎么做？"><a href="#层次凝聚聚类算法（HAC）怎么做？" class="headerlink" title="层次凝聚聚类算法（HAC）怎么做？"></a>层次凝聚聚类算法（HAC）怎么做？</h3><p>首先 我们要做一个树结构 （<strong>其过程 非常像 哈夫曼树的构造</strong>）<br><img src="https://img-blog.csdnimg.cn/463ed5f3dfb84f1bb525e37a39dfd42b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="HAC构建树"></p><p>假设有5个样本做层次聚类，先要做一个树结构。计算两两样本的相似度，挑出最相似的数据对。</p><p>比如第一个和第二个样本最相似，那就合并（比如用平均值代表），5个样本变为4个样本；再计算相似度，配对的是4，5样本，然后把他们合并（平均值），变成3个样本；接着计算相似度，配对的是黄色数据点和剩下的蓝色数据点，再次合并（平均），最后只剩红色和绿色，那么最后平均起来得到root。根据5笔数和之间的相似度，就建立出了一个树结构。</p><p><strong>但树结构只是告诉我们说哪些样本比较像，还没有做聚类。</strong></p><blockquote><p><strong>Q：那怎么做聚类呢？或者说我怎么看我分的那几个聚类？</strong><br><strong>答</strong>： 看你怎么切，如图上面不同颜色的切线。</p><ul><li>比如在上图蓝线初切一刀，意味着把数据分成3簇，1、2为一簇，3单独为一簇，4、5为一簇。</li><li>在红色线切一刀，则1、2、3为一簇，4、5为一簇。</li><li>在绿色点切一刀，则1、2为一簇，3、 4、 5单独为一簇。</li></ul><p><strong>Q：层次聚类 和 K-means的差别？</strong></p><ul><li>在K-means里要自己决定K的值，也就是你要分多少个簇。<ul><li>在层次聚类里要决定的是在哪里切一刀，如果切比较容易考虑的话，那层次聚类可能更好。</li></ul></li></ul></blockquote><h2 id="化繁为简包括-降维"><a href="#化繁为简包括-降维" class="headerlink" title="化繁为简包括 降维"></a>化繁为简包括 降维</h2><blockquote><p><strong>Q：什么是降维？</strong><br><strong>答</strong>：降维意思是说，原本高维的东西，其实是可以用低维去表示它。就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。换句话说，可以减少数据的维度。就是 </p><script type="math/tex; mode=display">z = Wx</script><p><img src="https://img-blog.csdnimg.cn/0f4adf004785425fa8e753ffc1e8c025.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>Q：为什么降维有用？</strong><br><strong>答</strong>：假设数据分布如上图左边，在3D空间里分布是螺旋的样子，但是用3D描述数据分布比较浪费的，直觉上也可以感觉可以摊开变成右边2D的样子，只需要2D的空间就可以描述3D的信息。在3D空间里面解比较麻烦，那就在2D里做这个任务。<br>考虑一个实际的简单栗子：<br>每一个input的数字都是28 × 28的矩阵来描述。但是实际上，多数28 × 28矩阵转成一个图像看起来都不像数字，在28 × 28空间里是数字的矩阵是很少的。所以要描述一个数字，或许不需要用到28 × 28维，远比28 × 28维少。<br><img src="https://img-blog.csdnimg.cn/1d2cd0f03fb540a886301947ef7bef60.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>所以举一个极端的例子，有一堆3，从像素点看要用28 × 28维来描述每张图像。实际上，只要用一个维度就可以表示，中间的是3，其他的3都是中间的3左转右转10、 20度。所以唯一需要记录的就是中间的3，左转和右转了多少度，即只需要角度的变化，就可以知道28维空间中的变化。</p><p><strong>Q：怎么做降维？</strong><br><strong>答</strong>：找一个函数，input是一个向量x，output是另外一个向量z（z的维度比x小）。</p><ul><li><p>在降维里最简单的方法是特征选择，把数据的分布拿出来看一下，    如在二维平面上发现数据集中在 $x$ 维度，所以 $y$ 这个维度没什么用，那么就把他拿掉，等于是降维这件事。<strong>特征选择不一定有用，有可能case里面任何一个维度都不能拿掉。</strong></p></li><li><p>另一个常见的方法是<strong>PCA</strong>，函数是一个很简单的线性函数，input x和output z之间的关系就是一个线性的transform，即 $x$ 乘上一个矩阵 $W$ 得到 $z$ 。现在不知道  $z$ 长什么样子，要根据一大堆的 $x$ 把 $W$ 找出来。</p></li></ul></blockquote><h3 id="分布式表示（Distributed-Representation）"><a href="#分布式表示（Distributed-Representation）" class="headerlink" title="分布式表示（Distributed Representation）"></a>分布式表示（Distributed Representation）</h3><blockquote><p><strong>Q：光做聚类的话是非常以偏概全的。为什么呢？</strong><br><strong>答</strong>：因为在聚类思想中，每个样本都必须属于某一个簇。就好像念力分成6大类，每个人都会被分配到6个大类其中一类。但这样分配太过粗糙，比如某个人的能力既有强化系的特性又有放出系的特性，只分为一类就会丢失很多信息。<img src="https://img-blog.csdnimg.cn/1ee8b3f2f1754e63bdc4826eba1aa3dc.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><font color="red">**只分为一类就是以偏概全了，应该要用一个向量来表示每个对象，向量的每个维度代表了某一种特质（属性）。这件事情叫做Distributed Representation**。</font> 比如上图所示，这个人每个系都可以有固定的能力占比。<font color="green">如果对象是一个高维的东西，例如图像，现在用它的特性来表示，就会把它从高维空间变成低维空间，这件事情叫做降维。</font> Distributed Representation和**降维**是一样的东西，不同的称呼。### 主成分分析（PCA）函数是一个很简单的线性函数，input x和output z之间的关系就是一个线性的transform，即 $x$ 乘上一个矩阵 $W$ 得到 $z$ 。现在不知道  $z$ 长什么样子，要根据一大堆的 $x$ 把 $W$ 找出来。PCA的实现一般有两种： - 一种是用特征值分解去实现的 - 一种是用奇异值分解去实现的#### PCA-用特征值分解实现![在这里插入图片描述](https://img-blog.csdnimg.cn/3acdbf6ffca44f85b12bb6a0719ae8c9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)刚才讲过PCA要做的事是找 $W$ ，假设一个比较简单的case，考虑一个维度的case。假设要把我们的数据投射到一维空间上，即 $z$ 只是一维的向量。 $w^{1}$ 是W的第一行，和 $x$ （列向量）做内积得到一个标量 $z_{1}$ 。> **Q：$w^{1}$应该长什么样子？**> 首先假设 $w^{1}$ 的长度是1，即 $||w^{1}||_{2}=1$ 。如果$||w^{1}||_{2}=1$，$w^{1}$ 是高维空间中的一个向量，那么 $z_{1}$ 就是就是 $x$ 在 $w^{1}$ 上的投影长度。现在要求出每一个 $x$ 在 $w^{1}$ 上的投影，那 $w^{1}$ 应该长什么样子？举个例子，假设上图右上方是 $x$ 的分布，$x$ 都是二维的，每个点代表一只宝可梦，横坐标是攻击力，纵坐标是防御力。<font color="blue">现在要把二维投影到一维，应该要选什么样的 $w^{1}$ ?</font>  可以选 $w^{1}$ 为上图右上方右斜方向，也可以选左斜方向，**选不同的方向，最后得到的投影的结果会不一样**。那总要给我们一个目标，我们才知道要选什么样的 $w^{1}$ ，现在目标是经过投影后得到的 $z_{1}$ 的分布越大越好。我们不希望投影后所有的点都挤在一起，把本来数据点之间的奇异度消去。我们希望投影后，数据点之间的区别仍然看得出来，那么我们可以找投影后方差越大的那个 $w^{1}$ 。看上面的例子，如果是右斜方向，那么方差较大，左斜方向方差则较小，所以更可能选择右斜方向作为 $w^{1}$ 。从上面的例子里看， $w^{1}$ 代表了宝可梦的强度，宝可梦可能有一个隐藏的向量代表它的强度，这个隐藏的向量同时影响了防御力和攻击力，所以防御力和攻击力会同时上升。![在这里插入图片描述](https://img-blog.csdnimg.cn/dd45a73b7b0640d1bbb675b9132e5044.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)开始计算：　　　　1、把方差式子展开，转化成协方差（具体转化过程不描述了）　　　　2、<font color="red">结论：我们要找的 $w^{1}$ 就是协方差矩阵 $S$ 的最大特征值所对应的特征向量， $w^{2}$ 就是协方差矩阵 $S$ 的第二大特征值所对应的特征向量，以此类推 .</font><h4 id="PCA-用奇异值分解（SVD）"><a href="#PCA-用奇异值分解（SVD）" class="headerlink" title="PCA - 用奇异值分解（SVD）"></a>PCA - 用奇异值分解（SVD）</h4><p>特征值分解是一个提取矩阵特征很不错的方法，但是<strong>特征值分解只是对方阵而言的</strong>，在现实的世界中，我们看到的<strong>大部分矩阵都不是方阵</strong>。奇异值分解是一个能适用于任意的矩阵的一种分解的方法。<br><img src="https://img-blog.csdnimg.cn/31679a1b2ba54844b2799661d9c1ff6c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>假设现在考虑手写数字识别，我们知道手写数字其实是由一些基本成分组成的，这些基本成分可能是笔画。例如斜的直线，横的直线，比较长的直线，小圈、大圈等等，这些基本成分加起来以后得到一个数字。</p><p>基本成分我们写作 $u^{1},u^{2},u^{3}…$，这些基本的成分其实就是一个一个的向量。考虑 MNIST数据集 ，一张图像是28 × 28像素，就是28 × 28维的向量。基本成分其实也是28 × 28维的向量，把这些基本成分向量加起来，得到的向量就代表了一个数字。</p><p>如果写成公式的话，就如上图最下方所示的公式。$x$ 代表某一张图像的像素，用向量表示。$x$ 会等于 $u^{1}$ 这个成分乘上 $c<em>{1}$ ，加上 $u^{2}$ 这个成分乘上 $c</em>{2}$，一直加到 $u^{K}$ 这个成分乘上$c_{K}$，再加上 $\bar{x}$（$\bar{x}$ 是所有图像的平均）。所以每一张图像，就是一堆成分的线性组合加上所有图像的平均所组成的。</p><p>例如数字7是 $u^{1},u^{3},u^{5}$ 加起来的结果，那么对数字7来说，公式里的 $c<em>{1}=1 ,c</em>{2}=0, c<em>{3}=1…$，所以可以用  $c</em>{1},c<em>{2},c</em>{3}…,c<em>{K}$ 来表示一张图像，如果成分远比像素维度小的话，那么用$\begin{bmatrix}<br>c</em>{1}\<br>c<em>{2}\<br>…\<br>c</em>{K}\<br>\end{bmatrix}$表示一张图片是会比较有效的比如7可以由向量 $\begin{bmatrix}<br>1\<br>0\<br>1\<br>0\<br>1\<br>…\<br>\end{bmatrix}$ 描述。</p><p><img src="https://img-blog.csdnimg.cn/dc78fa3c54bc4b2f94be1efcc5bdf034.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>我们把公式里的 $\bar{x}$ 移到左边，$x$ 减 $\bar{x}$ 等于一堆成分的线性组合，写作 $\hat{x}$  。</p><blockquote><p><strong>Q：如果我们不知道K个u（成分）是什么，那怎么找出这K个向量？</strong> 找K个u，让$x−\bar{x}$ 和 $\hat{x}$  越接近越好，$||(x-\bar{x})-\hat{x}||<em>{2}$ 称为重构误差，代表没办法用成分描述的部分。接下来，最小化 $||(x-\bar{x})-\hat{x}||</em>{2}$，损失函数如上图 $L$。</p><p>回忆下PCA，$w<em>{1},w</em>{2},w<em>{3}…w</em>{K}$ 是 $x$ 协方差矩阵的特征向量，事实上 $L$ 的解就是PCA的 $w<em>{1},w</em>{2},w<em>{3}…w</em>{K}$。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/cf51702d73ef4fa3ac7f2dd6c47ec6d6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="PCA实例"><a href="#PCA实例" class="headerlink" title="PCA实例"></a>PCA实例</h1><h2 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h2><p><img src="https://img-blog.csdnimg.cn/9581a527b4a0468f97458d5954e6bb89.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>以把每一张数字图像拆成成分的线性组合，每一个成分也是一张图像（28 × 28 维的向量），所以可以把成分画在图上变成一张图像。</p><font color="blue">通过PCA画出前30个成分如上图所示，白色的地方代表有笔画。用这些成分做线性组合，就可以得到0-9的数字，所以这些成分叫做Eigen-digit。</font> Eigen（本征）是说，这些成分都是协方差矩阵的特征向量。## 人脸识别![在这里插入图片描述](https://img-blog.csdnimg.cn/9e97b877a5114592ac90e2b7f5f3c0f6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)上图右上方有一大堆人脸，找它们前30个主成分。找出来就如上图最下方所示，每张图像都是哀怨的脸，叫做Eigen-face。把这些脸做线性组合，就可以得到所有的脸。> **Q：但这边有没有觉得有问题，因为主成分找出来的是成分，但是现在找出来的几乎都是完整的脸，也不像是成分啊？像前面的数字识别，成分看起来也像是玛雅文字，而不是笔画，看起来也不是成分啊？**![在这里插入图片描述](https://img-blog.csdnimg.cn/1215e23adb994fd6b8380a73b1e64527.png#pic_center)> **答**：仔细想想PCA的特性，$α_{1},α_{2}$ 这种权重可以是任何值，可以是正的，也可以是负的。所以当我们用这些主成分组成一张图像的时候，<font color="blue">可以把这些成分相加，也可以把这些成分相减，这就会导致你找出的东西不见得是一个图的基本的结构。</font>> > 比如我画一个9，那可以先画一个8，然后把下面的圆圈减掉，再把一杠加上去。我们不一定是把成分加起来，也可以相减，<font color="blue">所以说就可以先画一个很复杂的图，然后再把多余的东西减掉。这些成分不见得就是类似笔画的这种东西。</font>> > 如果要得到类似笔画的东西，就要用另一个技术*NMF（非负矩阵分解）*。PCA可以看成是对矩阵X做SVD，SVD就是一种矩阵分解的技术。**如果使用NMF，就会强迫所有成分的权重都是正的，正的好处就是一张图像必须由成分叠加得到，不能说先画一个复杂的东西再去掉一部分，再来就是所有成分的每个维度都必须是正的。**所以在同样的任务上，例如手写数字的测试上，使用NMF时，找出来的主成分会如下图所示。![在这里插入图片描述](https://img-blog.csdnimg.cn/ac0fc298e03847819527bc49a3198b45.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)你会发现，白色图案类似于笔画，找出来的主成分就成了笔画了。![在这里插入图片描述](https://img-blog.csdnimg.cn/36c61755d6ce40318f4df22c47721ab5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)看脸的话，会发现如上图所示。比较像脸的一部分，比如人中、眉毛、嘴唇、下巴。## 宝可梦![在这里插入图片描述](https://img-blog.csdnimg.cn/c19d6fca60984ad6a435fd0750f485fa.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)有800种宝可梦，每种宝可梦可以用6个特征来表示。所以每个宝可梦就是6维的数据点，6维向量。现在用PCA来分析，PCA里常有的问题是到底需要几个成分，即到底要把数据降到几维。这个一般取决于你的目的是什么，比如你想做可视化，分析宝可梦特性之间的关系，6维没办法可视化的，那就投影到二维。要用几个主成分就好像是神经网络需要几层，每层几个神经元一样。一个常见决定使用几个主成分的方法是，去计算每个主成分（特征向量）对应的特征值，这个特征值代表在该主成分上投影数据的方差。现在的例子里宝可梦是6维的，那就有6 × 6维的协方差矩阵，所以有6个特征值，如上图计算每个特征值比例，结果是0.45，0.18，0.13，0.12，0.07，0.04。那第5、6个主成分的作用比较小，意味着投影数据的方差很小，宝可梦的特性在这两个主成分上信息很少。那么分析宝可梦特性只需要前4个主成分。![在这里插入图片描述](https://img-blog.csdnimg.cn/8fcb23f86b76476e9b66585379b04cbb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)PCA后选择4个主成分，每个主成分是一个6维向量（因为原来每个特征都要投影，那就有6种投影数据）。每个宝可梦可以想成是4主成分向量做线性组合的结果，且每只宝可梦组合的权重不同。看第一个主成分PC1，数值都是正的，如果给它的权重大，意味着宝可梦6维都是强的，给它的权重小，意味着宝可梦6维都是弱的，所以第一个主成分，代表了这只宝可梦的强度。看第二个主成分PC2，Def防御力是正值，速度是负值，那么增加权重的时候，会增加防御力并减小速度。把第一个和第二个主成分画出来如上图最下方，图上有800个点，每个点代表一只宝可梦。![在这里插入图片描述](https://img-blog.csdnimg.cn/995d8f5d622b4b3691e432d0e2554a0e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)第三个主成分PC3，特殊防御力是正的，攻击力和HP都是负的，也就是说这是用攻击力和HP来换取特殊防御力的宝可梦。第四个主成分PC4，HP是正的，攻击力和防御力是负的，这是用攻击力和防御力换取生命值的宝可梦。把第三、第四主成分画出来如上图最下方，维度是去相关的。## 矩阵分解-推荐系统有时候，你会有两种东西，两种对象，它们之间受到某种共通的潜在因素操控。假设现在做一个调查，调查每个人手上买的公仔的数目，有5个宅男同学A,B,C,D,E，横轴的公仔人物是凉宫春日、御坂美琴、小野寺、小唯，调查结果如下图。![在这里插入图片描述](https://img-blog.csdnimg.cn/7557d3f48de34c97be70ec2ca4212e74.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)看这个矩阵可以发现，买凉宫春日的人，比较有可能有御坂美琴；买小野寺的人，也比较有可能买小唯。这说明人和公仔有一些共同的特性，有共同的因素在操控这些事情发生。动漫宅获取可以分成两种，一种是萌傲娇的，一种萌天然呆的。每个人都是萌傲娇和萌天然呆平面上的一个点，可以用一个向量表示，那么看上图，A是偏萌傲娇。每一个公仔角色，可能有傲娇属性或者天然呆属性，所以每一个角色，也是平面上一个点，可以用一个向量描述。如果某个人的属性和角色的属性匹配的话，他们背后的向量就很像（比如做内积的时候值很大），那么A就会买很多的凉宫春日。他们匹配的程度取决于潜在因素是不是匹配的。![在这里插入图片描述](https://img-blog.csdnimg.cn/d5f8553af83f4034bbcff5b07c95251a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)所以ABC的属性如上图最左边所示，A、B是萌傲娇的，B稍微没有那么傲娇，C是萌天然呆。每个动漫角色后面也有傲娇、天然呆这两种属性，如果人物属性和角色属性匹配的话，人买角色的可能性就很大。![在这里插入图片描述](https://img-blog.csdnimg.cn/006698ec686f439ea7e5161e55e7213f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)上图中右下方矩阵公式中，右边两个矩阵的N应该是M，代表M个人。我们知道的只有人买的角色的数目，然后凭着这种关系去推论每个人和每个动漫人物背后的潜在因素。每个人背后都有一个向量，代表萌傲娇或者萌天然呆的程度。每个角色后面也有一个序列，代表是傲娇或天然呆的属性。我们可以把购买的公仔数量合起来看做是一个矩阵X ，行数是人的数量，列数是公仔角色的数量。现在有一个假设，矩阵X里的每个元素都来自于两个向量的内积。为什么A会有5个凉宫春日的公仔，是因为 $r^{A}·r^{1}$ 的内积很大，约等于5。这件事情用数学公式表达的话，可以把 $r^{A}$ 到 $r^{M}$ 按列排起来，把 $r^{1}$ 到 $r^{4}$ 按行排起来，<font color="red">K是潜在因素的个数，一般没办法知道，需要自己测试出来。</font><blockquote><p><strong>Q：矩阵X的每个维度是什么？</strong><br>我们要做的事情就是找一组rA到rE，找一组r1到r4 ，让两个矩阵相乘后和矩阵X越接近越好，就是最小化重构误差。这个就可以用SVD来解，把Σ并到左边或右边变成两个矩阵就可以了。<br><img src="https://img-blog.csdnimg.cn/b2a2e51b938840b49fad8bcf3521f71b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>有时候有些信息是缺失的，比如上图所示的，你不知道A、B、C手上有没有小野寺，可能在那个地区没有发行，所以不知道发行的话到底会不会买。那用SVD就很怪，也可以把缺失值用0代替，但也很奇怪。</p><p>Q：那有缺失值怎么办呢？<br>可以用梯度下降的方法来做，写一个损失函数，让$r^{i}$（每个人背后的潜在因素）和$r^{j}$（角色背后的潜在因素）的内积和角色购买数量越接近越好。现在重点是，在<br>summation over<br>元素的时候，可以避开缺失的数据，如果值是缺失的，就不计算。有了损失函数后，就可以使用梯度下降了。<img src="https://img-blog.csdnimg.cn/7da15f3364a64482b6e8a8ed3d751acb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>根据刚才的方法实际计算一下，假设潜在因素的数量是2。那么A到E都是二维的向量，每个角色也是二维的向量。<br>数值代表了属性的程度，把大的用红色框框圈出来，会发现A、B萌同一组属性，C、D、E萌同一种属性，1,2有同样的属性，3,4有同样的属性。没有办法知道每个属性代表什么，要先找出这些潜在因素，再去分析它的结果。有了这些潜在因素数据，就可以用来预测缺失值。已经知道了$r^{A}$和$r^{3}$，那只要$r^{A}$和$r^{3}$做内积就可以了。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/d1cb16fb971b456ebe4b9e64b666e212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>之前的model可以做得更精致一点，刚才说A背后的潜在因素乘上 春日 背后的潜在因素，得到的结果就是矩阵里的数值。但是事实上，可能还会有其他因素操控这些数值。<br>那么更精确的写法就可以写成。</p><script type="math/tex; mode=display">r^{A}⋅r^{1}+b_{A}+b_{1}≈5</script><p>$b<em>{A}$是跟 $A$ 有关的标量，代表了 $A$ 有多喜欢买公仔，有的人就是喜欢买公仔，也不是喜欢某个角色。$b</em>{1}$是跟 春日 有关的标量，代表了角色有多想让人购买，这个事情是跟属性无关的，本来人就会买这个角色。</p><p>然后修改损失函数如上图所示，使用梯度下降求解即可。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;无监督学习 Unsupervised  Learning&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/9d69260c8d1442429d24ca7190dd4e5f.png?x-oss-p</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="无监督学习" scheme="http://example.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="聚类" scheme="http://example.com/tags/%E8%81%9A%E7%B1%BB/"/>
    
    <category term="降维" scheme="http://example.com/tags/%E9%99%8D%E7%BB%B4/"/>
    
    <category term="PCA" scheme="http://example.com/tags/PCA/"/>
    
    <category term="K-means" scheme="http://example.com/tags/K-means/"/>
    
    <category term="HAC" scheme="http://example.com/tags/HAC/"/>
    
  </entry>
  
  <entry>
    <title>半监督学习 Semi-Supervised</title>
    <link href="http://example.com/2021/08/17/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20Semi-Supervised/"/>
    <id>http://example.com/2021/08/17/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20Semi-Supervised/</id>
    <published>2021-08-17T14:36:01.000Z</published>
    <updated>2021-08-19T07:05:04.586Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="半监督学习 Semi-Supervised">TOC</a></p><blockquote><p>总结 半监督学习 的要点：<br>Q1：什么是Semi-Supervised？<br>Q2：Semi-Surpervised在生成模型中如何实现的（EM算法）？<br>Q3：Semi-Surpervised基于Low-density Separation（低密度分离）假设是如何实现的？<br>Q4：Semi-Surpervised基于Smoothness Assumption（平滑）假设是如何实现的？</p></blockquote><h1 id="什么是Semi-Supervised？"><a href="#什么是Semi-Supervised？" class="headerlink" title="什么是Semi-Supervised？"></a>什么是Semi-Supervised？</h1><p><img src="https://img-blog.csdnimg.cn/a671d2d30c944294a8b44fad632f7435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>大家知道在监督学习里，有一大堆的训练数据（由input和output对组成）。例如上图所示 $x^{j}$是一张图片，$y^{r}$ 是类别的 <em>label</em>。<br><strong>半监督学习是说，在label数据上面，有另外一组unlabeled的数据</strong>，写成$x^{u}$ (只有 input 没有 output )，有U笔 unlabeled 的数据。<br>通常做半监督学习的时候，我们常见的情景是 <strong>unlabeled 的数量远大于labeled 的数量</strong>（U&gt;&gt;R)。</p><blockquote><p><strong>举个栗子：现在我们要做一个猫狗分类</strong></p><ul><li>如果只考虑 labeled data，我们分类的分界线会画在中间；    </li><li>如果把 unlabeled data 也考虑进去，我们可能会根据 unlabeled data 的分布，分界线画成图中的斜线； 　<br>semi-supervised earning使用 unlabel 的方式往往伴随着<strong>一些假设</strong>，学习有没有用，取决于你这个<font color="red"><strong>假设合不合理</strong></font>。（比如灰色的点也可能是个狗不过背景跟猫照片比较像）<img src="https://img-blog.csdnimg.cn/994e98b4ab274acc8f394810b34dc7bf.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul></blockquote><p>半监督学习可以分成两种：</p><ul><li>一种叫做<strong>转换学习</strong>，unlabeled 数据就是 testing set ，使用的是testing set的特征。</li><li>另一种是<strong>归纳学习</strong>，不考虑testing set，学习model的时候不使用testing set。</li></ul><blockquote><p><strong>Q：用 testing set 作为 unlabeled 数据，不是相当于用到了未来数据吗？</strong><br><strong>答</strong>：用了 label 数据才算是用了未来数据，用 testing set 的特征不算是使用未来数据。<br><strong>Q：什么时候使用转换学习或者归纳学习？</strong><br><strong>答</strong>：看 testing set是不是给你了。在一些比赛里，testing set 它是给你的，那么就可以使用转换学习。但在真正的应用中，一般是没有 testing set 的，这时候就只能做归纳学习。<br><strong>Q：为什么使用半监督学习？</strong><br><strong>答</strong>：缺少 lable 的数据，比如图片，收集图片很容易，但是标注label很困难。半监督学习利用未标注数据做一些事。<br><strong>Q：用沙雕的简单日常语言，讲一讲什么是 半监督学习</strong><br><strong>答</strong>：对人类来说，可能也是一直在做半监督学习，比如小孩子会从父母那边做一些监督学习，看到一条狗，问父亲是什么，父亲说是狗。之后小孩子会看到其他东西，有狗有猫，没有人会告诉他这些动物是什么，需要自己学出来。</p></blockquote><h1 id="Semi-Surpervised在生成模型中如何实现的（EM算法）"><a href="#Semi-Surpervised在生成模型中如何实现的（EM算法）" class="headerlink" title="Semi-Surpervised在生成模型中如何实现的（EM算法）"></a>Semi-Surpervised在生成模型中如何实现的（EM算法）</h1><p><img src="https://img-blog.csdnimg.cn/33bc117e72974fe68bb8635b54369078.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>Q：在生成模型中为什么使用半监督学习？</strong><br><strong>答</strong>：在监督学习中，有一堆用来训练的样本（就是label），你知道它们分别属于类别1，还是类别2。会去估算类别1和类别2的先验概率<em>P(C1),P(C2)</em> ，然后计算类条件概率 <em>P(x|C1),P(x|C2)</em>  。<br>假设 <em>P(x|Ci)</em> 服从一个高斯分布。 假设类别1的数据是从均值为 <em>μ1</em>，协方差为 <em>Σ</em> 的分布中取出来的，而类别2的数据是从均值为 <em>μ2</em> ，协方差也为 <em>Σ</em> 的分布中取出来的（之前讲过共享协方差，效果会好一点）。<br>然后可以计算后验概率 <em>P(C1|x)</em> ，决定一个决策边界在哪里。<img src="https://img-blog.csdnimg.cn/81307014a68d4ae2a427e46e9212d4eb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如果今天有一些未标注数据，如上图绿点，那仍然假设均值和方差是μ1,μ2,Σ显然不合理。<br>如上图左下所示，Σ应该比较接近圆圈（蓝色圆圈），也许在类1采样的时候有问题，所以采样到奇怪的分布（蓝色椭圆）。如上图右下，类2的μ2不应该在橙色椭圆内，而应该在更下面。<br>这样会使先验概率受到影响，本来两个分布，正例数据是一样多，但是加入未标注数据之后，你可能会觉得类2的正例数据更多（先验概率就更大）。总之加入未标注数据后，会影响对均值和协方差的估测，继而影响类条件概率，最后影响了你的决策边界。</p></blockquote><p>通俗的讲，回顾有监督学习中的生成模型，由于data都是有label的，<em>P(Ci)</em> 是已知的，<em>P(x|Ci)</em> 是通过我们基于高斯分布的假设用最大似然估计出来的；现在半监督学习中的生成模型，data的一部分是unlabel的，<em>P(Ci)</em> 是不确定的（隐变量），<em>P(x|Ci)</em>的假设模型也不能套用原来的 <em>u</em> 等参数，这时候需要用<font color="red"><strong>EM算法</strong>(Expectation-Maximization algorithm，又译为期望最大化算法)</font></p><h2 id="EM算法-具体怎么做"><a href="#EM算法-具体怎么做" class="headerlink" title="EM算法 具体怎么做"></a>EM算法 具体怎么做</h2><p>EM算法适用于带有无法观测的隐变量的概率模型估计<br>初始化一组参数，如果是二分类任务，就是初始化类1和类2的先验概率、均值和协方差，可以随机初始化，用已经有标注的数据估测，统称为 <em>θ</em></p><ul><li><strong>第一步（E步）</strong>，用labeled data算出来的高斯模型参数 <em>θ</em> 代入公式去求出每一笔未标注数据（unlabeled data）的后验概率（属于类1 的概率）的 <em>P(C1|Xu)</em>；</li><li><strong>第二步（M步）</strong>，用<strong>极大似然估计</strong>更新 <em>P(Ci)</em> 以及高斯模型参数 <em>θ</em> ，求出 <em>P(x|Ci)</em>，进一步求出新的后验概率 <em>P(Ci|Xu)</em> ，重复这两步直到收敛（似然概率最大）</li><li>至于为什么更新参数是要加入<em>P(Ci|Xu)</em> 这一项，是因为EM算法的思想是把不确定的data用一个概率来表示label，而每一笔不确定的data都有可能来自 类C1 和 类C2，看右下图：<img src="https://img-blog.csdnimg.cn/7035355bb99c4d298af9344cdfb7614c.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="**加粗样式**在这里插入图片描述"></li></ul><blockquote><p>Q：EM 算法背后的理论是什么？<br>答：<strong>原来只有标注数据的时候</strong>，目标是最大化一个似然函数，那么给定θ，每一笔训练数据的似然函数值是可以计算的，然后把所有的似然函数值相加，就是总的似然函数值，然后找 <em>θ</em> 最大化。<em>θ</em> 有显式解，求最大值点（导数为0）。<br><strong>现在加入未标注数据后</strong>，我们不知道未标注数据来自哪一个类别，那么未标注数据出现的概率就是和  C1的联合概率+和C2的联合概率（相当于是$\sum<em>{C}P(x^{u},C^{i})$ 。接下来目标就是最大化 $P</em>{\theta }(x^{u})$ ，但是 $P_{\theta }(x^{u})$  的式子是非凸的，所以使用<strong>EM算法求解</strong>。<img src="https://img-blog.csdnimg.cn/5872e4d039e4413295b66b457cec8e23.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><h1 id="Semi-Surpervised基于Low-density-Separation（低密度分离）"><a href="#Semi-Surpervised基于Low-density-Separation（低密度分离）" class="headerlink" title="Semi-Surpervised基于Low-density Separation（低密度分离）"></a>Semi-Surpervised基于Low-density Separation（低密度分离）</h1><p><img src="https://img-blog.csdnimg.cn/ea72d9a8bb3448088c224becd5be516f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>低密度分离的假设是，不确定的data的label要不是1，要不是0（“非黑即白”）。低密度的意思是，两个Class的分界处是低密度的（分得比较开的）</strong></p><blockquote><p><strong>Q：这个世界是非黑即白的，什么是非黑即白？</strong><br><strong>答</strong>：假设现在有一大堆的data，有标注数据，有非标注数据，在两个类别之间会有一个明显的鸿沟。给一些标注数据，可以把边界分在上图右边的线，也可以把边界分在上图左边的线。但是考虑非标注数据，那么左边的边界会好一点，在边界处，两个类别的密度是低的（不会出现data）</p></blockquote><h2 id="Self-training-Entropy-based-Regularization-基于熵的正则化"><a href="#Self-training-Entropy-based-Regularization-基于熵的正则化" class="headerlink" title="Self-training + Entropy-based Regularization(基于熵的正则化)"></a>Self-training + Entropy-based Regularization(基于熵的正则化)</h2><h3 id="self-training"><a href="#self-training" class="headerlink" title="self-training"></a>self-training</h3><p><img src="https://img-blog.csdnimg.cn/007143d015a34ef9b00537c7dfc0c066.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>低密度分离最代表性、最简单的方法是self-training，非常直觉。</p><p>我们有一些标注数据，和一些未标注数据。接下来：</p><ul><li>从标注数据训练一个model   f* (用DNN，deep、shallow还是其他机器学习的方法都可以)</li><li>根据f*  标注未标注数据，丢入$x^{u}$ ，得到$y^{u}$  ，${(x^{u} ,y^{u} )}^{R+U}_{u=l}$ 叫做伪标签数据（称为Pseudo-label伪标签）</li><li>接下来，从伪标签数据集移除一些数据加到标注数据集（移除哪些数据需要自己决定，设计一些启发式的规则，或者给权重，有些数据的标签比较确定，那就给大的权重）</li><li>有了更多的标注数据之后，回头再去训练model f*</li></ul><blockquote><p><strong>Q：self-training在回归上有用吗？</strong><br><strong>回归问题用self-training不影响f∗，所以回归问题不能用self-training方法。</strong><img src="https://img-blog.csdnimg.cn/16b0c52a9a89411f80d53e91dc040f16.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>self-training 很像是刚才生成模型里面用的EM算法，唯一的差别是在做 self-training 的时候，用的是<strong>硬标签</strong>，生成模型里用的是<strong>软标签（概率）</strong>。在做 self-training 的时候，会强制分配一个数据属于某一个类别，在生成模型里，使用的是<strong>后验概率</strong>，部分属于类1，部分属于类2。</p></blockquote><h3 id="Entropy-based-Regularization-基于熵的正则化-—-self-training的进阶版"><a href="#Entropy-based-Regularization-基于熵的正则化-—-self-training的进阶版" class="headerlink" title="Entropy-based Regularization(基于熵的正则化) — self-training的进阶版"></a>Entropy-based Regularization(基于熵的正则化) — self-training的进阶版</h3><p><img src="https://img-blog.csdnimg.cn/26e158dbdc714533a4f0173142f2db23.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>熵：一个事件的不确定程度</strong></p><p>Entropy-based Regularization（基于熵的正则化）是self-training的进阶版，self-training里用概率划分类别，可能觉得比较武断，那就可以用Entropy-based的这个方法。</p><p>Entropy-based是说，如果使用神经网络，output是一个分布，我们不去限制output具体属于哪一个类别，而是假设分布很集中（非黑即白的世界）。如上图，假设做5个类别分类的 model：</p><ul><li>第一个 类别1的概率为1，其他类别概率为0，是good</li><li>第二个 类别5的概率为1，其他类别概率为0，是good</li><li>第三个 所有类别的概率很平均，是bad，不符合<strong>低密度分离的假设（非黑即白）</strong></li></ul><blockquote><p><strong>Q：怎么用数值的方法评估分布是集中还是不集中？</strong></p><p><font color="red"><strong>使用熵，分布的熵告诉你集中还是不集中。可以用 每个类别的概率 乘以 log（每个类别的概率），再对类别个数求和取负数。 这个式子来评估。</strong> </font><br>这个其实就是理解成损失函数，因为这边讲究的是非黑即白，所以其实是一致的。损失函数 也可以用分布的距离来描述。<br>我们希望model的output在标注集上正确，在未标注集上的<strong>熵越小越好</strong>。</p><ul><li>第一个分布，熵为0，分布集中 <ul><li>第二个分布，熵也为0，分布集中 </li><li>第三个分布，熵为 ln(5) ，分布比较散</li></ul></li></ul><p>根据这个目标，重新设计损失函数。原来只是希望model在标注集上的output和label距离越近越好，用交叉熵来评估它们之间的距离。</p></blockquote><ul><li><strong>现在在原来的基础上，加上未标注集的output分布的熵。</strong></li><li>然后在未标注集部分乘上一个权重，来表明偏向标注部分还是未标注部分。</li><li>上图右下的损失函数可以算微分，那就使用<strong>梯度下降最小化这个损失函数</strong>，迭代求解参数。加入未标注部分，作用就类似于正则化（在原来损失函数后加一个L1正则或者L2正则），这里则加入一个未标注集熵来防止过拟合，所以称之为基于熵的正则化。</li></ul><h2 id="Semi-supervised-SVM（半监督SVM"><a href="#Semi-supervised-SVM（半监督SVM" class="headerlink" title="Semi-supervised SVM（半监督SVM)"></a>Semi-supervised SVM（半监督SVM)</h2><p><img src="https://img-blog.csdnimg.cn/ddbfa33ce3a4446581d49d9494a497ed.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>Q：SVM 支持向量机（Support Vector Machine）是什么？</strong><br>SVM是找边界，给你两个类别的数据，SVM找一个边界，这个边界一方面要有最大的间隔（让两个class分的越开越好），一方面要有最小的分类错误。<br>如上图，假设现在有一些未标注数据，半监督SVM会穷举所有可能的label。<br>上图中，有四笔未标注数据，每笔数据既可以属于 class1，也可以属于 class2，可能的情况如上图右边所示（还有很多种其他的可能）。然后对每个可能的结果，都去做一个SVM，边界如上图红色线。然后再去找让间隔最大，错误最小的那一种结果。在例子里可能是黑色框这种结果。</p></blockquote><h1 id="Semi-Surpervised基于Smoothness-Assumption（平滑性）假设是如何实现的"><a href="#Semi-Surpervised基于Smoothness-Assumption（平滑性）假设是如何实现的" class="headerlink" title="Semi-Surpervised基于Smoothness Assumption（平滑性）假设是如何实现的"></a>Semi-Surpervised基于Smoothness Assumption（平滑性）假设是如何实现的</h1><h2 id="平滑性假设与高密度区域"><a href="#平滑性假设与高密度区域" class="headerlink" title="平滑性假设与高密度区域"></a>平滑性假设与高密度区域</h2><font color="red">假设：x的分布是不平均的，在某些地方很集中，在某些地方又很分散。如果 x1 和 x2 在一个高密度的区域很相似的话，两者的标签也会很像。</font><blockquote><p><strong>Q：什么叫在高密度区域下呢？</strong><br> 意思是说可以用高密度的路径做连接<img src="https://img-blog.csdnimg.cn/211207011e154abc9dc1609ad946223f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>举个例子，假设数据的分布如上图右边所示，像一个血轮眼。现在有3笔数据：x1,x2,x3，x1和x2中间是一个高密度区域（x1和x2由一个高密度区域连接），有很多数据(中间想成平原地带，地势平坦，人烟很多)，而x2和x3之间数据稀少(中间想成一座山，人烟稀少)，那么走平原会比走山容易，x2走到x1更容易（更相似）。</p></blockquote><p><strong>Q：举一个现实中，相似图形的栗子？为什么会有高密度区域假设？</strong><br>因为在真实情况下，这个假设成立的可能性很高。<br><img src="https://img-blog.csdnimg.cn/a7767f1d4ea04b788d2f9f17bfcec96f.png#pic_center" alt="在这里插入图片描述"><br>我们考虑手写数字识别的例子，有两个2一个3，如果计算像素点相似度的话，可能上图右边的2和3更像。但是从所有数据中看，<strong>左边的2到右边的2中间会有很多连续的形态</strong>。所以根据平滑度假设，左边的2和右边的2更像，因为右边的2和3之间没有过渡的形态。<br><img src="https://img-blog.csdnimg.cn/dfdb44a5a6af4aad901c4185bd21b74c.png#pic_center" alt="在这里插入图片描述"><br>看人脸识别也是一样的，比如左脸像和右脸像差很多，两个人的左脸像计算像素点相似度的话，可能比同一个人的两张侧脸像更高。但是如果收集到足够多的未标注数据，会找到两个侧脸像的很多过渡形态，根据高密度区域假设，这两张侧脸像就是同一个人。</p><p><img src="https://img-blog.csdnimg.cn/12dc2e73270c49b08efeee9c7c8c1d74.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>高密度区域假设，在文件上非常有用，假如现在要区分天文学和旅游的文章。</p><p>天文学的文章会出现asteroid、bright，而旅游的文章会出现yellowstone、zion。如果未标注文章和标注文章的词语有重叠，那可以很容易分类。但真实情况情况是，未标注文章和标注文章可能没有任何词语重叠，因为世界上的词语太多了，一篇文章词汇不会很多，每篇文章的词语是非常稀疏的。<br><img src="https://img-blog.csdnimg.cn/87bec6b6bb8c4603aacffd11ce899920.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>但是收集到够多的数据的话，就可以说上图d1、d5像，d5、d6像，传播下去就可以说d1、d3是一类，d2、d4是一类。</p><h2 id="方法一：聚类，而后标注（图像上不太行）"><a href="#方法一：聚类，而后标注（图像上不太行）" class="headerlink" title="方法一：聚类，而后标注（图像上不太行）"></a>方法一：聚类，而后标注（图像上不太行）</h2><p><img src="https://img-blog.csdnimg.cn/45202fb11c71422c8437bdd3c23e91a5.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>Q：如何实践平滑度假设？</strong><br>最简单的方法是聚类、然后标记。<br>假如数据分布如上图，橙色是class 1，绿色是class 2，蓝色是未标注数据。接下来做聚类，可能把所有数据分成3个簇。在簇1里，class 1的label最多，那簇1里所有数据标记为class 1，同样的簇2和簇3都标记为class 2。把标记后的数据拿去learn就结束了。</p><p>这个方式不一定有用，因为要求簇正确，这个方法有效的假设是同一个class的东西聚集在一起。但是在图像里要把同一个class的东西聚集在一起没有那么容易。之前深度学习讲过，不同class的图像可能会很像，同一个class可能会不像，只用像素点做聚类，结果八成是不好的。没办法把同一个class的数据聚集在一起，那未标注数据就没有用。</p><p>所以要有用，就要有一个好的方法来描述一张图像，比如用 <em>Deep Autoencoder</em> 抽特征，然后再做聚类。</p></blockquote><h2 id="方法二：基于图的方法"><a href="#方法二：基于图的方法" class="headerlink" title="方法二：基于图的方法"></a>方法二：基于图的方法</h2><p><img src="https://img-blog.csdnimg.cn/95a83cefc106475f915f00fe756c317b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>另一个方法是引入图结构，来表达通过高密度路径进行连接这件事情。</p><p>把现在所有的数据点都建成一个图，每个数据点就是图上的一个点，想办法计算它们之间的奇点，想办法把它们之间的边建出来。</p><p><strong>所谓的高密度路径的意思是说，如果有两个点，在图上是相连的，那它们就是同一个class，如果没有相连，就算距离很近，也走不到。</strong></p><blockquote><p><strong>Q：生活中，如何构建图？</strong><br>有些时候，图的表示可以很自然的得到。</p><ul><li>举例说网页的分类，你有记录网页和网页之间的超链接，那超链接就很自然的告诉你网页是怎么连接的。</li><li>又举例论文的分类，论文和论文之间有引用的关系，这种引用的关系也是另外一种图的边，可以很自然地把这种图画出来。</li></ul></blockquote><h3 id="怎么自己想办法构建图？"><a href="#怎么自己想办法构建图？" class="headerlink" title="怎么自己想办法构建图？*"></a>怎么自己想办法构建图？*</h3><p>其实的图的好坏对结果的影响是非常严重的，但是自己用什么方法做还是很启发的，用自己觉得合适的方式做就可以了。 通常的做法是：</p><p>先定义两个对象之间的相似度，比如图像可以是基于像素点的相似度（可能效果不好），也可以是基于自动编码器抽取出来的特征计算相似度（效果可能好一点）</p><p>定义完相似度后，就可以构建图了（添加边），图有很多种：</p><ul><li>K近邻的图，现在有一大堆数据，可以计算数据与数据之间的相似度，然后设置k例如3，就是3个最相似的点相连</li><li>e-Neighborhood的图，只有相似度超过某个阈值的点才会相连</li></ul><p><strong>所谓的边也不是只有相连和不相连这两种选择，可以给边一些权重，让边跟两个数据点的相似度成正比。</strong>相似度可以用Gaussian Radial Basis Function来定义<img src="https://img-blog.csdnimg.cn/755264db6fe240d798d40d43d90ea95f.png#pic_center" alt=""></p><h3 id="怎么计算这个相似度"><a href="#怎么计算这个相似度" class="headerlink" title="怎么计算这个相似度"></a>怎么计算这个相似度</h3><p>可以先算xi,xj的欧式距离，乘以一个参数取负号，再取e为底的指数函数。取exp很有必要，在经验上最后效果比较好。因为取exp，下降速度很快，只有当xi,xj非常靠近时，奇点才会大，距离远一点奇点就会下降很快变得很小。这样才能制造如上图右下方所示的，两个距离近的橙色点有连接，绿色点和橙色点虽然距离也近，但是使用了exp导致只有很近很近的点才有连接，即使远一点点就不会有连接了，有这样的机制才能避免跨海沟的连接(橙色点和绿色点连接)。</p><blockquote><p><img src="https://img-blog.csdnimg.cn/f5e991680cf44ff08f79229892cd8926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt=""><br>基于图的方法是，如果现在在图上面有一些标注数据，比如上图左上方，已经知道了蓝色圈的数据属于class 1，那么跟他们有相连的数据点属于class 1的概率也会上升。每一笔数据会去影响它的邻居。</p><ul><li>光会影响邻居还不够，因为有连接说明本来就很像，那很像的input<br> ，output本来也就很像。这种方法真正的精髓是，<strong>class是会传递的</strong>，虽然一个点没有与标注数据直接相连，但是有连接路径，那么class<br> 1就会随着边传递。<ul><li>例如上图右上方，所有数据点构建成一个图（理想的例子），然后有一个蓝色点属于class 1，一个红色点属于class 2。经过基于图的方法，蓝色点会传递，红色点也会传递，如上图右下方所示。</li></ul></li></ul></blockquote><p><strong>要让基于图的这种半监督学习方法有用的话，一个重要的原则是你的数据要多，如果数据不够多，例如下图所示，中间有断开，那信息就传递不过去。</strong><img src="https://img-blog.csdnimg.cn/9ce410bae05d47f4b6a06931ef349d7a.png#pic_center" alt=""></p><h2 id="考试一般的考题，定量的计算图"><a href="#考试一般的考题，定量的计算图" class="headerlink" title="考试一般的考题，定量的计算图"></a>考试一般的考题，定量的计算图</h2><p>定量的使用方式是在这个图的结构上面定一个东西，叫做label的平滑度，来表明这个label有多符合平滑度假设。</p><h3 id="怎么定平滑度？"><a href="#怎么定平滑度？" class="headerlink" title="怎么定平滑度？"></a>怎么定平滑度？</h3><p><img src="https://img-blog.csdnimg.cn/42dc067087334546bbf3a44bdb7f9d88.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>看上图两个例子，这两个例子都有4个数据点，数据点之间连接的数字代表了边的权重。现在给两个例子的数据不同的label，左边例子的label是1,1,1,0，右边例子的label是0,1,1,0，那谁更平滑呢？</p><p>直观感觉就是左边例子更平滑，但我们需要定量描述。常见的方法是，考虑两两相连的点（不管有label还是没有label），在所有的配对上，计算label差值的平方，然后乘上权重 ，最后求和。</p><p>所以左边这个例子的S就是0.5，右边例子的S是3，S越小越平滑。</p><h3 id="用矩阵来表达"><a href="#用矩阵来表达" class="headerlink" title="用矩阵来表达"></a>用矩阵来表达</h3><p><img src="https://img-blog.csdnimg.cn/3993d1ab2b6b4e79baeee94790c115f3.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>S可以稍微整理下，写成向量形式如上图。</strong></p><p>把y串成一个向量，y包括标注数据和未标注数据，所以有 R+U 维。</p><p><strong>L是<em>(R+U)×(R+U)</em>的矩阵，叫做图拉普拉斯，L的定义是 D−W ，W是两两数据点之间的权重，D是W每行值之和（放在对角线）。</strong></p><h3 id="再加一个正则化项"><a href="#再加一个正则化项" class="headerlink" title="再加一个正则化项"></a>再加一个正则化项</h3><p>现在可以用 y转置Ly 来评估现在得到的label有多平滑，<strong>式子里面的y是label的值，取决于神经网络的参数</strong>。那么如果要把平滑度考虑到神经网络里时，就是在原来的损失函数里加上λS（λ是某一个想要调的参数）。λS像一个正则化项，在调整参数时，不只是让标注数据的output跟真正的label越近越好，同时还要让output的label在标注数据和未标注数据上符合平滑度假设。平滑度假设由S衡量。</p><p>不一定要在output上计算平滑度，在深度神经网络里，可以把平滑度计算放在网络的任何地方。你可以假设你的output是平滑度，也可以把某个隐藏层乘上一些别的transform，它也要平滑，也可以要求每个隐藏层的output都是平滑的。</p><h3 id="转换的想法-Better-Representation"><a href="#转换的想法-Better-Representation" class="headerlink" title="转换的想法 Better Representation"></a>转换的想法 Better Representation</h3><p>我们观察到的世界其实是比较复杂的，在背后有一些比较简单的向量，比较简单的东西在操控这个复杂的世界。那只要看透假象，直指核心，就可以让学习变得比较容易。</p><p>例如上图右方剪胡子，胡子的变化是很复杂的，但是胡子受头操控，头的变化是有限的。所以胡子是观测，而头就是Better Representation。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;半监督学习 Semi-Supervised&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;总结 半监督学习 的要点：&lt;br&gt;Q1：什么是Semi-Supervised？&lt;br&gt;Q2：Semi-Surpervised在生成模型中如何实现的（EM</summary>
      
    
    
    
    <category term="机器学习基础-李宏毅" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="李宏毅" scheme="http://example.com/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    <category term="半监督学习" scheme="http://example.com/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
