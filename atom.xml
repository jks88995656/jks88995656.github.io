<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只柴犬</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-04-28T17:28:34.511Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>凯凯超人</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python 处理 nii 数据 保存为png图片</title>
    <link href="http://example.com/2022/04/28/python%20%E5%A4%84%E7%90%86%20nii%20%E6%95%B0%E6%8D%AE%20%E4%BF%9D%E5%AD%98%E4%B8%BApng%E5%9B%BE%E7%89%87/"/>
    <id>http://example.com/2022/04/28/python%20%E5%A4%84%E7%90%86%20nii%20%E6%95%B0%E6%8D%AE%20%E4%BF%9D%E5%AD%98%E4%B8%BApng%E5%9B%BE%E7%89%87/</id>
    <published>2022-04-28T07:30:01.000Z</published>
    <updated>2022-04-28T17:28:34.511Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="python 处理 nii 数据">TOC</a></p><p>nii 文件处理代码如下：</p><h4 id="处理代码"><a href="#处理代码" class="headerlink" title="处理代码"></a>处理代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os  <span class="comment"># 遍历文件夹</span></span><br><span class="line"><span class="keyword">import</span> nibabel <span class="keyword">as</span> nib  <span class="comment"># nii格式一般都会用到这个包</span></span><br><span class="line"><span class="keyword">import</span> imageio  <span class="comment"># 转换成图像</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold=np.inf)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nii_to_image</span>(<span class="params">niifile</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">filepath = <span class="string">&#x27;F:\ISIC-2017\img.nii&#x27;</span>  <span class="comment"># 读取本代码同个文件夹下所有的nii格式的文件</span></span><br><span class="line">filenames = os.listdir(filepath)</span><br><span class="line">imgfile = <span class="string">&#x27;./&#x27;</span></span><br><span class="line"></span><br><span class="line">slice_trans = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> filenames:  <span class="comment"># 开始读取nii文件</span></span><br><span class="line">    s = f[-<span class="number">4</span>:]   <span class="comment"># 获取文件的后缀名称</span></span><br><span class="line">    <span class="built_in">print</span>(s)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> s != <span class="string">&#x27;.nii&#x27;</span>:  <span class="comment"># 文件不是 .nii为结尾的就跳过</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    s1 = f[:-<span class="number">4</span>]   <span class="comment"># 读取 .nii 文件的 文件名称  如：img</span></span><br><span class="line">    <span class="built_in">print</span>(s1)</span><br><span class="line">    imgfile_path = imgfile + s1    <span class="comment"># ./img</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;imgfile_path:&quot;</span> + imgfile_path)</span><br><span class="line">    img_path = os.path.join(filepath, f)   <span class="comment"># ./img/img.nii</span></span><br><span class="line">    img = nib.load(img_path)  <span class="comment"># 读取nii</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;img:&quot;</span>)</span><br><span class="line">    <span class="comment"># print(img)   # 里面一大堆数据</span></span><br><span class="line">    img_fdata = img.get_fdata()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;**************&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;**************&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(img)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;**************&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;**************&quot;</span>)</span><br><span class="line">    fname = f.replace(<span class="string">&#x27;.nii&#x27;</span>, <span class="string">&#x27;&#x27;</span>)  <span class="comment"># 去掉nii的后缀名</span></span><br><span class="line">    img_f_path = os.path.join(imgfile, fname)      <span class="comment"># ./img</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(img_f_path):    <span class="comment"># 创建nii对应的图像的文件夹</span></span><br><span class="line">        os.mkdir(img_f_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># #开始转换为图像</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;.gz&#x27;</span> <span class="keyword">in</span> s1:</span><br><span class="line">        (x, y, z, _) = img.shape</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;img2:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(img.shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        (x, y, z) = img.shape     <span class="comment"># 里面没有 .gz的文件  z是图像的序列 一共 89张</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;img3:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(img.shape)   <span class="comment"># 例如：(512, 512, 89)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(z):  <span class="comment"># z是图像的序列</span></span><br><span class="line">        silce = img_fdata[:, :, i]  <span class="comment"># 选择哪个方向的切片都可以</span></span><br><span class="line">        imageio.imwrite(os.path.join(img_f_path, <span class="string">&#x27;&#123;&#125;_mask.png&#x27;</span>.<span class="built_in">format</span>(i)), silce)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(os.path.join(img_f_path, <span class="string">&#x27;&#123;&#125;_mask.png&#x27;</span>.<span class="built_in">format</span>(i)))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;save&quot;</span>)</span><br><span class="line">        img.save(os.path.join(img_f_path, <span class="string">&#x27;&#123;&#125;_mask.png&#x27;</span>.<span class="built_in">format</span>(i)))</span><br></pre></td></tr></table></figure><h4 id="处理结果"><a href="#处理结果" class="headerlink" title="处理结果"></a>处理结果</h4><p>一共89张。<br><img src="https://img-blog.csdnimg.cn/img_convert/4da35b21eee0ca64e9225abeb53f8e88.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;python 处理 nii 数据&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;nii 文件处理代码如下：&lt;/p&gt;
&lt;h4 id=&quot;处理代码&quot;&gt;&lt;a href=&quot;#处理代码&quot; class=&quot;headerlink&quot; title=&quot;处理代码&quot;&gt;&lt;/a&gt;处理代码&lt;/h4&gt;&lt;</summary>
      
    
    
    
    <category term="医学图像" scheme="http://example.com/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"/>
    
    
    <category term="医学图像处理" scheme="http://example.com/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>事件脉络第一次2.25</title>
    <link href="http://example.com/2022/02/25/%E4%BA%8B%E4%BB%B6%E8%84%89%E7%BB%9C%E7%AC%AC%E4%B8%80%E6%AC%A12.25/"/>
    <id>http://example.com/2022/02/25/%E4%BA%8B%E4%BB%B6%E8%84%89%E7%BB%9C%E7%AC%AC%E4%B8%80%E6%AC%A12.25/</id>
    <published>2022-02-25T13:00:00.000Z</published>
    <updated>2022-02-25T12:09:04.233Z</updated>
    
    <content type="html"><![CDATA[<h2 id="事件脉络介绍"><a href="#事件脉络介绍" class="headerlink" title="事件脉络介绍"></a>事件脉络介绍</h2><p>来源：</p><p>当今时代可以说是信息爆炸的时代，大大小小的事情都会引发人们的关注，而百度是我们获取新闻的第一途径，但各个事件往往展现出的信息都非常杂乱，因此就衍生出了百度事件脉络这一板块</p><p>实际：</p><p>通过对特定事件名称的检索，得到该事件的脉络，自动追溯事件的发展过程，根据时间线对热点事件的来龙去脉进行全面了解。</p><p>输入：</p><p>关于某一事件的相关新闻集合，应该是利用文本聚类(搜索等)得到的新闻簇。</p><p>输出：</p><p>事件的脉络信息，换句话说就是：给出一个重要新闻的列表，这些重要的新闻涵盖了该事件的各个重要阶段的重要信息。</p><p>如右边的图 就可以清晰的看到俄乌局势昨天的发展阶段，也就是这个方向应该做到的基本效果。</p><p>如果用于开发的话，真实的接口应该返回如图类似的json数据，包括新闻的时间、标题和链接。</p><h2 id="时间片聚类算法"><a href="#时间片聚类算法" class="headerlink" title="时间片聚类算法"></a>时间片聚类算法</h2><p>其主要的办法是 从<font color="red">新闻的外部属性（主要是分布事件和转载情况）</font>进行分析。</p><p>其前提<font color="blue">假设</font>为：在事件有重要进展的时候，一定会有一些高质量新闻的跟进报道，而且越是重要的进展情况，其报道也会越多越集中。</p><h6 id="如何寻找事件发展的主要阶段？"><a href="#如何寻找事件发展的主要阶段？" class="headerlink" title="如何寻找事件发展的主要阶段？"></a>如何寻找事件发展的主要阶段？</h6><font color="red">使用时间片聚类的方法</font> ，来发现事件发展的主要阶段+ 对每一篇新闻，抽取新闻的发布时间，按照新闻的发布时间将一个新闻事件集合内的新闻进行排列并投影到时间轴上。（根据之前的<font color="blue">假设</font>，时间轴上一定会有一段新闻密集的地方）+ 利用凝聚层次聚类方法，将时间轴分成若干个片段。将新闻聚类而分，这样我们才能明确时间的发展过程。提取每个过程的 确切新闻内容。  + 凝聚层次聚类的基本做法：其不需要指定划分的个数（因为我也不知道到底有几次发展阶段），把每篇新闻看成一个时间片段（其实是一个点），然后**每次合并距离最近的两个片段**，直到任意两个片段之间的距离都大于一个预先设定的阈值。    + 获取到时间发展过程后，则要在每个进展的新闻集合中，抽取一篇代表新闻  + 其具体的策略，可以根据具体的产品需求来定。<font color="blue">一般考虑例如：新闻来源站点是否权威，新闻的发布时间（在同一个聚类内），该新闻是否有更多的转载等等。</font>  + 如果要求更多媒体化，还可以从新闻集合中，抽取出相应的图片，视频等相关资源。#### 存在的问题一：自然时间距离问题？对于新闻来说，由于新闻发布并不是在24小时内均匀分布的，所以我们认为：在新闻发布高峰期间隔1个小时，要比在新闻发布的低谷期间隔一个小时，造成的时间跨度更长。**改进为：按照每半小时为一个小时间片，统计每个时间段内的新闻发布数，求出各个时间段新闻数占所有时间片新闻总数的比值，这个比值可以用来重新分配24小时的时间长度，计算“新闻时间距离”。**这样的效果得到了：在0点至6点之间的1个小时，在“新闻时间距离”中只有半个小时，甚至更少，而在9：00~11：00期间的一个小时，相当于2~3个小时。#### 存在的问题二：新闻集合的去噪各媒体对事件跟进报道的时效性不一，比如同样的新闻内容，新华网的报道更具时效性，在当前20:00就发布了，而一些小的新闻站点，则可能要等到第二天的9:00才发布，这样就导致描述同一阶段的新闻，往往会被分到不同的阶段中去，这样就影响了时间片聚类的效果。<font color="red">所以我们在进行时间片聚类之前，还进行了相似新闻的去重。</font><h6 id="去噪的方法"><a href="#去噪的方法" class="headerlink" title="去噪的方法"></a>去噪的方法</h6><p>对新闻集合内的新闻，进行一次相似度，如果碰到文本相似度很高的新闻，则归档在一起，以最早的那篇新闻为代表新闻，参加时间片聚类。具体的做法是：按照新闻的发布时间由远及近的顺序，计算每篇新闻与之前的新闻的文本相似度，如果相似度太高，则认为可能是重复的内容，则把这篇新闻标记为更早的新闻的转载或者相近报道。</p><h4 id="该方法总结"><a href="#该方法总结" class="headerlink" title="该方法总结"></a>该方法总结</h4><p>本方法利用了新闻媒体对新闻事件的报道行为，来挖掘出新闻事件的发展阶段以及代表新闻，从而给用户提供简单明了的新闻事件脉络。主要利用了时间片聚类算法来自动将事件划分成若干个进展阶段，然后从各个进展阶段中，抽取出代表新闻。为了改善算法，还提出了一种“新闻时间距离”的度量方法；同时，还结合了文本内容分析的手段，来对新闻集合进行精简，去噪，改善脉络抽取的效果。</p><h2 id="基于图结构方法"><a href="#基于图结构方法" class="headerlink" title="基于图结构方法"></a>基于图结构方法</h2><p>通过构建图的方法，将子事件之间的关系转换为图中结点的关系，寻找关键结点，连接关键结点得到最终的事件脉络。</p><h3 id="事件感知"><a href="#事件感知" class="headerlink" title="事件感知"></a>事件感知</h3><p>对微博数据过滤分析后，根据热点事件的关键字搜索数据库，筛选包含该关键字的微博。</p><p>首先对其进行文本预处理，根据中英文使用不同的工具或方法。预处理过程大致可包括，分词，去除停用词。</p><p>而后计算每条微博的 中每个关键词 的 TF-IDF 得分，将所有关键词的 TF-IDF 得分之和作为这条微博的 TF-IDF 得分。 然后进行排序，得分越高的与事件的相关度也就越高，从得到最后处理的库。</p><h3 id="事件脉络呈现"><a href="#事件脉络呈现" class="headerlink" title="事件脉络呈现"></a>事件脉络呈现</h3><p>分为3个子模块，构建图，寻找关键微博，连接关键微博。</p><p>整体思路是，在无向图中寻找关键结点，即关键微博。再在有向图中连接关键结点，最终得到事件脉络。</p><h4 id="如何构建图结构"><a href="#如何构建图结构" class="headerlink" title="如何构建图结构"></a>如何构建图结构</h4><p>需要构建两个图，一个有向图和一个无向图。每个结点的权值，为该结点微博与事件关键字集合Q的余弦相似度。无向图用来表示微博之间文本内容的关系，计算边时，2条微博文本之间的余弦相似度大于 一个阈值时。就用一条无向边将对应的2个结点连接起来。</p><p>有向图用来表示微博之间的时间关系，按时间顺序连接微博结点。</p><h4 id="如何选择关键结点"><a href="#如何选择关键结点" class="headerlink" title="如何选择关键结点"></a>如何选择关键结点</h4><p>如何选择关键结点 就是 如何寻找关键微博</p><p>其采用 加权相似度的方法来寻找 无向图中的关键结点。当 加权相似度越大时，表明这个结点对应的微博更具代表性，能够表示其领接结点对应微博的内容，即这个结点就是 图中的一个关键结点。</p><p>计算无向图中所有结点的加权相似度，选取其中加权相似度最大的结点作为一个关键结点。</p><p>之后采用迭代的过程，找到关键结点集合。</p><h4 id="如何链接关键结点"><a href="#如何链接关键结点" class="headerlink" title="如何链接关键结点"></a>如何链接关键结点</h4><p>在有向图中，对于任意2个关键结点，如果本来就是邻接结点，则直接连接。</p><p>如果不是的话，就需要添加过渡结点。其应该满足添加的过渡结点的加权相似度之和达到最小，</p><p>如右边的公式，</p><p>最后得到连接关键结点的边集合，即关键结点的连接结果。</p><p>给定一个过渡结点，可以体现事件一定程度的多样化，使得内容看起来更加的连贯。</p><h3 id="实验数据结果"><a href="#实验数据结果" class="headerlink" title="实验数据结果"></a>实验数据结果</h3><p>该论文爬取的数据集是 2014年 巴西世界杯的 推特数据，事件为7月14日 阿根廷与德国的实时球赛事件推特记录。大致能说清楚事件发展情况，并且存在一定的趣味事件。</p><p>但这里的趣味事件和整个事件发展，关系不大。  </p><p>并且这个数据由于本身太过于口语化，所以用户情感比较明显，对整体的事件脉络是有干扰的。</p><h3 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h3><p>这个方法在事件感知，对微博的处理方法过于单一，仅仅依靠微博中是否出现关键字来判断，过于片面。可能会漏掉一些有潜在关系的数据。</p><p>另外图求解关键结点和连接结点的复杂度理论上非常的高，实时性应该是比较差的。</p><p>另外这里只考虑到了文本属性，其他一些值的参考的评论、微博博主的权威值等其实都可以引入。</p><p>甚至是多模态的多媒体也值的参考。</p><h3 id="后续的一些问题"><a href="#后续的一些问题" class="headerlink" title="后续的一些问题"></a>后续的一些问题</h3><p>第一 如何评价事件脉络实验结果好坏？</p><p>之前看的有一篇中文文档，他是采用召回率和准确率来定量的描述。 他根据正确人工拟定的事件脉络，统计他的时间日期个数和 算法命中的日期个数 比值为召回率；同理也可以计算出准确率。</p><p>另外也有采用人调查评价是否能明白事件经过的打分情况，主观成分较大。</p><p>第二 就是这个方向的整体思路不够清晰</p><p>有之前提到的 时间片聚类的算法，构造图算法</p><p>也有后续的多模考虑的 基于关键字，基于概率，基于主题的方法。</p><p>主体上是偏向于机器学习，图论，数据挖掘。</p><p>第三就是 新闻数据集的获取问题。</p><p>做新闻类的话，可能需要爬取一些权威性较高的平台。</p><p>另外 存在多媒体，新浪微博、百度新闻等 的所谓的数据流特征不太类似，不确定使用同种方法是不是都试用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;事件脉络介绍&quot;&gt;&lt;a href=&quot;#事件脉络介绍&quot; class=&quot;headerlink&quot; title=&quot;事件脉络介绍&quot;&gt;&lt;/a&gt;事件脉络介绍&lt;/h2&gt;&lt;p&gt;来源：&lt;/p&gt;
&lt;p&gt;当今时代可以说是信息爆炸的时代，大大小小的事情都会引发人们的关注，而百度是我们获取新闻</summary>
      
    
    
    
    <category term="研究课题-事件脉络" scheme="http://example.com/categories/%E7%A0%94%E7%A9%B6%E8%AF%BE%E9%A2%98-%E4%BA%8B%E4%BB%B6%E8%84%89%E7%BB%9C/"/>
    
    
    <category term="事件脉络" scheme="http://example.com/tags/%E4%BA%8B%E4%BB%B6%E8%84%89%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>ResNet迁移学习 之 花图像分类</title>
    <link href="http://example.com/2021/11/23/ResNet%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%20%E4%B9%8B%20%E8%8A%B1%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    <id>http://example.com/2021/11/23/ResNet%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%20%E4%B9%8B%20%E8%8A%B1%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/</id>
    <published>2021-11-23T15:52:01.000Z</published>
    <updated>2021-11-26T13:25:48.163Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="ResNet迁移学习 之 花图像分类">TOC</a><br>迁移学习 (Transfer Learning) 是把已学训练好的模型参数用作新训练模型的起始参数. 迁移学习是深度学习中非常重要和常用的一个策略.</p><h1 id="为什么使用迁移学习"><a href="#为什么使用迁移学习" class="headerlink" title="为什么使用迁移学习"></a>为什么使用迁移学习</h1><p>迁移学习 (Transfer Learning) 可以帮助我们得到更好的结果.</p><ol><li>当我们手上的数据比较少的时候,<br>训练非常容易造成过拟合的现象。使用迁移学习可以帮助我们通过更少的训练数据达到更好的效果。使得模型的泛化能力更强, 训练过程更稳定。</li><li>迁移学习 (Transfer Learning) 可以帮助我们节省时间。通过迁移学习，利用前人花大量时间训练好的参数，能帮助我们在模型的训练上节省大把的时间。</li></ol><h2 id="常见的迁移学习-backbone"><a href="#常见的迁移学习-backbone" class="headerlink" title="常见的迁移学习 backbone"></a>常见的迁移学习 backbone</h2><p><a href="https://pytorch.org/vision/stable/models.html">Pytorch 迁移学习官网 API</a></p><ul><li>VGG   </li><li>ResNet </li><li>SqueezeNet </li><li>DenseNet </li><li>Inception  <a href="https://jks88995656.github.io/2021/09/21/Pytorch%20GoogleNet%E4%B8%AD%E7%9A%84Inception/">GoogleNet 中的 Inception结构</a></li><li>GoogLeNet </li><li>ShuffleNet</li><li>MobileNet</li></ul><h1 id="冻层实现"><a href="#冻层实现" class="headerlink" title="冻层实现"></a>冻层实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_parameter_requires_grad</span>(<span class="params">model, feature_extracting</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    是否保留梯度, 实现冻层  保留梯度也就是冻结层</span></span><br><span class="line"><span class="string">    :param model:模型</span></span><br><span class="line"><span class="string">    :param feature_extracting:是否冻层</span></span><br><span class="line"><span class="string">    :return:无返回值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> feature_extracting:  <span class="comment"># 如果冻层</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():  <span class="comment"># 遍历每个权重参数</span></span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line">            <span class="comment"># param.requires_grad = False的作用是:</span></span><br><span class="line">            <span class="comment"># 屏蔽预训练模型的权重。</span></span><br></pre></td></tr></table></figure><h1 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h1><p>ResNet模型的初始化，是冻结全连接层之前所有的层参数，并将全连接层重写为符合自己数据集的输出。例如：花图像数据集一共有102个类，所以最后的输出为102个。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_model</span>(<span class="params">model_name, num_classes, feature_extract, use_pretrained_state=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    初始化模型</span></span><br><span class="line"><span class="string">    :param model_name: 模型名字</span></span><br><span class="line"><span class="string">    :param num_classes:  类别数</span></span><br><span class="line"><span class="string">    :param feature_extract: 是否部冻层</span></span><br><span class="line"><span class="string">    :param use_pretrained_state: 是否下载模型  为True的话为自动下载加载到模型内 为False的话就自己加载模型</span></span><br><span class="line"><span class="string">    :return: 返回模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model_ft = <span class="literal">None</span></span><br><span class="line">    input_size = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> model_name == <span class="string">&quot;resnet&quot;</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">            原来Resnet152模型结构中最后两层为：</span></span><br><span class="line"><span class="string">                (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))</span></span><br><span class="line"><span class="string">                (fc): Linear(in_features=2048, out_features=1000, bias=True)  </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        model_ft = models.resnet152(pretrained=use_pretrained_state)  <span class="comment"># 下载参数 False就不下载 需要下面手动加</span></span><br><span class="line">        model_ft.load_state_dict(torch.load(<span class="string">&#x27;./dataset/pth/resnet152-b121ed2d.pth&#x27;</span>))  <span class="comment"># 手动加 模型参数</span></span><br><span class="line">        set_parameter_requires_grad(model_ft, feature_extract)  <span class="comment"># 冻层</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 修改全连接层</span></span><br><span class="line">        num_ftrs = model_ft.fc.in_features  <span class="comment"># 获取resnet最后全连接层输入的维度 2048，默认解冻</span></span><br><span class="line">        model_ft.fc = nn.Sequential(nn.Linear(num_ftrs, num_classes),</span><br><span class="line">                                    nn.LogSoftmax(dim=<span class="number">1</span>))</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">            经过修改线性层输出的维度之后变为：</span></span><br><span class="line"><span class="string">                (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))</span></span><br><span class="line"><span class="string">                (fc): Sequential(</span></span><br><span class="line"><span class="string">                    (0): Linear(in_features=2048, out_features=102, bias=True)</span></span><br><span class="line"><span class="string">                    (1): LogSoftmax()</span></span><br><span class="line"><span class="string">                )</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        input_size = <span class="number">224</span></span><br><span class="line">        <span class="built_in">print</span>(model_ft)</span><br><span class="line">    <span class="keyword">return</span> model_ft, input_size</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h1 id="获取需更新参数"><a href="#获取需更新参数" class="headerlink" title="获取需更新参数"></a>获取需更新参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parameter_to_update</span>(<span class="params">model</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    获取需要更新的参数</span></span><br><span class="line"><span class="string">    :param model: 模型</span></span><br><span class="line"><span class="string">    :return: 需要更新的参数列表</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 是否训练所有层</span></span><br><span class="line">    params_to_update = model.parameters()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Params to learn:&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> feature_extract:</span><br><span class="line">        params_to_update = []</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                params_to_update.append(param)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\t&quot;</span>, name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\t&quot;</span>, name)</span><br><span class="line">    <span class="keyword">return</span> params_to_update</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p>里面有一些API 不明白</p><ul><li><a href="https://www.jianshu.com/p/3ed11362b54f">torch.max(outputs, 1)</a> </li><li><a href="https://www.cnblogs.com/liujianing/p/13428387.html">optimizer.state_dict()</a></li><li><a href="https://www.jianshu.com/p/60fc57e19615">model.load_state_dict(best_model_wts)</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">model, dataloaders, criterion, optimizer, filename, num_epochs</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    训练模型</span></span><br><span class="line"><span class="string">    :param model: 引入的权重模型</span></span><br><span class="line"><span class="string">    :param dataloaders: 导入数据 格式为dataloaders</span></span><br><span class="line"><span class="string">    :param criterion: 损失函数</span></span><br><span class="line"><span class="string">    :param optimizer: 优化器</span></span><br><span class="line"><span class="string">    :param filename: 模型名称（地址）</span></span><br><span class="line"><span class="string">    :param num_epochs: epoch数</span></span><br><span class="line"><span class="string">    :return:model, val_acc_history, train_acc_history, valid_losses, train_losses, LRs</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 获取起始时间</span></span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于GPU</span></span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    best_acc = <span class="number">0</span></span><br><span class="line">    val_acc_history = []</span><br><span class="line">    train_acc_history = []</span><br><span class="line">    train_loss = []</span><br><span class="line">    valid_loss = []</span><br><span class="line">    LRs = [optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]]  <span class="comment">## ？？？</span></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())  <span class="comment"># 保存最好的模型权重参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_epochs)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch &#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练和验证</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                model.train()  <span class="comment"># 训练</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.<span class="built_in">eval</span>()  <span class="comment"># 验证</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 遍历数据</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)  <span class="comment"># inputs shape : torch.Size([16, 3, 224, 224])</span></span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 梯度清零</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 只有训练的时候计算和更新梯度</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">                    outputs = model(inputs)  <span class="comment"># torch.Size([16, 102])</span></span><br><span class="line">                    <span class="comment"># https://www.jianshu.com/p/3ed11362b54f</span></span><br><span class="line">                    _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)  <span class="comment"># dim是max函数索引的维度0/1，0是每列的最大值，1是每行的最大值</span></span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;preds&quot;</span>, preds)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 计算损失</span></span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 训练阶段更新权重</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 计算损失</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)  <span class="comment"># size 表示 inputs 的形状的 第一个也就是 batch 16</span></span><br><span class="line">                running_corrects += torch.<span class="built_in">sum</span>(preds == labels.data)  <span class="comment"># 预测和实际标签一样就算上</span></span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / <span class="built_in">len</span>(dataloaders[phase].dataset)  <span class="comment"># 这个loss的定义 ？？？</span></span><br><span class="line">            epoch_acc = running_corrects.double() / <span class="built_in">len</span>(</span><br><span class="line">                dataloaders[phase].dataset)  <span class="comment"># ???  dataloaders[phase].dataset  为什么有dataset属性 是啥呢</span></span><br><span class="line"></span><br><span class="line">            time_spend = time.time() - start  <span class="comment"># 所花费的时间 start是初始开始时间</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Time spend &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;</span>.<span class="built_in">format</span>(time_spend // <span class="number">60</span>, time_spend % <span class="number">60</span>))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 得到最好那次的模型</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;valid&#x27;</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())  <span class="comment"># copy当前最好模型的所有权重</span></span><br><span class="line">                state = &#123;</span><br><span class="line">                    <span class="string">&#x27;state_dict&#x27;</span>: model.state_dict(),  <span class="comment"># 当前最好模型的所有权重</span></span><br><span class="line">                    <span class="string">&#x27;best_acc&#x27;</span>: best_acc,  <span class="comment"># 最好的准确率 （测试集上）</span></span><br><span class="line">                    <span class="string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(),  <span class="comment"># https://www.cnblogs.com/liujianing/p/13428387.html</span></span><br><span class="line">                &#125;</span><br><span class="line">                torch.save(state, filename)  <span class="comment"># filename 保存的模型名（其实是地址）</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;valid&#x27;</span>:</span><br><span class="line">                val_acc_history.append(epoch_acc)</span><br><span class="line">                valid_loss.append(epoch_loss)</span><br><span class="line">                scheduler.step(epoch_loss)  <span class="comment"># ??? 更新权重参数</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                train_acc_history.append(epoch_acc)</span><br><span class="line">                train_loss.append(epoch_loss)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Optimizer learning rate : &#123;:.7f&#125;&#x27;</span>.<span class="built_in">format</span>(optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]))</span><br><span class="line">        LRs.append(optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>])  <span class="comment"># 添加此时的学习率</span></span><br><span class="line"></span><br><span class="line">    time_spend_all = time.time() - start  <span class="comment"># 所花费的时间 start是初始开始时间</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;</span>.<span class="built_in">format</span>(time_spend_all // <span class="number">60</span>, time_spend_all % <span class="number">60</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Best val Acc: &#123;:4f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练完后用最好的一次当做模型最终的结果</span></span><br><span class="line">    model.load_state_dict(best_model_wts)  <span class="comment"># model加载 可以查看 https://www.jianshu.com/p/60fc57e19615</span></span><br><span class="line">    <span class="comment"># 返回</span></span><br><span class="line">    <span class="keyword">return</span> model, val_acc_history, train_acc_history, valid_loss, train_loss, LRs</span><br></pre></td></tr></table></figure><h2 id="如果是仅仅微调所有参数的话"><a href="#如果是仅仅微调所有参数的话" class="headerlink" title="如果是仅仅微调所有参数的话"></a>如果是仅仅微调所有参数的话</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有全体参数的情况下  微调训练全部层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_all_layers</span>(<span class="params">num_epochs</span>):</span></span><br><span class="line">    <span class="comment"># 保存文件的名字</span></span><br><span class="line">    file_path = <span class="string">&#x27;./dataset/pth/resnet152_fc.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    checkpoint = torch.load(file_path)</span><br><span class="line">    best_acc = checkpoint[<span class="string">&#x27;best_acc&#x27;</span>]</span><br><span class="line">    model_ft = models.resnet152(pretrained=<span class="literal">False</span>)</span><br><span class="line">    model_ft.load_state_dict(checkpoint[<span class="string">&#x27;state_dict&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有参数的requires_grad 设为True 微调 训练所有层</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model_ft.parameters():</span><br><span class="line">        param.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(model_ft.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">6</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">    optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs = train_model(model_ft, dataloaders,</span><br><span class="line">                                                                                                criterion, optimizer,</span><br><span class="line">                                                                                                num_epoch=num_epochs,</span><br><span class="line">                                                                                                filename=<span class="string">&#x27;resnet152_all_layers.pth&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="用于最后模型测试"><a href="#用于最后模型测试" class="headerlink" title="用于最后模型测试"></a>用于最后模型测试</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fortest</span>():</span></span><br><span class="line">    <span class="comment"># 保存文件的名字</span></span><br><span class="line">    file_path = <span class="string">&#x27;./dataset/pth/resnet152_fc.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    model_ft = torch.load(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># GPU模式</span></span><br><span class="line">    model_ft = model_ft.to(device)</span><br><span class="line">    <span class="comment"># 得到一个batch的测试数据</span></span><br><span class="line">    dataiter = <span class="built_in">iter</span>(dataloaders[<span class="string">&#x27;valid&#x27;</span>])</span><br><span class="line">    images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line">    model_ft.<span class="built_in">eval</span>()</span><br><span class="line">    train_on_gpu = torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">if</span> train_on_gpu:</span><br><span class="line">        output = model_ft(images.cuda())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = model_ft(images)</span><br><span class="line"></span><br><span class="line">    _, preds_tensor = torch.<span class="built_in">max</span>(output, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    preds = np.squeeze(preds_tensor.numpy()) <span class="keyword">if</span> <span class="keyword">not</span> train_on_gpu <span class="keyword">else</span> np.squeeze(preds_tensor.cpu().numpy())</span><br><span class="line">    <span class="built_in">print</span>(preds)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结果展示</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">    columns = <span class="number">4</span></span><br><span class="line">    rows = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(columns * rows):</span><br><span class="line">        ax = fig.add_subplot(rows, columns, idx + <span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">        plt.imshow(im_convert(images[idx]))</span><br><span class="line">        ax.set_title(<span class="string">&quot;&#123;&#125; (&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(cat_to_name[<span class="built_in">str</span>(preds[idx])], cat_to_name[<span class="built_in">str</span>(labels[idx].item())]),</span><br><span class="line">                     color=(<span class="string">&quot;green&quot;</span> <span class="keyword">if</span> cat_to_name[<span class="built_in">str</span>(preds[idx])] == cat_to_name[<span class="built_in">str</span>(labels[idx].item())] <span class="keyword">else</span> <span class="string">&quot;red&quot;</span>))</span><br><span class="line">    plt.savefig(<span class="string">&#x27;./img_show.png&#x27;</span>)</span><br><span class="line">    plt.close(fig)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h1><p>一般这个部分 不同数据集是不一样的。<br>但我们需要的 数据一定是 一个 dataloader （同时其一般为 字典格式）例如： <strong>data_loader = {“train”: train_loader, “valid”: test_loader}</strong></p><p>如上面所示，所以我们一般数据集是划分为 训练集、验证集、测试集的。（但训练一般前两者即可）。</p><p>在花朵任务内：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line">image_datasets, dataloaders, dataset_sizes, class_names = create_dataset(batch_size=batch_size)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>定义数据预处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片路径</span></span><br><span class="line">data_dir = <span class="string">&#x27;./dataset/flower_data/&#x27;</span></span><br><span class="line">train_dir = data_dir + <span class="string">&#x27;/train&#x27;</span></span><br><span class="line">valid_dir = data_dir + <span class="string">&#x27;/valid&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取标签对应的实际名字</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./dataset/flower_data/cat_to_name.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    cat_to_name = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">&#x27;train&#x27;</span>: transforms.Compose([transforms.RandomRotation(<span class="number">45</span>),  <span class="comment"># 随机旋转，-45到45度之间随机选</span></span><br><span class="line">                                 transforms.CenterCrop(<span class="number">224</span>),  <span class="comment"># 从中心开始裁剪</span></span><br><span class="line">                                 transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),  <span class="comment"># 随机水平翻转 选择一个概率概率</span></span><br><span class="line">                                 transforms.RandomVerticalFlip(p=<span class="number">0.5</span>),  <span class="comment"># 随机垂直翻转</span></span><br><span class="line">                                 transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.1</span>, saturation=<span class="number">0.1</span>, hue=<span class="number">0.1</span>),</span><br><span class="line">                                 <span class="comment"># 参数1为亮度，参数2为对比度，参数3为饱和度，参数4为色相</span></span><br><span class="line">                                 transforms.RandomGrayscale(p=<span class="number">0.025</span>),  <span class="comment"># 概率转换成灰度率，3通道就是R=G=B</span></span><br><span class="line">                                 transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 均值，标准差</span></span><br><span class="line">                                 ]),</span><br><span class="line">    <span class="string">&#x27;valid&#x27;</span>: transforms.Compose([transforms.Resize(<span class="number">256</span>),</span><br><span class="line">                                 transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                 transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">                                 ]),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>create_dataset方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 数据集创建初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span>(<span class="params">batch_size</span>):</span></span><br><span class="line">    <span class="comment"># 数据创建初始化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这就类似于一个字典  &#123;&#x27;train&#x27;:./dataset/flower_data/train....,&#x27;valid&#x27;:......&#125;</span></span><br><span class="line">    image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) <span class="keyword">for</span> x <span class="keyword">in</span></span><br><span class="line">                      [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看一下 image_datasets 的内容</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;image_datasets&quot;</span>, image_datasets)</span><br><span class="line">    <span class="comment"># torch.utils.data.DataLoader使用方法</span></span><br><span class="line">    <span class="comment"># 数据加载器，结合了数据集和取样器，并且可以提供多个线程处理数据集。</span></span><br><span class="line">    <span class="comment"># 在训练模型时使用到此函数，用来把训练数据分成多个小组，此函数每次抛出一组数据。直至把所有的数据都抛出。就是做一个数据的初始化。</span></span><br><span class="line">    <span class="comment"># 这里同样是生成一个字典格式</span></span><br><span class="line">    dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=<span class="literal">True</span>) <span class="keyword">for</span> x <span class="keyword">in</span></span><br><span class="line">                   [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看一下dataloaders 的内容</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dataloaders&quot;</span>, dataloaders)</span><br><span class="line">    <span class="comment"># 字典形式</span></span><br><span class="line">    dataset_sizes = &#123;x: <span class="built_in">len</span>(image_datasets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]&#125;</span><br><span class="line">    <span class="comment"># 查看一下 dataset_sizes 的内容</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dataset_sizes&quot;</span>, dataset_sizes)</span><br><span class="line">    <span class="comment"># 返回一个list类型</span></span><br><span class="line">    class_names = image_datasets[<span class="string">&#x27;train&#x27;</span>].classes</span><br><span class="line">    <span class="keyword">return</span> image_datasets, dataloaders, dataset_sizes, class_names</span><br></pre></td></tr></table></figure><h1 id="最终Main调用顺序"><a href="#最终Main调用顺序" class="headerlink" title="最终Main调用顺序"></a>最终Main调用顺序</h1><h2 id="配置图片路径"><a href="#配置图片路径" class="headerlink" title="配置图片路径"></a>配置图片路径</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片路径</span></span><br><span class="line">data_dir = <span class="string">&#x27;./dataset/flower_data/&#x27;</span></span><br><span class="line">train_dir = data_dir + <span class="string">&#x27;/train&#x27;</span></span><br><span class="line">valid_dir = data_dir + <span class="string">&#x27;/valid&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取标签对应的实际名字</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./dataset/flower_data/cat_to_name.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    cat_to_name = json.load(f)</span><br></pre></td></tr></table></figure><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>划分训练集、验证集、测试集等。并做图像增强。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">&#x27;train&#x27;</span>: transforms.Compose([transforms.RandomRotation(<span class="number">45</span>),  <span class="comment"># 随机旋转，-45到45度之间随机选</span></span><br><span class="line">                                 transforms.CenterCrop(<span class="number">224</span>),  <span class="comment"># 从中心开始裁剪</span></span><br><span class="line">                                 transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),  <span class="comment"># 随机水平翻转 选择一个概率概率</span></span><br><span class="line">                                 transforms.RandomVerticalFlip(p=<span class="number">0.5</span>),  <span class="comment"># 随机垂直翻转</span></span><br><span class="line">                                 transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.1</span>, saturation=<span class="number">0.1</span>, hue=<span class="number">0.1</span>),</span><br><span class="line">                                 <span class="comment"># 参数1为亮度，参数2为对比度，参数3为饱和度，参数4为色相</span></span><br><span class="line">                                 transforms.RandomGrayscale(p=<span class="number">0.025</span>),  <span class="comment"># 概率转换成灰度率，3通道就是R=G=B</span></span><br><span class="line">                                 transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 均值，标准差</span></span><br><span class="line">                                 ]),</span><br><span class="line">    <span class="string">&#x27;valid&#x27;</span>: transforms.Compose([transforms.Resize(<span class="number">256</span>),</span><br><span class="line">                                 transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                 transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">                                 ]),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="配置超参数"><a href="#配置超参数" class="headerlink" title="配置超参数"></a>配置超参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参数配置</span></span><br><span class="line"><span class="comment"># 是否用人家训练好的特征来做,True 为 不改变权重参数也就是冻层，False 为自己重新训练所有层的权重参数</span></span><br><span class="line">feature_extract = <span class="literal">True</span>  <span class="comment"># 冻层</span></span><br><span class="line">num_classes = <span class="number">102</span>  <span class="comment"># 输出的类别数</span></span><br><span class="line">batch_size = <span class="number">16</span>  <span class="comment"># 一次训练的样本数目</span></span><br><span class="line">num_epochs = <span class="number">20</span></span><br></pre></td></tr></table></figure><h2 id="创建数据-DataLoader"><a href="#创建数据-DataLoader" class="headerlink" title="创建数据 DataLoader"></a>创建数据 DataLoader</h2><p>一般这都是一个整体方法。这里 分来开写了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line">image_datasets, dataloaders, dataset_sizes, class_names = create_dataset(batch_size=batch_size)</span><br></pre></td></tr></table></figure><h2 id="获取初始化ResNet模型"><a href="#获取初始化ResNet模型" class="headerlink" title="获取初始化ResNet模型"></a>获取初始化ResNet模型</h2><p>冻结了最后一层全连接层外的所有层参数，并对最后一层线性层修改为自己的数据集样本个数输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 获取模型  模型冻结了部分层，最后一层线性层修改</span></span><br><span class="line">resnet152 = initialize_model(</span><br><span class="line">    model_name=<span class="string">&#x27;resnet&#x27;</span>,</span><br><span class="line">    num_classes=num_classes,</span><br><span class="line">    feature_extract=feature_extract,</span><br><span class="line">    use_pretrained=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="是否使用GPU训练"><a href="#是否使用GPU训练" class="headerlink" title="是否使用GPU训练"></a>是否使用GPU训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否使用GPU训练</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> torch.cuda.is_available():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;CUDA is not available.  Training on CPU ...&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;CUDA is available!  Training on GPU ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="模型训练属性配置"><a href="#模型训练属性配置" class="headerlink" title="模型训练属性配置"></a>模型训练属性配置</h2><h3 id="哪些参数需要被训练"><a href="#哪些参数需要被训练" class="headerlink" title="哪些参数需要被训练"></a>哪些参数需要被训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练参数</span></span><br><span class="line">params_to_update = parameter_to_update(resnet152)</span><br></pre></td></tr></table></figure><h3 id="优化器、衰减器、损失函数"><a href="#优化器、衰减器、损失函数" class="headerlink" title="优化器、衰减器、损失函数"></a>优化器、衰减器、损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 优化器设置</span></span><br><span class="line">optimizer = torch.optim.Adam(params_to_update, lr=<span class="number">0.01</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">6</span>, gamma=<span class="number">0.1</span>)  <span class="comment"># 学习率每7个epoch衰减成原来的1/10</span></span><br><span class="line"><span class="comment"># 最后一层已经LogSoftmax()了，所以不能nn.CrossEntropyLoss()来计算了，nn.CrossEntropyLoss()相当于logSoftmax()和nn.NLLLoss()整合</span></span><br><span class="line">criterion = nn.NLLLoss()</span><br></pre></td></tr></table></figure><h2 id="开始ResNet预训练（最后一层线性训练）"><a href="#开始ResNet预训练（最后一层线性训练）" class="headerlink" title="开始ResNet预训练（最后一层线性训练）"></a>开始ResNet预训练（最后一层线性训练）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # 开始训练</span></span><br><span class="line">model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs = train_model(resnet152, dataloaders,</span><br><span class="line">                                                                                            criterion, optimizer,</span><br><span class="line">                                                                                            num_epoch=num_epochs,</span><br><span class="line">                                                                                            filename=<span class="string">&quot;resnet152_fc.pth&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="微调所有ResNet参数训练"><a href="#微调所有ResNet参数训练" class="headerlink" title="微调所有ResNet参数训练"></a>微调所有ResNet参数训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_all_layers(num_epochs)</span><br></pre></td></tr></table></figure><h2 id="评估模型结果"><a href="#评估模型结果" class="headerlink" title="评估模型结果"></a>评估模型结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fortest()</span><br></pre></td></tr></table></figure><h1 id="整体代码"><a href="#整体代码" class="headerlink" title="整体代码"></a>整体代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, models, datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">im_convert</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="comment"># 展示数据</span></span><br><span class="line">    image = tensor.to(<span class="string">&quot;cpu&quot;</span>).clone().detach()  <span class="comment"># 方法解读 https://zhuanlan.zhihu.com/p/148061684</span></span><br><span class="line">    <span class="comment"># 结论：根据上述例1~3可知，np.squeeze（）函数可以删除数组形状中的单维度条目，</span></span><br><span class="line">    <span class="comment"># 即把shape中为1的维度去掉，但是对非单维的维度不起作用。</span></span><br><span class="line">    image = image.numpy().squeeze()  <span class="comment"># 方法解读 https://blog.csdn.net/qq_38675570/article/details/80048650</span></span><br><span class="line">    <span class="comment"># 这里用np.transpose（img，(1,2,0)）</span></span><br><span class="line">    <span class="comment"># 将图片的格式由（channels,imagesize,imagesize）转化为（imagesize,imagesize,channels）,这样plt.show()就可以显示图片了。</span></span><br><span class="line">    <span class="comment"># pytorch中 最开始的应该是 维度</span></span><br><span class="line">    image = image.transpose(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 乘以标准差 加上 均值 就是 原来的图片样本值</span></span><br><span class="line">    image = image * np.array((<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)) + np.array((<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>))</span><br><span class="line">    <span class="comment"># ？？？</span></span><br><span class="line">    image = image.clip(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图片效果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_image</span>():</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">20</span>, <span class="number">12</span>))</span><br><span class="line">    columns = <span class="number">4</span></span><br><span class="line">    rows = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为什么加了一个迭代器就这样了</span></span><br><span class="line">    dataiter = <span class="built_in">iter</span>(dataloaders[<span class="string">&#x27;valid&#x27;</span>])</span><br><span class="line">    inputs, classes = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(columns * rows):</span><br><span class="line">        ax = fig.add_subplot(rows, columns, idx + <span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">        ax.set_title(cat_to_name[<span class="built_in">str</span>(<span class="built_in">int</span>(class_names[classes[idx]]))])  <span class="comment"># 为什么还要 先转int类型？？？</span></span><br><span class="line">        plt.imshow(im_convert(inputs[idx]))  <span class="comment"># 图片转换显示</span></span><br><span class="line">    plt.show()  <span class="comment"># 显示整个图</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 原版的数据获取 mnt</span></span><br><span class="line"><span class="comment"># def get_data():</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;获取数据&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     # 获取测试集</span></span><br><span class="line"><span class="comment">#     train = torchvision.datasets.CIFAR100(root=&quot;./mnt&quot;, train=True, download=True,</span></span><br><span class="line"><span class="comment">#                                           transform=torchvision.transforms.Compose([</span></span><br><span class="line"><span class="comment">#                                               torchvision.transforms.ToTensor(),  # 转换成张量</span></span><br><span class="line"><span class="comment">#                                               torchvision.transforms.Normalize((0.1307,), (0.3081,))  # 标准化</span></span><br><span class="line"><span class="comment">#                                           ]))</span></span><br><span class="line"><span class="comment">#     train_loader = DataLoader(train, batch_size=batch_size)  # 分割测试集</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     # 获取测试集</span></span><br><span class="line"><span class="comment">#     test = torchvision.datasets.CIFAR100(root=&quot;./mnt&quot;, train=False, download=True,</span></span><br><span class="line"><span class="comment">#                                          transform=torchvision.transforms.Compose([</span></span><br><span class="line"><span class="comment">#                                              torchvision.transforms.ToTensor(),  # 转换成张量</span></span><br><span class="line"><span class="comment">#                                              torchvision.transforms.Normalize((0.1307,), (0.3081,))  # 标准化</span></span><br><span class="line"><span class="comment">#                                          ]))</span></span><br><span class="line"><span class="comment">#     test_loader = DataLoader(test, batch_size=batch_size)  # 分割训练</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     data_loader = &#123;&quot;train&quot;: train_loader, &quot;valid&quot;: test_loader&#125;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     # 返回分割好的训练集和测试集</span></span><br><span class="line"><span class="comment">#     return data_loader</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 数据集创建初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span>(<span class="params">batch_size</span>):</span></span><br><span class="line">    <span class="comment"># 数据创建初始化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这就类似于一个字典  &#123;&#x27;train&#x27;:./dataset/flower_data/train....,&#x27;valid&#x27;:......&#125;</span></span><br><span class="line">    image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) <span class="keyword">for</span> x <span class="keyword">in</span></span><br><span class="line">                      [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看一下 image_datasets 的内容</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;image_datasets&quot;</span>, image_datasets)</span><br><span class="line">    <span class="comment"># torch.utils.data.DataLoader使用方法</span></span><br><span class="line">    <span class="comment"># 数据加载器，结合了数据集和取样器，并且可以提供多个线程处理数据集。</span></span><br><span class="line">    <span class="comment"># 在训练模型时使用到此函数，用来把训练数据分成多个小组，此函数每次抛出一组数据。直至把所有的数据都抛出。就是做一个数据的初始化。</span></span><br><span class="line">    <span class="comment"># 这里同样是生成一个字典格式</span></span><br><span class="line">    dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=<span class="literal">True</span>) <span class="keyword">for</span> x <span class="keyword">in</span></span><br><span class="line">                   [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看一下dataloaders 的内容</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dataloaders&quot;</span>, dataloaders)</span><br><span class="line">    <span class="comment"># 字典形式</span></span><br><span class="line">    dataset_sizes = &#123;x: <span class="built_in">len</span>(image_datasets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]&#125;</span><br><span class="line">    <span class="comment"># 查看一下 dataset_sizes 的内容</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dataset_sizes&quot;</span>, dataset_sizes)</span><br><span class="line">    <span class="comment"># 返回一个list类型</span></span><br><span class="line">    class_names = image_datasets[<span class="string">&#x27;train&#x27;</span>].classes</span><br><span class="line">    <span class="keyword">return</span> image_datasets, dataloaders, dataset_sizes, class_names</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_parameter_requires_grad</span>(<span class="params">model, feature_extracting</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    是否保留梯度, 实现冻层  保留梯度也就是冻结层</span></span><br><span class="line"><span class="string">    :param model:模型</span></span><br><span class="line"><span class="string">    :param feature_extracting:是否冻层</span></span><br><span class="line"><span class="string">    :return:无返回值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> feature_extracting:  <span class="comment"># 如果冻层</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():  <span class="comment"># 遍历每个权重参数</span></span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line">            <span class="comment"># param.requires_grad = False的作用是:</span></span><br><span class="line">            <span class="comment"># 屏蔽预训练模型的权重。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_model</span>(<span class="params">model_name, num_classes, feature_extract, use_pretrained_state=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    初始化模型</span></span><br><span class="line"><span class="string">    :param model_name: 模型名字</span></span><br><span class="line"><span class="string">    :param num_classes:  类别数</span></span><br><span class="line"><span class="string">    :param feature_extract: 是否部冻层</span></span><br><span class="line"><span class="string">    :param use_pretrained_state: 是否下载模型  为True的话为自动下载加载到模型内 为False的话就自己加载模型</span></span><br><span class="line"><span class="string">    :return: 返回模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model_ft = <span class="literal">None</span></span><br><span class="line">    input_size = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> model_name == <span class="string">&quot;resnet&quot;</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">            原来Resnet152模型结构中最后两层为：</span></span><br><span class="line"><span class="string">                (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))</span></span><br><span class="line"><span class="string">                (fc): Linear(in_features=2048, out_features=1000, bias=True)  </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        model_ft = models.resnet152(pretrained=use_pretrained_state)  <span class="comment"># 下载参数 False就不下载 需要下面手动加</span></span><br><span class="line">        model_ft.load_state_dict(torch.load(<span class="string">&#x27;./dataset/pth/resnet152-b121ed2d.pth&#x27;</span>))  <span class="comment"># 手动加 模型参数</span></span><br><span class="line">        set_parameter_requires_grad(model_ft, feature_extract)  <span class="comment"># 冻层</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 修改全连接层</span></span><br><span class="line">        num_ftrs = model_ft.fc.in_features  <span class="comment"># 获取resnet最后全连接层输入的维度 2048</span></span><br><span class="line">        model_ft.fc = nn.Sequential(nn.Linear(num_ftrs, num_classes),</span><br><span class="line">                                    nn.LogSoftmax(dim=<span class="number">1</span>))</span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">            经过修改线性层输出的维度之后变为：</span></span><br><span class="line"><span class="string">                (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))</span></span><br><span class="line"><span class="string">                (fc): Sequential(</span></span><br><span class="line"><span class="string">                    (0): Linear(in_features=2048, out_features=102, bias=True)</span></span><br><span class="line"><span class="string">                    (1): LogSoftmax()</span></span><br><span class="line"><span class="string">                )</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        input_size = <span class="number">224</span></span><br><span class="line">        <span class="built_in">print</span>(model_ft)</span><br><span class="line">    <span class="keyword">return</span> model_ft, input_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parameter_to_update</span>(<span class="params">model</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    获取需要更新的参数</span></span><br><span class="line"><span class="string">    :param model: 模型</span></span><br><span class="line"><span class="string">    :return: 需要更新的参数列表</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 是否训练所有层</span></span><br><span class="line">    params_to_update = model.parameters()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Params to learn:&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> feature_extract:</span><br><span class="line">        params_to_update = []</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                params_to_update.append(param)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\t&quot;</span>, name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\t&quot;</span>, name)</span><br><span class="line">    <span class="keyword">return</span> params_to_update</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">model, dataloaders, criterion, optimizer, filename, num_epochs</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    训练模型</span></span><br><span class="line"><span class="string">    :param model: 引入的权重模型</span></span><br><span class="line"><span class="string">    :param dataloaders: 导入数据 格式为dataloaders</span></span><br><span class="line"><span class="string">    :param criterion: 损失函数</span></span><br><span class="line"><span class="string">    :param optimizer: 优化器</span></span><br><span class="line"><span class="string">    :param filename: 模型名称（地址）</span></span><br><span class="line"><span class="string">    :param num_epochs: epoch数</span></span><br><span class="line"><span class="string">    :return:model, val_acc_history, train_acc_history, valid_losses, train_losses, LRs</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 获取起始时间</span></span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于GPU</span></span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    best_acc = <span class="number">0</span></span><br><span class="line">    val_acc_history = []</span><br><span class="line">    train_acc_history = []</span><br><span class="line">    train_loss = []</span><br><span class="line">    valid_loss = []</span><br><span class="line">    LRs = [optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]]  <span class="comment">## ？？？</span></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())  <span class="comment"># 保存最好的模型权重参数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_epochs)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch &#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练和验证</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                model.train()  <span class="comment"># 训练</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.<span class="built_in">eval</span>()  <span class="comment"># 验证</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 遍历数据</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)  <span class="comment"># inputs shape : torch.Size([16, 3, 224, 224])</span></span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 梯度清零</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 只有训练的时候计算和更新梯度</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">                    outputs = model(inputs)  <span class="comment"># torch.Size([16, 102])</span></span><br><span class="line">                    <span class="comment"># https://www.jianshu.com/p/3ed11362b54f</span></span><br><span class="line">                    _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)  <span class="comment"># dim是max函数索引的维度0/1，0是每列的最大值，1是每行的最大值</span></span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;preds&quot;</span>, preds)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 计算损失</span></span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 训练阶段更新权重</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 计算损失</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)  <span class="comment"># size 表示 inputs 的形状的 第一个也就是 batch 16</span></span><br><span class="line">                running_corrects += torch.<span class="built_in">sum</span>(preds == labels.data)  <span class="comment"># 预测和实际标签一样就算上</span></span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / <span class="built_in">len</span>(dataloaders[phase].dataset)  <span class="comment"># 这个loss的定义 ？？？</span></span><br><span class="line">            epoch_acc = running_corrects.double() / <span class="built_in">len</span>(</span><br><span class="line">                dataloaders[phase].dataset)  <span class="comment"># ???  dataloaders[phase].dataset  为什么有dataset属性 是啥呢</span></span><br><span class="line"></span><br><span class="line">            time_spend = time.time() - start  <span class="comment"># 所花费的时间 start是初始开始时间</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Time spend &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;</span>.<span class="built_in">format</span>(time_spend // <span class="number">60</span>, time_spend % <span class="number">60</span>))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 得到最好那次的模型</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;valid&#x27;</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())  <span class="comment"># copy当前最好模型的所有权重</span></span><br><span class="line">                state = &#123;</span><br><span class="line">                    <span class="string">&#x27;state_dict&#x27;</span>: model.state_dict(),  <span class="comment"># 当前最好模型的所有权重</span></span><br><span class="line">                    <span class="string">&#x27;best_acc&#x27;</span>: best_acc,  <span class="comment"># 最好的准确率 （测试集上）</span></span><br><span class="line">                    <span class="string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(),  <span class="comment"># https://www.cnblogs.com/liujianing/p/13428387.html</span></span><br><span class="line">                &#125;</span><br><span class="line">                torch.save(state, filename)  <span class="comment"># filename 保存的模型名（其实是地址）</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;valid&#x27;</span>:</span><br><span class="line">                val_acc_history.append(epoch_acc)</span><br><span class="line">                valid_loss.append(epoch_loss)</span><br><span class="line">                scheduler.step(epoch_loss)  <span class="comment"># ??? 更新权重参数</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                train_acc_history.append(epoch_acc)</span><br><span class="line">                train_loss.append(epoch_loss)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Optimizer learning rate : &#123;:.7f&#125;&#x27;</span>.<span class="built_in">format</span>(optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]))</span><br><span class="line">        LRs.append(optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>])  <span class="comment"># 添加此时的学习率</span></span><br><span class="line"></span><br><span class="line">    time_spend_all = time.time() - start  <span class="comment"># 所花费的时间 start是初始开始时间</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;</span>.<span class="built_in">format</span>(time_spend_all // <span class="number">60</span>, time_spend_all % <span class="number">60</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Best val Acc: &#123;:4f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练完后用最好的一次当做模型最终的结果</span></span><br><span class="line">    model.load_state_dict(best_model_wts)  <span class="comment"># model加载 可以查看 https://www.jianshu.com/p/60fc57e19615</span></span><br><span class="line">    <span class="comment"># 返回</span></span><br><span class="line">    <span class="keyword">return</span> model, val_acc_history, train_acc_history, valid_loss, train_loss, LRs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有全体参数的情况下  微调训练全部层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_all_layers</span>(<span class="params">num_epochs</span>):</span></span><br><span class="line">    <span class="comment"># 保存文件的名字</span></span><br><span class="line">    file_path = <span class="string">&#x27;./dataset/pth/resnet152_fc.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    checkpoint = torch.load(file_path)</span><br><span class="line">    best_acc = checkpoint[<span class="string">&#x27;best_acc&#x27;</span>]</span><br><span class="line">    model_ft = models.resnet152(pretrained=<span class="literal">False</span>)</span><br><span class="line">    model_ft.load_state_dict(checkpoint[<span class="string">&#x27;state_dict&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有参数的requires_grad 设为True 微调 训练所有层</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model_ft.parameters():</span><br><span class="line">        param.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(model_ft.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">6</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">    optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs = train_model(model_ft, dataloaders,</span><br><span class="line">                                                                                                criterion, optimizer,</span><br><span class="line">                                                                                                num_epoch=num_epochs,</span><br><span class="line">                                                                                                filename=<span class="string">&#x27;resnet152_all_layers.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fortest</span>():</span></span><br><span class="line">    <span class="comment"># 保存文件的名字</span></span><br><span class="line">    file_path = <span class="string">&#x27;./dataset/pth/resnet152_fc.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    model_ft = torch.load(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># GPU模式</span></span><br><span class="line">    model_ft = model_ft.to(device)</span><br><span class="line">    <span class="comment"># 得到一个batch的测试数据</span></span><br><span class="line">    dataiter = <span class="built_in">iter</span>(dataloaders[<span class="string">&#x27;valid&#x27;</span>])</span><br><span class="line">    images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line">    model_ft.<span class="built_in">eval</span>()</span><br><span class="line">    train_on_gpu = torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">if</span> train_on_gpu:</span><br><span class="line">        output = model_ft(images.cuda())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = model_ft(images)</span><br><span class="line"></span><br><span class="line">    _, preds_tensor = torch.<span class="built_in">max</span>(output, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    preds = np.squeeze(preds_tensor.numpy()) <span class="keyword">if</span> <span class="keyword">not</span> train_on_gpu <span class="keyword">else</span> np.squeeze(preds_tensor.cpu().numpy())</span><br><span class="line">    <span class="built_in">print</span>(preds)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结果展示</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">    columns = <span class="number">4</span></span><br><span class="line">    rows = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(columns * rows):</span><br><span class="line">        ax = fig.add_subplot(rows, columns, idx + <span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">        plt.imshow(im_convert(images[idx]))</span><br><span class="line">        ax.set_title(<span class="string">&quot;&#123;&#125; (&#123;&#125;)&quot;</span>.<span class="built_in">format</span>(cat_to_name[<span class="built_in">str</span>(preds[idx])], cat_to_name[<span class="built_in">str</span>(labels[idx].item())]),</span><br><span class="line">                     color=(<span class="string">&quot;green&quot;</span> <span class="keyword">if</span> cat_to_name[<span class="built_in">str</span>(preds[idx])] == cat_to_name[<span class="built_in">str</span>(labels[idx].item())] <span class="keyword">else</span> <span class="string">&quot;red&quot;</span>))</span><br><span class="line">    plt.savefig(<span class="string">&#x27;./img_show.png&#x27;</span>)</span><br><span class="line">    plt.close(fig)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    plt.switch_backend(<span class="string">&#x27;agg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    配置图片路径以及数据预处理</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 图片路径</span></span><br><span class="line">    data_dir = <span class="string">&#x27;./dataset/flower_data/&#x27;</span></span><br><span class="line">    train_dir = data_dir + <span class="string">&#x27;/train&#x27;</span></span><br><span class="line">    valid_dir = data_dir + <span class="string">&#x27;/valid&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取标签对应的实际名字</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./dataset/flower_data/cat_to_name.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        cat_to_name = json.load(f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据预处理</span></span><br><span class="line">    data_transforms = &#123;</span><br><span class="line">        <span class="string">&#x27;train&#x27;</span>: transforms.Compose([transforms.RandomRotation(<span class="number">45</span>),  <span class="comment"># 随机旋转，-45到45度之间随机选</span></span><br><span class="line">                                     transforms.CenterCrop(<span class="number">224</span>),  <span class="comment"># 从中心开始裁剪</span></span><br><span class="line">                                     transforms.RandomHorizontalFlip(p=<span class="number">0.5</span>),  <span class="comment"># 随机水平翻转 选择一个概率概率</span></span><br><span class="line">                                     transforms.RandomVerticalFlip(p=<span class="number">0.5</span>),  <span class="comment"># 随机垂直翻转</span></span><br><span class="line">                                     transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.1</span>, saturation=<span class="number">0.1</span>, hue=<span class="number">0.1</span>),</span><br><span class="line">                                     <span class="comment"># 参数1为亮度，参数2为对比度，参数3为饱和度，参数4为色相</span></span><br><span class="line">                                     transforms.RandomGrayscale(p=<span class="number">0.025</span>),  <span class="comment"># 概率转换成灰度率，3通道就是R=G=B</span></span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 均值，标准差</span></span><br><span class="line">                                     ]),</span><br><span class="line">        <span class="string">&#x27;valid&#x27;</span>: transforms.Compose([transforms.Resize(<span class="number">256</span>),</span><br><span class="line">                                     transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">                                     ]),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 超参数配置</span></span><br><span class="line">    <span class="comment"># 是否用人家训练好的特征来做,True 为 不改变权重参数也就是冻层，False 为自己重新训练所有层的权重参数</span></span><br><span class="line">    feature_extract = <span class="literal">True</span>  <span class="comment"># 冻层</span></span><br><span class="line">    num_classes = <span class="number">102</span>  <span class="comment"># 输出的类别数</span></span><br><span class="line">    batch_size = <span class="number">16</span>  <span class="comment"># 一次训练的样本数目</span></span><br><span class="line">    num_epochs = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建数据</span></span><br><span class="line">    image_datasets, dataloaders, dataset_sizes, class_names = create_dataset(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 获取模型  模型冻结了部分层，最后一层线性层修改</span></span><br><span class="line">    resnet152 = initialize_model(</span><br><span class="line">        model_name=<span class="string">&#x27;resnet&#x27;</span>,</span><br><span class="line">        num_classes=num_classes,</span><br><span class="line">        feature_extract=feature_extract,</span><br><span class="line">        use_pretrained=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 是否使用GPU训练</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.cuda.is_available():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;CUDA is not available.  Training on CPU ...&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;CUDA is available!  Training on GPU ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出网络结构</span></span><br><span class="line">    <span class="built_in">print</span>(summary(resnet152, (<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练参数</span></span><br><span class="line">    params_to_update = parameter_to_update(resnet152)</span><br><span class="line">    <span class="comment"># model_ft = model_ft.to(device)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化器设置</span></span><br><span class="line">    optimizer = torch.optim.Adam(params_to_update, lr=<span class="number">0.01</span>)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">6</span>, gamma=<span class="number">0.1</span>)  <span class="comment"># 学习率每7个epoch衰减成原来的1/10</span></span><br><span class="line">    <span class="comment"># 最后一层已经LogSoftmax()了，所以不能nn.CrossEntropyLoss()来计算了，nn.CrossEntropyLoss()相当于logSoftmax()和nn.NLLLoss()整合</span></span><br><span class="line">    criterion = nn.NLLLoss()</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># # 开始训练</span></span><br><span class="line">    model_ft, val_acc_history, train_acc_history, valid_losses, train_losses, LRs = train_model(resnet152, dataloaders,</span><br><span class="line">                                                                                                criterion, optimizer,</span><br><span class="line">                                                                                                num_epoch=num_epochs,</span><br><span class="line">                                                                                                filename=<span class="string">&quot;resnet152_fc.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line">    train_all_layers(num_epochs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 评估模型结果</span></span><br><span class="line">    fortest()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="博客参考"><a href="#博客参考" class="headerlink" title="博客参考"></a>博客参考</h1><p><a href="https://www.jb51.net/article/222221.htm">PyTorch一小时掌握之ResNet迁移学习篇</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;ResNet迁移学习 之 花图像分类&quot;&gt;TOC&lt;/a&gt;&lt;br&gt;迁移学习 (Transfer Learning) 是把已学训练好的模型参数用作新训练模型的起始参数. 迁移学习是深度学习中非常重要和常用的一个策略.&lt;/p&gt;
&lt;h1 id=&quot;为什么使用迁移学习</summary>
      
    
    
    
    <category term="深度学习基础" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="ResNet" scheme="http://example.com/tags/ResNet/"/>
    
    <category term="迁移学习" scheme="http://example.com/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="http://example.com/2021/11/20/ResNet/"/>
    <id>http://example.com/2021/11/20/ResNet/</id>
    <published>2021-11-20T15:52:01.000Z</published>
    <updated>2021-11-26T13:26:02.431Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="ResNet">TOC</a></p><h1 id="ResNet解决-网络退化问题"><a href="#ResNet解决-网络退化问题" class="headerlink" title="ResNet解决 网络退化问题"></a>ResNet解决 网络退化问题</h1><p>从经验来看，网络的深度对模型的性能至关重要，当增加网络层数后，网络可以进行更加复杂的特征模式的提取，所以当模型更深时理论上可以取得更好的结果。但研究表明，加深网络深度，会出现网络准确度 饱和甚至下降的情况，如下图1所示。 这个现象被称为 <font color="red">网络退化现象 </font><br><img src="https://img-blog.csdnimg.cn/img_convert/a4362ebe13d4e05ed73f84e3fea663d0.png#pic_center" alt="图1：20层和56层在CIFAR-10数据集上的训练error表现"><br>那网络退化问题是过拟合导致的么？当然不是，因为网络越深理论上提取特征的能力越强，越不容易过拟合。如上图1所示，56层的网络误差整体都比26层的高。</p><p>所以其从本质上看，其实是模型深度越大，其训练不动的情况。换句话说他可能存在<strong>一定的梯度消失问题</strong>，导致高深度网络难以训练。也就是，现在的网络训练方式肯定有点问题，让深度网络很难的反向传播找到一组很好的参数。</p><h1 id="ResNet-残差学习"><a href="#ResNet-残差学习" class="headerlink" title="ResNet 残差学习"></a>ResNet 残差学习</h1><p>其实网络退化的现象可以通俗的理解为，一个小孩报了更多的班，然而成绩还下降了。我们的目的应该是最起码报的班可能没作用，但是不至于成绩还下降了。</p><p>基于这个想法，现在我们有一个浅层网络，我们想通过堆积更多的层次来建立深层网络，一个极端的情况就是这些新的层可能作用都不起，仅仅复制了浅层网络的特征（也就是不至于退步），这样的新层可以被称为 <font color="red">恒等映射 Identity mapping </font></p><p>在ResNet中，何大佬想到了利用之前的 机器学习的残差 和 跳跃（短路）连接 来实现一种新的结构。</p><blockquote><p>机器学习的<strong>残差</strong>，其实就是预测值和标签值之间的距离。我们的目的其实是让预测毕竟真实的标签值。如下图2所示，大括号的部分就是所谓的残差。<br><img src="https://img-blog.csdnimg.cn/img_convert/4978f970591937eed026ea8b6992c338.png#pic_center" alt="图2：何为残差？"></p></blockquote><p>何大佬的具体想法是，对于一个堆集层结构。当输入为 $x$ 时其学习到的特征记作 $H(x)$ ，而我们要 <strong>学习的部分为 残差</strong>  $F(x) = H(x)-x$，这样原始的学习特征为 $F(x)+x$。 <font color="red">为什么这么设计？</font> <font color="purple">因为残差学习相比原始特征的直接学习容易得多。</font>  如下图3所示，为残差结构：<br><img src="https://img-blog.csdnimg.cn/img_convert/41e7e4a3768e9840bbca32443df7aa67.png#pic_center" alt="图3 残差结构"><br>当残差为0时，此时堆积层仅仅做了恒等映射，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。</p><h2 id="为什么残差学习更容易？"><a href="#为什么残差学习更容易？" class="headerlink" title="为什么残差学习更容易？"></a>为什么残差学习更容易？</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/b1c4a0899a120d761099b2add95c77da.png#pic_center" alt="图4 为什么残差学习更容易？"></p><h1 id="ResNet的网络结构"><a href="#ResNet的网络结构" class="headerlink" title="ResNet的网络结构"></a>ResNet的网络结构</h1><p>ResNet网络是参考了 VGG19 网络，在其基础上进行了修改，并通过短路机制加入了残差单元，如图4所示。变化主要体现在 ResNet 直接使用 stride=2 的卷积做下采样，并且用 global average pool 层替换了全连接层。</p><p>ResNet 的一个重要设计原则是：当 feature map 大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。从图4中可以看到，ResNet相比普通网络每两层间增加了短路机制，这就形成了残差学习，其中虚线表示feature map数量发生了改变。图4展示的34-layer的ResNet，还可以构建更深的网络如表1所示。从表1中可以看到，对于18-layer和34-layer的ResNet，其进行的两层间的残差学习，当网络更深时，其进行的是三层间的残差学习，三层卷积核分别是1x1，3x3和1x1，一个值得注意的是隐含层的feature map数量是比较小的，并且是输出feature map数量的1/4。<br><img src="https://img-blog.csdnimg.cn/img_convert/f2f0cda5c64aad9bd20d77d4a67a3019.png#pic_center" alt="图4 ResNet网络结构图"><br><img src="https://img-blog.csdnimg.cn/img_convert/f110d29b9825b218ceb06b6b1263c539.png#pic_center" alt="表1 不同深度的ResNet"></p><blockquote><p>为什么ResNet50 明明模型深很多但是 参数量却和ResNet34差不多呢？<br>因为 ResNet50采用的为bottleneck残差模块。那为什么用这个模块呢？ 又因为通道数比较大，比如64直接通过变成256的。<br>相当于4倍，如果我们还是使用3×3的卷积的话，那计算复杂度会高很多。 （这也是用到1×1的优势）可以看一下 Inception 里 描述 1×1的优势 <a href="https://jks88995656.github.io/2021/09/21/Pytorch%20GoogleNet%E4%B8%AD%E7%9A%84Inception/">Inception</a></p></blockquote><p>下面我们再分析一下残差单元，ResNet使用两种残差单元，如图5所示。<br>左图对应的是浅层网络普通残差模块，而右图对应的是深层网络bottleneck残差模块。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/e03e8e41aae5616f3349e8e135f34422.png#pic_center" alt="图5 两种残差模块结构"><br>对于短路连接，当输入和输出维度一致时，可以直接将输入加到输出上。但是当维度不一致时（对应的是维度增加一倍），这就不能直接相加。</p><p>有2种策略：</p><ol><li>采用zero-padding增加维度，此时一般要先做一个downsamp，可以采用strde=2的pooling，这样不会增加参数；</li><li>采用新的映射（projection shortcut），一般采用1x1的卷积，这样会增加参数，也会增加计算量。短路连接除了直接使用恒等映射，当然都可以采用projection shortcut。（所以这里有两种选择）</li></ol><p>综上所述，理论上有3种方式A、B、C来用于增加维度：<br><strong>我们一般会选用B方案。</strong></p><ul><li>A 所有的短路连接升维度都采用 padding 补零方式。 其不增加参数量。</li><li>B 需要调整维度的 才有 projection shortcut 1×1卷积，其他的短路连接保持不变。 其参数量有一些。</li><li>C 所有的短路连接都采用 projection shortcut 1×1卷积。 引入参数量较大。</li></ul><blockquote><p>从实验结果看，如下图6所示。之所以我们选择B方法，是因为其效果相对A来说还不错，参数增加相比C方案来说少。<br><img src="https://img-blog.csdnimg.cn/img_convert/be137c8d1dee1ee3617bb1da03b1b9ee.png#pic_center" alt="图6 A、B、C三种方式的error"></p></blockquote><h1 id="ResNet的迁移"><a href="#ResNet的迁移" class="headerlink" title="ResNet的迁移"></a>ResNet的迁移</h1><p>其在各个领域都有不错的效果。一般都可作为backbone，例如faster R-CNN 就是这么干的。<br>作者在论文中做了很多实验，后续写。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;ResNet&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;ResNet解决-网络退化问题&quot;&gt;&lt;a href=&quot;#ResNet解决-网络退化问题&quot; class=&quot;headerlink&quot; title=&quot;ResNet解决 网络退化问题&quot;&gt;&lt;/a&gt;ResNet解决</summary>
      
    
    
    
    <category term="深度学习基础" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="ResNet" scheme="http://example.com/tags/ResNet/"/>
    
  </entry>
  
  <entry>
    <title>浅谈 VGG</title>
    <link href="http://example.com/2021/11/01/%E6%B5%85%E8%B0%88%20VGG/"/>
    <id>http://example.com/2021/11/01/%E6%B5%85%E8%B0%88%20VGG/</id>
    <published>2021-11-01T15:52:01.000Z</published>
    <updated>2021-11-26T13:26:55.049Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="浅谈 VGG">TOC</a><br>《Very Deep Convolutional Networks for Large-Scale Image Recognition》</p><p>arXiv：<a href="https://arxiv.org/abs/1409.1556">[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition<br>intro：ICLR 2015</a><br>homepage：<a href="https://www.robots.ox.ac.uk/~vgg/research/very_deep/">Visual Geometry Group Home Page</a></p><h1 id="VGG-特点"><a href="#VGG-特点" class="headerlink" title="VGG 特点"></a>VGG 特点</h1><p>VGG 网络是在ILSVRC 2014上的相关工作，主要工作是证明了增加网络的深度能够在一定程度上影响网络最终的性能。<br>VGG有两种结构，分别是VGG16和VGG19，两者并没有本质上的区别，只是网络深度不一样。</p><ul><li>VGG16包含了16个隐藏层（13个卷积层和3个全连接层） 其特点为 卷积块的卷积层个数为  2 2 3 3 3<br><img src="https://img-blog.csdnimg.cn/img_convert/22de401629c305540a9f2ccaef96ba63.png#pic_center" alt="VGG16"><br><img src="https://img-blog.csdnimg.cn/img_convert/786fc264427f593ff33f5dede514a990.png#pic_center" alt="VGG16"></li></ul><ul><li>VGG19包含了19个隐藏层（16个卷积层和3个全连接层） 的特点为 卷积块的卷积层个数为  2 2 4 4 4</li></ul><p><img src="https://img-blog.csdnimg.cn/img_convert/27f07e7317617a43f88350a560c371a6.png#pic_center" alt="VGG网络架构"><br>全连接转卷积（测试阶段）<br>这也是VGG的一个特点，在网络测试阶段将训练阶段的三个全连接替换为三个卷积，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入，这在测试阶段很重要。<br>例如，输入图像是224x224x3，如果后面三个层都是全连接，那么在测试阶段就只能将测试的图像全部都要缩放大小到224x224x3，才能符合后面全连接层的输入数量要求，这样就不便于测试工作的开展。</p><h1 id="VGG-原理"><a href="#VGG-原理" class="headerlink" title="VGG 原理"></a>VGG 原理</h1><p>VGG16相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（如11x11，7x7，5x5）。对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。</p><p>为什么2个3×3的卷积核 可以 代替 1个5×5的卷积核？<br><img src="https://img-blog.csdnimg.cn/img_convert/5c12d10dd0738e23846584801c3a686f.png#pic_center" alt="两个3×3代替5×5"><br>因为我们知道   例如输入图片大小为C 卷积核大小为k  步长为d 那输出图片大小就是 $\lfloor \frac{C+2 * padding-k}{d} \rfloor +1$<br>那pad=0 d=1 对于2个3×3的而言 计算得到的 输出图片大小为 C-4<br>同理 一个5×5的 也为 C-4</p><p>总结的来说优势有3点：</p><ol><li>分成多个层的小卷积核来代替大卷积核，可以增加网络深度。而每一层都会引入非线性的激活函数，所以其非线性表达的能力会更好，模型的分类性能更好。</li><li>计算参数减少了。举个例子比如输入维度是C(也就是卷积核通道维度)，输出维度是C(也就是卷积核个数)，如果使用2层3×3的卷积层来代替一层5×5的，3×3的参数总量为 3×3×C×C×2 = $18C^{2}$；对应的  5×5的 为 5×5×C×C×1 = $25C^{2}$</li><li>使用多个层的小卷积核，相当于将一个大的感受野分成多个小感受野来学习，是一种正则化的思想。注意：正则化是一种思想方式，其目的为防止过拟合。其具体方法有：BN、dropout、L1和L2正则化项等。</li></ol><h1 id="VGG优缺点"><a href="#VGG优缺点" class="headerlink" title="VGG优缺点"></a>VGG优缺点</h1><h2 id="VGG优点"><a href="#VGG优点" class="headerlink" title="VGG优点"></a>VGG优点</h2><ul><li>VGG16和VGG19 的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。</li><li>几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5或7x7）卷积层好：</li><li>验证了通过不断加深网络结构可以提升性能。</li><li><p>VGG 是传统的串行结构，输出的内容例如左上角提取的就是原图像左上角的特征。<br><img src="https://img-blog.csdnimg.cn/img_convert/afe80f4458553cf142cd9a8f6b7cfc40.png#pic_center" alt="FeatureMap的提取情况"></p></li><li><p>VGG 的迁移学习能力很强，一般都用来做基础模型。我们只需要修改最后1层的输出结构，冻结前面所有在ImageNet上训练VGG后的参数，再拿自己的数据集来训练最后4096神经元和最后一层输出的参数即可。</p></li><li>采用多尺度训练方式，训练的数据可以有 224、256、384、[256，512]</li><li>作者提出的 在test上改进的方法，将最后的全连接层改为全卷积层，但是作者训练的时候并没有这么用。这个方法可以实现多尺度输入，原始的 全神经网络的话 他层次的神经元是定死的 所以我没办法换个尺度的图片输入。<br><img src="https://img-blog.csdnimg.cn/img_convert/d2092e465499fab47e8e6587f744e396.png#pic_center" alt="test"></li></ul><h2 id="VGG缺点"><a href="#VGG缺点" class="headerlink" title="VGG缺点"></a>VGG缺点</h2><p>参数非常庞大。庞大在卷积层输出压平后与第一层全神经网络之间的参数。<br><img src="https://img-blog.csdnimg.cn/img_convert/5a91f370bba686cfa4003affe2a9eefc.png#pic_center" alt="参数计算"><br>首先看第二行，所谓的内存就是输出图片大小  而参数就是卷积核×通道数×卷积核个数64个。<br>注意这里是没有算上偏置量的，算的话就是每个卷积核一个偏置项，会增广成矩阵加上去的。</p><h1 id="VGG的训练和测试"><a href="#VGG的训练和测试" class="headerlink" title="VGG的训练和测试"></a>VGG的训练和测试</h1><p> 论文首先将训练图像缩放到最小边长度的方形，设缩放后的训练图像的尺寸为S×S。网络训练时对训练图像进行随机裁剪，裁剪尺寸为网络的输入尺寸224×224。如果S=224，则输入网络的图像就是整个训练图像；如果S&gt;224，则随机裁剪训练图像包含目标的部分。</p><p>对于训练集图像的尺寸设置，论文中使用了两种方法：</p><p>固定尺寸训练，设置 <strong>S=256</strong> 和 <strong>S=384</strong><br>多尺度训练，每个训练图像从一定范围内 <strong>[Smin,Smax],(Smin=256,Smax=512)</strong> 进行随机采样。由于图像中的目标可能具有不同的大小，因此在训练期间考虑到这一点是有益的。这也可以看作是通过尺度抖动进行训练集增强，其中单个模型被训练在一定尺度范围内识别对象。</p><h2 id="网络性能评估"><a href="#网络性能评估" class="headerlink" title="网络性能评估"></a>网络性能评估</h2><ul><li><strong>单尺度评估，测试图像固定尺度。结果如下表</strong><br><img src="https://img-blog.csdnimg.cn/img_convert/c06443bd58d1867902a59902e0a5b2c7.png#pic_center" alt="单一尺度评估"><br>通过评估结果，可以看出：<ul><li>局部归一化（A-LRN）网络，对网络A的结果并没有很大的提升。</li><li>网络的性能随着网络的加深而提高。应该注意到B，C，D这个网络的性能。C网络好于B网络，说明额外添加的非线性激活函数，确实是有好处的；但是，D网络好于C网络，这说明也可以使用非平凡的感受野来捕获更多的信息更有用。</li><li>当网络层数达到19层时，使用VGG架构的错误率就不再随着层数加深而提高了。更深的网络应该需要更多的数据集。</li><li>论文还将网络B与具有5×5卷积层的浅层网络进行了比较，浅层网络可以通过用单个5×5卷积层替换B中每对3×3卷积层得到。测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，<strong>这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络</strong>。</li><li>训练时的尺寸抖动（训练图像大小S∈[256,512])得到的结果好于固定尺寸，这证实了通过尺度抖动进行的训练集增强确实有助于捕获多尺度图像统计。</li></ul></li></ul><ul><li><p><strong>多尺度评估，测试图像的尺度抖动对性能的影响</strong><br>  对同一张测试图像，将其缩放到不同的尺寸进行测试，然后取这几个测试结果的平均值，作为最终的结果（有点像集成学习，所不同的是，这里是测试图像的尺寸不同）。使用了三种尺寸的测试图像：Q表示测试图像，S表示训练是图像尺寸：Q=S−32，Q=S+32，前面两种是针对训练图像是固定大小的，对于训练时图像尺寸在一定范围内抖动的，则可以使用更大的测试图像尺寸。   Q={Smin,0.5(Smin+Smax),Smax}.<br>评估结果如下：<br><img src="https://img-blog.csdnimg.cn/img_convert/339fb07aaff8cf5e01ac2dab96205f39.png#pic_center" alt="多尺度评估，测试图像的尺度抖动对性能的影响"><br>评估结果表明，训练图像尺度抖动优于使用固定最小边S。</p></li><li><p><strong>稠密和多裁剪图像评估</strong><br>Dense（密集评估），即指全连接层替换为卷积层（第一FC层转换到7×7卷积层，最后两个FC层转换到1×1卷积层），最后得出一个预测的score map，再对结果求平均。<br>multi-crop，即对图像进行多样本的随机裁剪，将得到多张裁剪得到的图像输入到网络中，最终对所有结果平均.<br><img src="https://img-blog.csdnimg.cn/img_convert/925e5742f42b654fabe4bf03cf56f1b8.png#pic_center" alt="多裁剪与密度估计"><br>从上图可以看出，<strong>多裁剪的结果是好于密集估计的</strong>。而且这两种方法确实是互补的，因为它们的组合优于其中的每一种。<br>由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的图像内容信息。</p></li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul><li>在训练时，可以使用多尺度抖动的训练图像，其精度好于固定尺寸的训练集。</li><li>测试时，使用多裁剪和密集评估（卷积层替换全连接层）像结合的方法<img src="https://img-blog.csdnimg.cn/img_convert/d2092e465499fab47e8e6587f744e396.png#pic_center" alt="卷积层替换全连接层"></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;浅谈 VGG&quot;&gt;TOC&lt;/a&gt;&lt;br&gt;《Very Deep Convolutional Networks for Large-Scale Image Recognition》&lt;/p&gt;
&lt;p&gt;arXiv：&lt;a href=&quot;https://arxiv.org</summary>
      
    
    
    
    <category term="深度学习基础" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>针对 VOC2007和VOC2012 的具体用法</title>
    <link href="http://example.com/2021/11/01/%E9%92%88%E5%AF%B9%20VOC2007%E3%80%81VOC2012%E5%92%8CCOCO%20%E7%9A%84%E5%85%B7%E4%BD%93%E7%94%A8%E6%B3%95/"/>
    <id>http://example.com/2021/11/01/%E9%92%88%E5%AF%B9%20VOC2007%E3%80%81VOC2012%E5%92%8CCOCO%20%E7%9A%84%E5%85%B7%E4%BD%93%E7%94%A8%E6%B3%95/</id>
    <published>2021-11-01T14:52:01.000Z</published>
    <updated>2021-11-26T13:26:49.930Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="针对 VOC2007、VOC2012和COCO 的具体用法">TOC</a></p><p>目前广大研究者们普遍使用的是 VOC2007和VOC2012数据集，因为二者是互斥的，不相容的。</p><p>论文中针对 VOC2007和VOC2012 的具体用法有以下几种：</p><ul><li>只用VOC2007的trainval 训练，使用VOC2007的test测试。</li><li>只用VOC2012的trainval 训练，使用VOC2012的test测试，这种用法很少使用，因为大家都会结合VOC2007使用。</li><li>使用 VOC2007 的 train+val 和 VOC2012的 train+val 训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 <strong>07+12</strong> ，研究者可以自己测试在VOC2007上的结果，因为VOC2007的test是公开的。</li><li>使用 VOC2007 的 train+val+test 和 VOC2012的 train+val训练，然后使用 VOC2012的test测试，这个用法是论文中经常看到的 <strong>07++12</strong> ，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。</li><li>先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val、 VOC2012的 train+val 微调训练，然后使用 VOC2007的test测试，这个用法是论文中经常看到的 <strong>07+12+COCO</strong> 。</li><li>先在 MS COCO 的 trainval 上预训练，再使用 VOC2007 的 train+val+test 、 VOC2012的 train+val 微调训练，然后使用 VOC2012的test测试 ，这个用法是论文中经常看到的 <strong>07++12+COCO</strong>，这种方法需提交到VOC官方服务器上评估结果，因为VOC2012 test没有公布。</li></ul><h1 id="代码分离-VOC训练集-的-val-和-train-数据"><a href="#代码分离-VOC训练集-的-val-和-train-数据" class="headerlink" title="代码分离 VOC训练集 的 val 和 train 数据"></a>代码分离 VOC训练集 的 val 和 train 数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">anchors =[<span class="number">0.57273</span>, <span class="number">0.677385</span>, <span class="number">1.87446</span>, <span class="number">2.06253</span>, <span class="number">3.33843</span>, <span class="number">5.47434</span>, <span class="number">7.88282</span>, <span class="number">3.52778</span>, <span class="number">9.77052</span>, <span class="number">9.16828</span>]</span><br><span class="line">anchors = np.array(anchors).reshape(-<span class="number">1</span>, <span class="number">2</span>) <span class="comment">#(5,2)</span></span><br><span class="line">input_shape = (<span class="number">416</span>, <span class="number">416</span>)</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">colors = [[<span class="number">255</span>,<span class="number">0</span>,<span class="number">255</span>],[<span class="number">218</span>,<span class="number">112</span>,<span class="number">214</span>],[<span class="number">100</span>,<span class="number">149</span>,<span class="number">237</span>],[<span class="number">95</span>,<span class="number">158</span>,<span class="number">160</span>],[<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>],[<span class="number">0</span>,<span class="number">255</span>,<span class="number">127</span>],[<span class="number">107</span>,<span class="number">142</span>,<span class="number">35</span>],</span><br><span class="line">        [<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>],[<span class="number">184</span>,<span class="number">134</span>,<span class="number">11</span>],[<span class="number">255</span>,<span class="number">165</span>,<span class="number">0</span>],[<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">224</span>,<span class="number">255</span>,<span class="number">255</span>],[<span class="number">70</span>,<span class="number">130</span>,<span class="number">180</span>],[<span class="number">255</span>,<span class="number">192</span>,<span class="number">203</span>],[<span class="number">255</span>,<span class="number">240</span>,<span class="number">245</span>],[<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>],</span><br><span class="line">        [<span class="number">240</span>,<span class="number">128</span>,<span class="number">128</span>],[<span class="number">220</span>,<span class="number">220</span>,<span class="number">220</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">169</span>,<span class="number">169</span>,<span class="number">169</span>]]</span><br><span class="line">VOCdevkit_path = <span class="string">&#x27;./VOCdevkit&#x27;</span></span><br><span class="line">classes = [<span class="string">&#x27;aeroplane&#x27;</span>,<span class="string">&#x27;bicycle&#x27;</span>,<span class="string">&#x27;bird&#x27;</span>,<span class="string">&#x27;boat&#x27;</span>,<span class="string">&#x27;bottle&#x27;</span>,<span class="string">&#x27;bus&#x27;</span>,<span class="string">&#x27;car&#x27;</span>,<span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;chair&#x27;</span>,<span class="string">&#x27;cow&#x27;</span>,<span class="string">&#x27;diningtable&#x27;</span>,<span class="string">&#x27;dog&#x27;</span>,<span class="string">&#x27;horse&#x27;</span>,<span class="string">&#x27;motorbike&#x27;</span>,<span class="string">&#x27;person&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;pottedplant&#x27;</span>,<span class="string">&#x27;sheep&#x27;</span>,<span class="string">&#x27;sofa&#x27;</span>,<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;tvmonitor&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把数据集拆分成训练和测试集两部分</span></span><br><span class="line"><span class="comment"># 分别保存在 train.txt 和 val.txt 中</span></span><br><span class="line"><span class="comment"># 把数据集拆分成训练和测试集两部分</span></span><br><span class="line"><span class="comment"># 分别保存在val.txt和train.txt中</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> classes,VOCdevkit_path</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">VOCdevkit_sets = [(<span class="string">&#x27;2012&#x27;</span>, <span class="string">&#x27;train&#x27;</span>), (<span class="string">&#x27;2012&#x27;</span>, <span class="string">&#x27;val&#x27;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_annotation</span>(<span class="params">year, image_id, list_file</span>):</span></span><br><span class="line">    in_file = <span class="built_in">open</span>(os.path.join(VOCdevkit_path, <span class="string">&#x27;VOC%s/Annotations/%s.xml&#x27;</span> % (year, image_id)), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    tree = ET.parse(in_file)</span><br><span class="line">    root = tree.getroot()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> root.<span class="built_in">iter</span>(<span class="string">&#x27;object&#x27;</span>):</span><br><span class="line">        difficult = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> obj.find(<span class="string">&#x27;difficult&#x27;</span>) != <span class="literal">None</span>:</span><br><span class="line">            difficult = obj.find(<span class="string">&#x27;difficult&#x27;</span>).text</span><br><span class="line">        cls = obj.find(<span class="string">&#x27;name&#x27;</span>).text</span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> classes <span class="keyword">or</span> <span class="built_in">int</span>(difficult) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        cls_id = classes.index(cls)</span><br><span class="line">        xmlbox = obj.find(<span class="string">&#x27;bndbox&#x27;</span>)</span><br><span class="line">        b = (<span class="built_in">int</span>(<span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmin&#x27;</span>).text)), <span class="built_in">int</span>(<span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymin&#x27;</span>).text)),</span><br><span class="line">             <span class="built_in">int</span>(<span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;xmax&#x27;</span>).text)), <span class="built_in">int</span>(<span class="built_in">float</span>(xmlbox.find(<span class="string">&#x27;ymax&#x27;</span>).text)))</span><br><span class="line">        list_file.write(<span class="string">&quot; &quot;</span> + <span class="string">&quot;,&quot;</span>.join([<span class="built_in">str</span>(a) <span class="keyword">for</span> a <span class="keyword">in</span> b]) + <span class="string">&#x27;,&#x27;</span> + <span class="built_in">str</span>(cls_id))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    random.seed(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> year, image_set <span class="keyword">in</span> VOCdevkit_sets:  <span class="comment">#[(&#x27;2012&#x27;, &#x27;train&#x27;), (&#x27;2012&#x27;, &#x27;val&#x27;)]</span></span><br><span class="line">        image_ids = <span class="built_in">open</span>(os.path.join(VOCdevkit_path, <span class="string">&#x27;VOC%s/ImageSets/Main/%s.txt&#x27;</span> % (year, image_set)),</span><br><span class="line">                         encoding=<span class="string">&#x27;utf-8&#x27;</span>).read().strip().split()</span><br><span class="line">        <span class="built_in">print</span>(image_ids)</span><br><span class="line">        list_file = <span class="built_in">open</span>(<span class="string">&#x27;%s_%s.txt&#x27;</span> % (year, image_set), <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> image_id <span class="keyword">in</span> image_ids:</span><br><span class="line">            list_file.write(<span class="string">&#x27;%s/VOC%s/JPEGImages/%s.jpg&#x27;</span> % (os.path.abspath(VOCdevkit_path), year, image_id))</span><br><span class="line">            convert_annotation(year, image_id, list_file)</span><br><span class="line">            list_file.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        list_file.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;针对 VOC2007、VOC2012和COCO 的具体用法&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;目前广大研究者们普遍使用的是 VOC2007和VOC2012数据集，因为二者是互斥的，不相容的。&lt;/p&gt;
&lt;p&gt;论文中针对 VOC2007和VOC2012 的具体</summary>
      
    
    
    
    <category term="数据集" scheme="http://example.com/categories/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据集" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>Python数据处理篇之Matplotlib系列</title>
    <link href="http://example.com/2021/10/30/Python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%AF%87%E4%B9%8BMatplotlib%E7%B3%BB%E5%88%97/"/>
    <id>http://example.com/2021/10/30/Python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%AF%87%E4%B9%8BMatplotlib%E7%B3%BB%E5%88%97/</id>
    <published>2021-10-30T14:52:01.000Z</published>
    <updated>2021-11-26T13:26:44.207Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Python数据处理篇之Matplotlib系列">TOC</a></p><h1 id="创建画布与显示"><a href="#创建画布与显示" class="headerlink" title="创建画布与显示"></a>创建画布与显示</h1><p><a href="https://www.cnblogs.com/zyg123/p/10504622.html">Python数据处理篇之Matplotlib系列(一)—-初识Matplotlib</a></p><h1 id="plt-scatter-散点图"><a href="#plt-scatter-散点图" class="headerlink" title="plt.scatter()散点图"></a>plt.scatter()散点图</h1><p><a href="https://www.cnblogs.com/zyg123/p/10504628.html">Python数据处理篇之Matplotlib系列(二)—-plt.scatter()散点图</a></p><h1 id="plt-plot-折线图"><a href="#plt-plot-折线图" class="headerlink" title="plt.plot()折线图"></a>plt.plot()折线图</h1><p><a href="https://www.cnblogs.com/zyg123/p/10504633.html">Python数据处理篇之Matplotlib系列(三)—-plt.plot()折线图</a></p><h1 id="plt-bar-与plt-barh条形图"><a href="#plt-bar-与plt-barh条形图" class="headerlink" title="plt.bar()与plt.barh条形图"></a>plt.bar()与plt.barh条形图</h1><p><a href="https://www.cnblogs.com/zyg123/p/10504637.html">Python数据处理篇之Matplotlib系列(四)—-plt.bar()与plt.barh条形图</a></p><h1 id="plt-pie-饼状图"><a href="#plt-pie-饼状图" class="headerlink" title="plt.pie()饼状图"></a>plt.pie()饼状图</h1><p><a href="https://www.cnblogs.com/zyg123/p/10504640.html">Python数据处理篇之Matplotlib系列(五)—-plt.pie()饼状图</a></p><h1 id="plt-hist-与plt-hist2d-直方图"><a href="#plt-hist-与plt-hist2d-直方图" class="headerlink" title="plt.hist()与plt.hist2d()直方图"></a>plt.hist()与plt.hist2d()直方图</h1><p><a href="https://www.cnblogs.com/zyg123/p/10504645.html">Python数据处理篇之Matplotlib系列(六)—-plt.hist()与plt.hist2d()直方图</a></p><h1 id="matplotlib原理分析"><a href="#matplotlib原理分析" class="headerlink" title="matplotlib原理分析"></a>matplotlib原理分析</h1><p><a href="https://www.cnblogs.com/zyg123/p/10512513.html">Python数据处理篇之Matplotlib系列(七)—-matplotlib原理分析</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Python数据处理篇之Matplotlib系列&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;创建画布与显示&quot;&gt;&lt;a href=&quot;#创建画布与显示&quot; class=&quot;headerlink&quot; title=&quot;创建画布与显示&quot;&gt;&lt;/a&gt;创建画布与显示&lt;/h1&gt;&lt;p</summary>
      
    
    
    
    <category term="python工具包类" scheme="http://example.com/categories/python%E5%B7%A5%E5%85%B7%E5%8C%85%E7%B1%BB/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Matplotlib" scheme="http://example.com/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet 元老 开创的创新点</title>
    <link href="http://example.com/2021/10/29/AlexNet%20%E5%85%83%E8%80%81%20%E5%BC%80%E5%88%9B%E7%9A%84%E5%88%9B%E6%96%B0%E7%82%B9/"/>
    <id>http://example.com/2021/10/29/AlexNet%20%E5%85%83%E8%80%81%20%E5%BC%80%E5%88%9B%E7%9A%84%E5%88%9B%E6%96%B0%E7%82%B9/</id>
    <published>2021-10-29T14:52:01.000Z</published>
    <updated>2021-10-29T14:58:11.730Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="AlexNet 元老 开创的创新点">TOC</a></p><h1 id="创新点1：CNN-卷积神经网络"><a href="#创新点1：CNN-卷积神经网络" class="headerlink" title="创新点1：CNN 卷积神经网络"></a>创新点1：CNN 卷积神经网络</h1><p>这个就不用说了 CNN 都懂的啊</p><h1 id="创新点2：relu激活函数"><a href="#创新点2：relu激活函数" class="headerlink" title="创新点2：relu激活函数"></a>创新点2：relu激活函数</h1><p>relu 是一个 非饱和的激活函数。用公式来说就是：</p><script type="math/tex; mode=display">f(x) = max (0 , x )</script><p>饱和函数最大的问题在，他左右两边随自变量的变化，应变量变化缓慢。换句话就是说，他的梯度变化基本没有，甚至有可能是0。这就有可能导致梯度消失（如果是0的话），或者训练时间很长，且收敛效果不佳。</p><h1 id="创新点3：双GPU模型并行"><a href="#创新点3：双GPU模型并行" class="headerlink" title="创新点3：双GPU模型并行"></a>创新点3：双GPU模型并行</h1><p>作者使用这个完全是因为当年的GPU的内存实在太小了。那对于现在来说这并不是什么问题，但是这个思想可以参考。<br><img src="https://img-blog.csdnimg.cn/img_convert/2addade07ed13528e4c86bdc62d8b5dc.png#pic_center" alt="AlexNet的网络结构"></p><h1 id="创新点4：LRN局部响应归一化"><a href="#创新点4：LRN局部响应归一化" class="headerlink" title="创新点4：LRN局部响应归一化"></a>创新点4：LRN局部响应归一化</h1><p>其提出的目的，是作者认为，每个通道的像素点不应该过高的激活，过高的激活可能就会导致 <strong>其他通道的对应像素点激活被抑制</strong>。这就好像，生物学上，你过分的激活了对顶芽的生长，就会抑制其侧芽的生长一样。</p><p>其作用在AlexNet的前两个卷积层，顺序为 Relu之后为LRN再 MaxPooling</p><p>他整体的思路如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/fa9ffa5a4bcf383a94fcb9f0f7f5d496.png#pic_center" alt="LRN描述"><br>上面相当于其查看了其附近两个通道的，公式就是上面红色的部分。其具体的计算公式如右图所示（其参数意思看下图）<br><img src="https://img-blog.csdnimg.cn/img_convert/c0ed720c1d6fb8a5a998d12f1aee458c.png#pic_center" alt="LRN 公式参考"></p><p><font color="red">但这个方法 被后来的VGG证实没有啥用，就是其根本就不起作用。用了还占内存</font><br>其实想想看确实没啥用，Relu函数作为一个非饱和激活函数，其压根不需要normalization啊。normalization的意思是将数据统一在中间区域内，本质的含义其实就是为了让他避免饱和区啦。</p><h1 id="创新点5：重叠池化-overlapping-pooling"><a href="#创新点5：重叠池化-overlapping-pooling" class="headerlink" title="创新点5：重叠池化 overlapping pooling"></a>创新点5：重叠池化 overlapping pooling</h1><p>作者认为可以 防止过拟合。 现在没人用了。<br><img src="https://img-blog.csdnimg.cn/img_convert/fa0b6df532ca6ec291650ea1cc13ca44.png#pic_center" alt="重叠池化"></p><h1 id="创新点6：防止过拟合之数据增强"><a href="#创新点6：防止过拟合之数据增强" class="headerlink" title="创新点6：防止过拟合之数据增强"></a>创新点6：防止过拟合之数据增强</h1><p>其提供了两种数据增强的方式。 注意：这个完全可以由CPU来做，所以作者在训练上一个batch的时候，已经准备好了下一次batch所需要的图像数据。</p><ol><li>第一种就是图像翻转，裁剪等（平移和水平翻转）<br><img src="https://img-blog.csdnimg.cn/img_convert/8a93f605bc3e15d5edda9ad9893c5fc0.png#pic_center" alt="图像翻转，裁剪"></li><li>颜色变换。<br> 其使用了PCA的方式先提取了一下他的主成分。他的意思是我在其主成分的基础上进行一定的调整颜色和光照强度和亮度，这样的话可以使我生成的图片更加的自然。具体可以看下面的公式：<br><img src="https://img-blog.csdnimg.cn/img_convert/a6c26321e94e016760b8adc7858da580.png#pic_center" alt="颜色变换的公式"></li></ol><h1 id="创新点7：防止过拟合之Dropout"><a href="#创新点7：防止过拟合之Dropout" class="headerlink" title="创新点7：防止过拟合之Dropout"></a>创新点7：防止过拟合之Dropout</h1><p>这个非常有用。现在还在用。<br>其大致的意思就是，训练阶段每一个batch随机掐死一半的神经元（也就是将神经元的输出设置为0 即其在前向和反向传播中均不起作用）。<br>在预测阶段，保留所有神经元。<br>这里还是要具体了解一下，暂时我还不知道具体原理啥的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;AlexNet 元老 开创的创新点&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;创新点1：CNN-卷积神经网络&quot;&gt;&lt;a href=&quot;#创新点1：CNN-卷积神经网络&quot; class=&quot;headerlink&quot; title=&quot;创新点1：CNN 卷积神经网络&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>如何评价目标检测的性能 MAP如何计算等</title>
    <link href="http://example.com/2021/10/26/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E6%80%A7%E8%83%BD%20MAP%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E7%AD%89/"/>
    <id>http://example.com/2021/10/26/%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E6%80%A7%E8%83%BD%20MAP%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E7%AD%89/</id>
    <published>2021-10-26T14:35:01.000Z</published>
    <updated>2021-10-29T15:03:55.678Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="如何评价目标检测的性能 MAP如何计算等">TOC</a><br>目标检测模型的性能指标分为速度指标和准确度指标。速度指标通常用每秒推断帧数 FPS （Frames Per Second）衡量，但受硬件影响较大。</p><p>目标检测输入图像，输出图像中各个目标预测框的矩形坐标及各类别预测置信度Conf。</p><p>采用交并比 IOU 衡量预测框和标注框的重合程度，即预测框的定位是否准确。</p><script type="math/tex; mode=display">IOU=\frac{area(B_{p}∩B_{gt})}{area(B_{p}∪B_{gt})}</script><p>式子中，$B<em>{p}$为预测框，$B</em>{gt}$为标准框</p><p>根据与标注框的关系，可将某一预测框划分为下式四类中的某一类</p><blockquote><ul><li>真正例（TP）：样本的预测与实际标签相同 </li><li>假正例（FP）：样本实际标签不是C，但模型预测成C了 </li><li>假负例（FN）：样本实际标签为C，模型预测错了</li><li>真负例（TN）：样本实际为其他类，模型也预测为其他类</li></ul></blockquote><p> TP： $Conf &gt; P<em>{thresh}$ 且 $IOU &gt; IOU</em>{thresh}$<br> FP： $Conf &gt; P<em>{thresh}$ 且 $IOU &lt; IOU</em>{thresh}$<br> FN： $Conf &lt; P<em>{thresh}$ 且 $IOU &gt; IOU</em>{thresh}$<br> TN： $Conf &lt; P<em>{thresh}$ 且 $IOU &lt; IOU</em>{thresh}$</p><p>式子中，$IOU_{thresh}$为0-1之间的常数，需人工指定。<br>对于某一特定类别，TP、FP、FN、TN 四种预测框的个数构成混淆矩阵（Confusion Matrix）。例如这边用 同济子豪兄大哥的 毕设波磨检测为例。（这里的其他类别指的是背景）<br><img src="https://img-blog.csdnimg.cn/img_convert/222b3b01bcd2ff73f2408bddfc8abf77.png#pic_center" alt="混淆矩阵"><br>进一步定义以下参数：</p><ol><li>Precison(查准率) 是指所有预测框中预测正确的比例，反应了模型“不把背景冤枉成目标”的准确性。<br><img src="https://img-blog.csdnimg.cn/img_convert/f3ae30d6a98aeb65efad9cbf82b5123e.png#pic_center" alt="在这里插入图片描述">    </li><li>Recall(查全率、敏感性、召回率)是指所有 应该被预测出来的标准框 中被正确预测的比例，反应了模型“不把目标放过为背景”的敏感性。<br><img src="https://img-blog.csdnimg.cn/img_convert/cfb0d9475b56951b1d78def0e79655a9.png#pic_center" alt="在这里插入图片描述"></li><li>Average Precision（平均精度，简称AP）：<br>将 $P<em>{thrshold}$ 阈值从0到1变化，计算每个  $P</em>{threshold}$ 阈值对应的Precision和Recall，绘制成某类别的PR性能曲线，其围成的面积为该类别的AP。<br><img src="https://img-blog.csdnimg.cn/img_convert/553b6e044b25f0efbf068d4794dedb68.png#pic_center" alt="比如猫的AP"></li></ol><p>取所有类别的AP和不同的 $IOU<em>{thresh}$ ，可分别计算 mAP@0.5 和 mAP@0.5:0.95 。                     mAP@0.5为 $IOU</em>{thresh}$ 取0.5时，各类别AP的平均值。 mAP@0.5:0.95为 $IOU_{thresh}$ 分别取以0.05为步长，从0.5增大到0.95的10个数时，各类别AP的平均值。如下式所示，j分别取 0.5、0.55、0.6、0.65、0.7、0.75、0.8、0.85、0.9、0.95，N为类别总数20（类别总数）。</p><script type="math/tex; mode=display">AP = \int_{0}^{1}P_{thresh}(r) dr</script><script type="math/tex; mode=display">mAP@0.5=\frac{1}{N}\sum_{i=1}^{N}AP_{i}(IOU_{thresh}=0.5)</script><script type="math/tex; mode=display">mAP@0.5:0.95=\frac{1}{N}\sum_{i=1}^{N}\sum_{j}AP_{i}(IOU_{thresh}=j)</script>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;如何评价目标检测的性能 MAP如何计算等&quot;&gt;TOC&lt;/a&gt;&lt;br&gt;目标检测模型的性能指标分为速度指标和准确度指标。速度指标通常用每秒推断帧数 FPS （Frames Per Second）衡量，但受硬件影响较大。&lt;/p&gt;
&lt;p&gt;目标检测输入图像，输出图像</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="目标检测" scheme="http://example.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>cv2 各类函数 详解</title>
    <link href="http://example.com/2021/10/25/cv2%20%E5%90%84%E7%B1%BB%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2021/10/25/cv2%20%E5%90%84%E7%B1%BB%E5%87%BD%E6%95%B0%20%E8%AF%A6%E8%A7%A3/</id>
    <published>2021-10-25T14:09:01.000Z</published>
    <updated>2021-10-29T15:03:40.074Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="cv2 各类函数 详解">TOC</a></p><h1 id="cv2-rectangle-在任何图像上绘制矩形"><a href="#cv2-rectangle-在任何图像上绘制矩形" class="headerlink" title="cv2.rectangle  在任何图像上绘制矩形"></a>cv2.rectangle  在任何图像上绘制矩形</h1><p>用法： cv2.rectangle(image, start_point, end_point, color, thickness) 参数：</p><blockquote><p>image:它是要在其上绘制矩形的图像。 start_point：它是矩形的起始坐标。坐标表示为两个值的元组，即(X坐标值，Y坐标值)。<br>end_point：它是矩形的结束坐标。坐标表示为两个值的元组，即(X坐标值ÿ坐标值)。<br>color:它是要绘制的矩形的边界线的颜色。对于BGR，我们通过一个元组。例如：(255，0，0)为蓝色。<br>thickness:它是矩形边框线的粗细像素。厚度-1像素将以指定的颜色填充矩形形状。 返回值：它返回一个图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.rectangle(img,(x,y),(x+w,y+h),color,<span class="number">8</span>)</span><br></pre></td></tr></table></figure></blockquote><h1 id="cv2-putText-在图像上加字"><a href="#cv2-putText-在图像上加字" class="headerlink" title="cv2.putText 在图像上加字"></a>cv2.putText 在图像上加字</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">string = <span class="string">&#x27;&#123;&#125;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(class_names[i],confidence)</span><br><span class="line">cv2.putText(img,string,(x,y+<span class="number">20</span>),cv2.FONT_HERSHEY_PLAIN,<span class="number">3</span>,(<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>),<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>各参数依次为：图片，添加的文字，左上角坐标，字体，字体大小，颜色，字体粗细</p><p>其中字体可以选择：<br>FONT_HERSHEY_SIMPLEX 、 FONT_HERSHEY_PLAIN、 FONT_HERSHEY_DUPLEX 等</p><h1 id="cv2-dnn-readNet-用于读取网络参数并构建网络"><a href="#cv2-dnn-readNet-用于读取网络参数并构建网络" class="headerlink" title="cv2.dnn.readNet 用于读取网络参数并构建网络"></a>cv2.dnn.readNet 用于读取网络参数并构建网络</h1><p>注意 cv2的dnn 网络库集合了多种网络<br>具体可以查看这个 博客 <a href="https://blog.csdn.net/Hellow_RMB/article/details/110070686">OpenCV中DNN支持的网络架构</a><br>如：读取 yolov3的配置文件与网络参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = cv2.dnn.readNet(<span class="string">&#x27;yolov3.weights&#x27;</span>,<span class="string">&#x27;yolov3.cfg&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="cv2-imread-用于读取图片文件"><a href="#cv2-imread-用于读取图片文件" class="headerlink" title="cv2.imread 用于读取图片文件"></a>cv2.imread 用于读取图片文件</h1><p>imread函数有两个参数。<br>第一个参数是图片路径，第二个参数表示读取图片的形式，有三种：</p><ul><li>cv2.IMREAD_COLOR：加载彩色图片，这个是默认参数，可以直接写1。<br>cv2.IMREAD_GRAYSCALE：以灰度模式加载图片，可以直接写0。<br>cv2.IMREAD_UNCHANGED：包括alpha，可以直接写-1</li></ul><p>cv2.imread()读取图片后已多维数组的形式保存图片信息，前两维表示图片的像素坐标，最后一维表示图片的通道索引，具体图像的通道数由图片的格式来决定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入图像 默认彩色</span></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;guoge_and_ shark.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="cv2-imwrite-保存图像"><a href="#cv2-imwrite-保存图像" class="headerlink" title="cv2.imwrite  保存图像"></a>cv2.imwrite  保存图像</h1><p>cv2.imwrite(file，img，num)保存一个图像。</p><blockquote><p>第一个参数是要保存的文件名，第二个参数是要保存的图像。<br>可选的第三个参数，它针对特定的格式：<br>对于JPEG，其表示的是图像的质量，用0 -100的整数表示，默认95;<br>对于png ,第三个参数表示的是压缩级别。默认为3.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存图片</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;result-guoge.jpg&#x27;</span>,img)</span><br></pre></td></tr></table></figure><h1 id="cv2-imshow-显示图像"><a href="#cv2-imshow-显示图像" class="headerlink" title="cv2.imshow()  显示图像"></a>cv2.imshow()  显示图像</h1><p>cv2.imshow(窗口名字，图像名称) 显示图像。<br>窗口会自动调整为图像大小。第一个参数是窗口的名字，其次才是我们的图像。你可以创建多个窗口，只要你喜欢，但是必须给他们不同的名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.imwrite(<span class="string">&#x27;lena.png&#x27;</span>,img)</span><br></pre></td></tr></table></figure><h1 id="cv2-dnn-blobFromImage-对图像进行预处理，包括减均值，比例缩放，裁剪，交换通道等"><a href="#cv2-dnn-blobFromImage-对图像进行预处理，包括减均值，比例缩放，裁剪，交换通道等" class="headerlink" title="cv2.dnn.blobFromImage 对图像进行预处理，包括减均值，比例缩放，裁剪，交换通道等"></a>cv2.dnn.blobFromImage 对图像进行预处理，包括减均值，比例缩放，裁剪，交换通道等</h1><blockquote><p>参数：<br>image:输入图像（1、3或者4通道）<br>———————————————<br>可选参数 ：<br>scalefactor:图像各通道数值的缩放比例<br>size:输出图像的空间尺寸,如size=(200,300)表示高h=300,宽w=200<br>mean:用于各通道减去的值，以降低光照的影响<br> swapRB:交换RB通道，默认为False.(cv2.imread读取的是彩图是BGR通道，正常图片都为RGB)<br>crop:图像裁剪,默认为False.当值为True时，先按比例缩放，然后从中心裁剪成size尺寸<br>ddepth:输出的图像深度，可选CV_32F 或者 CV_8U.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对图像预处理</span></span><br><span class="line"><span class="comment"># 1/255把所有像素归一化 除以255 （0，0，0）表示对每个像素RGB减去常数 这里不减</span></span><br><span class="line"><span class="comment"># 因为opencv读入的为BGR 所以我们这里要反一下 swapRB=True crop不对图片进行裁剪</span></span><br><span class="line">blob = cv2.dnn.blobFromImage(img,<span class="number">1</span>/<span class="number">255</span>,(<span class="number">416</span>,<span class="number">416</span>),(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>),swapRB=<span class="literal">True</span>,crop=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h1 id="cv2-dnn-NMSBoxes-非极大值抑制法"><a href="#cv2-dnn-NMSBoxes-非极大值抑制法" class="headerlink" title="cv2.dnn.NMSBoxes 非极大值抑制法"></a>cv2.dnn.NMSBoxes 非极大值抑制法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CONF_THRES = <span class="number">0.1</span> <span class="comment">#指定置信度阈值，阈值越大，置信度过滤越强</span></span><br><span class="line">NMS_THRES = <span class="number">0.4</span> <span class="comment">#指定NMS阈值，阈值越小，NMS越强</span></span><br><span class="line"></span><br><span class="line">indexes = cv2.dnn.NMSBoxes(boxes,confidences,CONF_THRES,NMS_THRES)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;cv2 各类函数 详解&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;cv2-rectangle-在任何图像上绘制矩形&quot;&gt;&lt;a href=&quot;#cv2-rectangle-在任何图像上绘制矩形&quot; class=&quot;headerlink&quot; title=&quot;cv2.re</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="目标检测，OpenCv" scheme="http://example.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%8COpenCv/"/>
    
  </entry>
  
  <entry>
    <title>yolov3 详解</title>
    <link href="http://example.com/2021/10/23/yolov3%20%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2021/10/23/yolov3%20%E8%AF%A6%E8%A7%A3/</id>
    <published>2021-10-23T14:52:01.000Z</published>
    <updated>2021-10-29T15:04:08.518Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="同济子豪兄 之 yolov3 详解">TOC</a></p><h1 id="yolov1v2v3系列的区别"><a href="#yolov1v2v3系列的区别" class="headerlink" title="yolov1v2v3系列的区别"></a>yolov1v2v3系列的区别</h1><p><img src="https://img-blog.csdnimg.cn/img_convert/6c76949fa52cb10efb04a7115232ec10.png" alt="yolov1v2v3系列的区别"></p><h1 id="yolov3-存在的问题-与-改进"><a href="#yolov3-存在的问题-与-改进" class="headerlink" title="yolov3 存在的问题 与 改进"></a>yolov3 存在的问题 与 改进</h1><blockquote><p>如何评判目标检测的效果好坏，以及AP如何计算？ 请看另一文章<br><a href="http://www.baidu.com">评判目标检测性能以及如何计算AP等</a></p></blockquote><p>yolov3 在 IoU 阈值为0.5的情况下，AP值较好（也就是框框并不需要和 Ground Truth 贴合的很准，条件比较宽松）。<br>但 IoU 阈值在0.5-0.95，以0.05作为步长的1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0个阶段，得到的 AP平均值 就不佳， 也就是说明 yolov3在<strong>精准定位对象的性能上仍然是比较差的</strong>，但是换来的FPS是比较不错的。</p><p>在检测 小目标或密集目标 能力上有所改进。</p><ol><li>grid cell 个数增加（v2 和 v3 都可以兼容任意图片大小的输入） 能检测对象的预测框多了。v2和v3 你输入的图像越大，那能用于检测对象的 预测框也就越多</li><li>Anchor   预先设置了一些比较小的anchor。 v1中直接生成小目标的预测框是比较难的，但在小anchor的基础上计算预测框容易的多</li><li>多尺度预测（FPN） 例如 52×52这个大尺度的，感受野就对应于小目标。FPN 既提取了深层网络中特化语义信息，并结合了浅层细粒度像素结构信息（边缘轮廓转角等）。对于小目标而言，边缘和轮廓等细粒度信息是很重要的。</li><li>损失函数 含有 惩罚小框项</li><li>网络结构（骨干网络 跨层链接 残差链接）</li></ol><h1 id="yolov3-相较于-v2-的改进-（即细节）"><a href="#yolov3-相较于-v2-的改进-（即细节）" class="headerlink" title="yolov3 相较于 v2 的改进 （即细节）"></a>yolov3 相较于 v2 的改进 （即细节）</h1><p>yolov2 存在的问题是,对小物体的检测精度还是不高。<br>毕竟，一张图片经过卷积层（特征提取）到最后的特征图，包含小物体的信息是会有损失的。</p><h2 id="多尺度输出"><a href="#多尺度输出" class="headerlink" title="多尺度输出"></a>多尺度输出</h2><p>在 yolov3 中，共设置9个宽高不同的anchor（同样是通过 K-means 聚类获取得到），每个 grid cell 的 anchor 的数量为 9/3=3 个，因为 yolov3 有3个feature_map，不同feature_map的size和感受野是不一样的，<strong>较小size的feature_map具有较大的感受野</strong>，所以负责检测较大的物体，同理，<strong>较大size的feature_map有较小的感受野</strong>，负责检测较小的物体。如下图举个例子：</p><blockquote><p><img src="https://img-blog.csdnimg.cn/img_convert/c87f61996eac9a9575d77e7adb179aef.png#pic_center" alt="yolov3 的整个网络模型"><br>上面的就是 yolov3 的整个网络模型，输出的一共有3种尺度（255 = 3×(5+80) 因为用的COCO数据集）。对于第一个 13×13×255 的输出来说，416（输入尺寸）÷ 13(输出尺寸) = 32（也就是 下采样 32），<strong>换句话说就是 对于13×13的特征图，单元网格1×1×255的特征向量 对应 输入原图 32×32×3 的信息</strong> （感受野最大，所以负责检测较大的物体）<br><img src="https://img-blog.csdnimg.cn/img_convert/cb17483efe5e14f32454f709268f27bf.png#pic_center" alt="在这里插入图片描述"><br>用如上图的例子可以看出，<strong>小特征图有大的anchor box，适合检测大物体</strong>（你可以这么理解，对于大物体我们其实只要很粗糙的像素内容就可以认出来，比如我们把一张小马图片用切片机去掉几行几列，我们还是认得出来这是匹马；而对于小物体，我们确实需要更大的特征图，因为其更能保留细度的特征，你也可以这么理解就是我裁剪的太过分了，小物体主体都快被我裁完了怎么可能认得出来）</p></blockquote><h2 id="输入图片-等比例变化"><a href="#输入图片-等比例变化" class="headerlink" title="输入图片 等比例变化"></a>输入图片 等比例变化</h2><p>v2的时候采取了多尺度训练的方式，并且使用了一个GAP层来帮助解决输入图片尺寸不一样的情况。<br>v3对输入图片本身做了一定的处理。<br>一般对图形的处理有两种方式：</p><ul><li>第一种就是把图片不管比例直接变成这边的 416×416 分辨率；</li><li>第二种就是先设定一个 416×416 的黑框，然后将图片等比例变化后放入黑框中，在训练和预测的时候黑色的部分会被当做成背景，不会对结果有什么影响。</li></ul><p>在实际操作中这两种方法都是可行的。</p><h2 id="yolov3的-预测框计算机制-未变"><a href="#yolov3的-预测框计算机制-未变" class="headerlink" title="yolov3的 预测框计算机制 未变"></a>yolov3的 预测框计算机制 未变</h2><p>yolov3的预测框，预测出来的方法与yolov2是<strong>一致的</strong>。</p><p><font color="red">训练过程中，都是看实际框中心点落在哪个grid cell，拿出这个grid cell 的所有anchor box 看实际框与哪个anchor box 的IoU最大</font>  <font color="green">（注意：是两个框中心点对其后的IoU）</font>，就选择这个anchor box 作为模板。 <font color="blue">然后根据这个anchor box 进行校准</font>。 </p><p><font color="green">注意： $t<em>{x}$ 这些才是我们网络的输出，也就是这边的 $b</em>{x}$ 我们是命令一个新的函数对其操作得到的。所以ground truth 也要将 长宽中心点这些 $b<em>{i}$ 调整成 $t</em>{i}$ 来进行计算损失</font><br>如下图所示： 其中 $c<em>{x}$ 和 $c</em>{y}$ 表示单元格长度（分别为 +2 和 +1）<br>注意：在实际运算中，为了运算方便，$b<em>{x}$ 与 $b</em>{x}^{‘}$ 都指的是<font color="red">预测框中心点到图像边界的距离</font>，而不是相对于到单元格的距离。<br>另外 因为 $c<em>{x}$和$c</em>{y}$都是以1为单位的（也就是下采样过），所以我们计算得到的 $b<em>{x}$ 与 $b</em>{y}$ 其实都是<strong>相对量</strong>，（$b<em>{h}$与$b</em>{w}$是正确尺寸的值）所以我们在最后将框还原到原图尺寸的时候，中心点需乘以grid cell 的下采样倍数，得到原始图像输入图像上预测框 中心点 的坐标。<br><img src="https://img-blog.csdnimg.cn/img_convert/9e6dfd393abd6cb189a7cbf246524bd2.png#pic_center" alt="预测框位置计算方法"></p><h2 id="yolov3-Anchor-box对应机制"><a href="#yolov3-Anchor-box对应机制" class="headerlink" title="yolov3 Anchor box对应机制"></a>yolov3 Anchor box对应机制</h2><p>注： yolov3 中使用的 anchor box 的数量是3个<br>如下图所示，每个 grid cell 都可以有3个 anchor box，那样就可以达到如下满满当当的效果，也就是可以检测出更多的物体对象。<br><img src="https://img-blog.csdnimg.cn/img_convert/cdd82c32539d5b3a137afa3f7abad3d2.png#pic_center" alt="每个grid cell 可以有 3个 anchor box"></p><h2 id="yolov3-在置信度上与yolov1和v2的区别"><a href="#yolov3-在置信度上与yolov1和v2的区别" class="headerlink" title="yolov3 在置信度上与yolov1和v2的区别"></a>yolov3 在置信度上与yolov1和v2的区别</h2><blockquote><p><strong>在 yolov1和v2中使用IoU作为置信度标签</strong>有何不好的地方？<br> <font color="blue">v1和v2中  $P(object)* IoU$ 来计算置信度标签 </font>，对于ground truth $P{object}=1$ 所以 就是使用 IoU最为置信度标签。这样的坏处在：<br>1.很多预测框与 ground truth 的IoU最高只有0.7（最好的只有70分）也就是置信度最高只有0.7. 换句话就是我最高就0.7了，我网络怎么学也很难超过0.7.<br>2.COCO中的小目标，IoU对像素偏移很敏感，无法有效学习</p></blockquote><p>yolov3 中使用逻辑回归来计算每个预测框的置信度，每个预测框标签的置信度均为1 （告诉网络我这个就是正样本，你得给我好好学。）<strong>正样本(也就是和GT IoU最大的那个anchor 校准得到的预测框)的置信度规定为1，负样本的置信度规定为0</strong>。下面可以参考一下什么是正负样本：<br><img src="https://img-blog.csdnimg.cn/img_convert/c7b5415b0d72118add3de4035995175a.png#pic_center" alt="正样本、不参与 与 负样本"><br>正样本：与ground truth IoU最大的anchor 校准的预测框<br>不参与（不负责预测）：anchor与ground truth IoU大于阈值（论文是0.5）但是 不是最大的  就什么也不是 。 其实本来就没考虑它，你也不是最大的。<br>负样本：anchor 与ground truth IoU 小于阈值（论文是0.5） 的  或者完全就不重叠的anchor。 其实本来也就没考虑它。</p><h2 id="具体-如何预测-预测框"><a href="#具体-如何预测-预测框" class="headerlink" title="具体 如何预测 预测框"></a>具体 如何预测 预测框</h2><p>yolov3的预测框，预测出来的方法与yolov2是<strong>一致的</strong>。 <strong>每个GT仅仅分配一个anchor（也就是一个预测框）负责预测</strong>，这个是相对于</p><p><font color="red">训练过程中，都是看实际框中心点落在哪个grid cell，拿出这个grid cell 的所有anchor box 看实际框与哪个anchor box 的IoU最大<font color="green">（注意：是两个框中心点对其后的IoU）</font>，就选择这个anchor box 作为模板。&lt;/font&gt; 然后对其进行训练修正（上面讲的计算偏移机制），<font color="green">注意：$t<em>{x}$ 这些才是我们网络的输出，也就是这边的 $b</em>{x}$我们是命令一个新的函数对其操作得到的。所以ground truth 也要将 长宽中心点这些 $b<em>{i}$调整成 $t</em>{i}$ 来进行计算损失</font>，调整模型参数后 得到最佳的预测框。</p><h2 id="yolov3的网络结构-变化"><a href="#yolov3的网络结构-变化" class="headerlink" title="yolov3的网络结构 变化"></a>yolov3的网络结构 变化</h2><p>最后 应该是 255维度（用的COCO） 注意<br><img src="https://img-blog.csdnimg.cn/img_convert/9e22e1993729ae01cbe5f304f4aff4b0.png#pic_center" alt="yolov3网络结构"></p><h3 id="Backbone-Darknet-53"><a href="#Backbone-Darknet-53" class="headerlink" title="Backbone  Darknet-53"></a>Backbone  Darknet-53</h3><blockquote><p><strong>53的由来</strong><br>步长为2的卷积层有5个  图中<font color="turquoise">蓝色的部分 </font><br>步长为1的卷积层有47个  （47=1+1×2+2×2+8×2+8×2+4×2 ）图中<font color="green">绿色的部分</font><br>最后 的全连接网络层 为1层 含有参数<br>所以一共53层</p><p><font color="red">红色的部分</font> 是残差层 和全连接层之前的下采样层一样都是没有参数的  所以不参与网络层的统计</p></blockquote><p><img src="https://img-blog.csdnimg.cn/img_convert/08afccd5b374fc0bd1119467928a74ed.png#pic_center" alt="Darknet-53分类网络模型"><br>作者构建了darknet-53，在<strong>ImageNet数据集上，进行分类测试</strong>。发现精度和Resnet-101和Resnet-152 精度都差不多，但计算速度比这两者快很多，而且网络层数少很多。</p><p>其中里面的每一个 Convolution 都是由 卷积层+BN层+Leakyrelu激活函数层。 这边统一叫做 CBL<br><img src="https://img-blog.csdnimg.cn/img_convert/652afb5491146ca97dbed44900e85413.png#pic_center" alt="CBL层"></p><h4 id="换句话说的-Darknet-53分类模型结构"><a href="#换句话说的-Darknet-53分类模型结构" class="headerlink" title="换句话说的 Darknet-53分类模型结构"></a>换句话说的 Darknet-53分类模型结构</h4><p><img src="https://img-blog.csdnimg.cn/img_convert/33b1068d6caaf8a31fe6bb30db83901a.png#pic_center" alt="Darknet-53 分类模型结构"><br>包括之前说的各种结构部分 Res4也就是上上图中框出来的那部分。</p><h3 id="Neck-层"><a href="#Neck-层" class="headerlink" title="Neck 层"></a>Neck 层</h3><p>注意 这里的 21 其实是 255 （这个作者 定义的是两种类别 实际是 80种即COCO （1+4+80）×3）<br><img src="https://img-blog.csdnimg.cn/img_convert/9e22e1993729ae01cbe5f304f4aff4b0.png#pic_center" alt="看各层的维度"><br>在主干网络提取出图像特征之后，为了能更好的融合提取的特征，还是用了Neck结构。不过因为最后的输出层有3种特征图（13×13，26×26以及52×52），而主干网络输出的矩阵尺寸为 13×13。所以要经过Neck结构中的 <font color="blue">FPN结构</font> 和前面的 26×26，52×52 特征图进行多尺度融合。</p><p>这里我们对上图种框出的 <font color="violet">上采样结构</font> 做分析：</p><ul><li>上面那条支路 为 骨干网络 最后的输出维度 13×13  通过CBL 再 <strong>上采样</strong>一下 变为了 26×26</li><li>下面那条支路 为 骨干网络种 中途输出的维度 26×26</li><li>所以整合Concat操作  得到还是 26×26的维度  <font color="red">注意他是通道相叠</font><br><img src="https://img-blog.csdnimg.cn/img_convert/245e8edc104a69ed5d3588936d7de912.png#pic_center" alt="在这里插入图片描述"></li></ul><p>同理后面几次操作：<br><img src="https://img-blog.csdnimg.cn/img_convert/334d06804ca1bae824c16ef34334ae28.png#pic_center" alt="后续操作"></p><h2 id="最后输出的-类别的条件概率-理解"><a href="#最后输出的-类别的条件概率-理解" class="headerlink" title="最后输出的 类别的条件概率 理解"></a>最后输出的 类别的条件概率 理解</h2><p>yolov3与v1和v2在 输出的类别条件概率的想法是一致的，都采用的是 各类别独立的逻辑回归（即多分类标签），每个类别的最大概率都为1，不互相干扰都互相独立。 也就是不适用softmax，因为softmax的话，输出的结果就是每个类别条件概率之和为1。</p><p>对于yolov3 每个预测框的每个类别逐一用逻辑回归输出概率，可有多个类别输出高概率。</p><p>拿分类误差，使用的损失函数，那肯定也是 二分类交叉熵损失函数（因为我每个类是分开单独考虑的 逻辑回归）</p><h1 id="yolov3的训练过程"><a href="#yolov3的训练过程" class="headerlink" title="yolov3的训练过程"></a>yolov3的训练过程</h1><p><img src="https://img-blog.csdnimg.cn/img_convert/0432e7404ad9328cd5878ca4cf4b9999.png#pic_center" alt="yolov3 训练过程"></p><p>输入416×416大小的图像，在经过主干网络darknet-53的卷积网络以及Neck中的FPN结构。可以得到输出层(P0、P1、P2)三种特征图，P0的尺寸最大为52×52，P1是中间尺寸为26×26，P2尺寸最小为13×13。</p><p>特征图上每个单元格的特征向量都可以对应416×416图像上的一块感受野区域，而每个区域都会生成三个初始anchor box，因此网络输出的 $t<em>{x} 、t</em>{y}、t<em>{h}、t</em>{w}$ 可以利用anchor，计算得到预测框。再加上前景背景的概率obj以及类别概率cls，就可以汇总得到，目标检测网络输出的预测信息。</p><p>但从监督学习的角度，我们会对训练的样本进行标注，因此我们可以知道，图片上每一个物体，它实际框的位置和类别。所以可以根据，前面讲的anchor的对应机制，分别对应到各自anchor上，并打上类别的标签。这样，就可以将预测框和 ground truth 的信息，对应关联起来。</p><p>从P0、P1、P2，三个特征图的角度，对前景和背景的概率以及Location位置信息，和class类别信息。从这三个方面，来计算预测框和ground truth 之间的偏差即损失函数，而总体的损失函数，等于三个特征图的损失函数之和。<br><img src="https://img-blog.csdnimg.cn/img_convert/086ebe71480140b2b7d74addd16dfec2.png#pic_center" alt="yolov3 损失函数"></p><p>当有了损失函数，就可以利用网络的梯度更新方式，来进行反向传播了，从而不断迭代，更新网络中的参数。使得损失函数的值，越来越小，从而越来越准确。</p><h2 id="详细-损失函数-介绍"><a href="#详细-损失函数-介绍" class="headerlink" title="详细 损失函数 介绍"></a>详细 损失函数 介绍</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/086ebe71480140b2b7d74addd16dfec2.png#pic_center" alt="yolov3 损失函数"><br>yolov3的损失函数主要分为3部分： 分类误差，定位误差，置信度误差</p><ul><li>正样本对 分类误差、定位误差、置信度误差 学习产生贡献</li><li>对不负责预测的框（其实本来就不考虑） 置信度为0 所以 它啥也不算</li><li><font color="blue">负样本只对置信度误差 学习产生贡献 </font> <font color="violet">为什么呢？ 负样本的置信度不是 规定为0了么？ 为什么会对置信度误差产生贡献呢？？？</font>  <h1 id="yolov3的测试过程"><a href="#yolov3的测试过程" class="headerlink" title="yolov3的测试过程"></a>yolov3的测试过程</h1></li></ul><p>yolov3的测试过程主要分为两步。<br><img src="https://img-blog.csdnimg.cn/img_convert/89a8c6292269e80936b9a1415dd918de.png#pic_center" alt="yolov3 的测试过程"></p><ul><li>第一步，先通过网络的$t<em>{x}、t</em>{y}、t_{h}、t{w}$等输出向量，计算处预测框的位置和所属的类别信息。注意预测框的得分应该为  <strong>前景和背景的概率 × class 类别的得分</strong>。<ul><li>并设置阈值分数 例如为0.3。将预测框分数，大于0.3的都保留下来。而分数比较低的都过滤掉，这样的好处是保留的框既是前景即目标的框，同时也是类别分数，比较大的目标。</li><li>第一步如上，得到了三个特征图预测框的信息<ul><li>但现在的框，还是太多了。因此我们需要在一定的标准下去评判这些目标框信息。所以第二步 我们需要将三个特征图上的结果，全部映射回416×416输入图像上。这时，在416×416的图片上，就有多种类别很多的框了。</li><li>这时需要针对每个类别，做一个NMS，即非极大值抑制处理，消除重叠度很高的框。这样到了最后，我们就可以得到最终的预测框的信息，以及得分，从而完成目标的检测和定位。</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;同济子豪兄 之 yolov3 详解&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;yolov1v2v3系列的区别&quot;&gt;&lt;a href=&quot;#yolov1v2v3系列的区别&quot; class=&quot;headerlink&quot; title=&quot;yolov1v2v3系列的区别&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="http://example.com/tags/yolo/"/>
    
    <category term="目标检测" scheme="http://example.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>同济子豪兄 之 yolov2 详解</title>
    <link href="http://example.com/2021/10/08/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov2%20%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2021/10/08/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov2%20%E8%AF%A6%E8%A7%A3/</id>
    <published>2021-10-08T07:09:01.000Z</published>
    <updated>2021-10-29T15:04:20.352Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="同济子豪兄 之 yolov2 详解">TOC</a></p><font color="blue">蓝色： 表示还不明白什么意思</font> <p>v1存在 一些性能和原理上的问题：</p><ol><li>mAP相比 R-CNN 系列比较低</li><li>定位性能比较差，定位错误占总错误的比例很大</li><li>Recall比较低，就是把全部目标全部检测出来的能力比较差</li><li>检测密集和小目标的能力比较差</li></ol><p>这篇文章其实主要有两个模型，即Yolov2 和 Yolo9000. 9000这个只是一个想法，不太实用。<br>Yolov2 是在此基础上做了很多的 tricks，作者分别归为了三个类，分别是Better（更准确），Faster（更快的），Stronger（类别更多的）。其中Stronger这块 作者说他可以预测9000种，但其实效果不好的，他只是提供一个想法。我们的重点在 前面的Better（更准确）和Faster（更快）（换用了Darknet-19网络）上。</p><blockquote><p>总结来看，虽然YOLOv2做的改进，基本都是借鉴其它论文的一些tricks，比如Faster R-CNN的anchor box，YOLOv2采用anchor box和卷积做预测，这基本上与SSD模型（单尺度特征图的SSD）非常类似了，而且SSD也是借鉴了Faster R-CNN的RPN网络。<br>从某种意义上来说，YOLOv2和SSD这两个one-stage模型与RPN网络本质上无异，<strong>只不过RPN不做类别的预测，只是简单地区分物体与背景</strong>。在two-stage方法中，RPN起到的作用是给出region proposals，其实就是作出粗糙的检测，所以另外增加了一个stage，即采用R-CNN网络来进一步提升检测的准确度（包括给出类别预测）。<br>而对于one-stage方法，它们想要一步到位，直接采用“RPN”网络作出精确的预测，要因此要在网络设计上做很多的tricks。<br>YOLOv2的一大创新是采用Multi-Scale Training策略，这样同一个模型其实就可以适应多种大小的图片了。</p></blockquote><p>提问：他的模型是 首先是个分类模型（带 global average pooling）可以实现输入多维尺度 这个网络不是用来检测的<br>但他又调整 这个网络，去掉了GAP 然后加了3个卷积 和 passthrough 来做检测 ？？== 那我之前分类的意义在哪？ </p><font color="red">答：它相当于就是分类对模型做了个预训练，先大致确定好参数。然后再去检测训练的时候fine-tune，可以加快训练的速度，并且提升实际效果（实际上这个就是有用，但不知道为什么有用呢）</font><h1 id="Better（更准确）"><a href="#Better（更准确）" class="headerlink" title="Better（更准确）"></a>Better（更准确）</h1><p>作者为使yolo的精度更高，使用了很多个tricks。包括<br><strong>Batch Normalization</strong>、High Resolution Classifer、<strong>Anchor</strong>、Dimension Cluster、Direct location prediction、Fine-Graind Features、Muti-Scale Training</p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p><blockquote><p>Batch Normalization 的详细内容请看<br><a href="https://jks88995656.github.io/2021/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20-%20Batch%20Normalization/">一只柴犬 深度学习基础 - Batch Normalization </a></p></blockquote><h2 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h2><p>目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分（CNN特征提取器），由于历史原因，ImageNet分类模型基本采用大小为 224×224 的图片作为输入，分辨率相对较低，不利于检测模型。</p><p>  Yolov1中 </p><ul><li>所以 YOLOv1 在采用 224×224 分类模型预训练后，将分辨率增加至448×448，并使用这个高分辨率在检测数据集上finetune。<strong>但是直接切换分辨率，检测模型可能难以快速适应高分辨率。</strong></li></ul><p><img src="https://img-blog.csdnimg.cn/img_convert/803c6a4d557bf242831be696587816d0.png#pic_center" alt="Yolov1中作者模型训练和测试 输入图片大小情况"></p><p> Yolov2中</p><ul><li><strong>YOLOv2增加了在ImageNet数据集上使用 448×448 输入来finetune分类网络这一中间过程（10 epochs）</strong>（也就是他先是224×224 分辨率训练了一会，再用 448×448<br>分辨率上训练了10个epoch，），这可以使得模型在检测数据集上finetune之前已经适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</li></ul><h2 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a>Convolutional With Anchor Boxes</h2><p>  Yolov1中</p><ul><li>在YOLOv1中，输入图片最终被划分为 7×7 个 grid cell，每个grid cell预测2个边界框（bounding box）。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中<strong>存在不同尺度和长宽比（scales and ratios）的物体</strong>，YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在<strong>精确定位方面表现较差</strong>。In another word, v1中并没有对框和对象的关系做匹配，也就是我的框可能都是长的或者都是宽的没个标准，这样匹配起来就比较麻烦。另外Yolov1 左上角的bounding box 有可能预测在右下角，这就导致了训练起来非常不稳定，需要加以限制。<br><img src="https://img-blog.csdnimg.cn/img_convert/4059677c62015c45885ecfff1025240b.png" alt="v1中的bounding box 野蛮生长"><br>比如上图所示，长的那个符合人这个物体，但对车就不太符合了。</li><li>对于YOLOv1，每个grid cell都预测2个 bounding box，每个box包含5个值： (中心点横坐标，中心店纵坐标，框宽度，框高度，置信度)，最后一个置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。<strong>但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。</strong></li></ul><p> Yolov2</p><ul><li>v2中引入了先验参考框这个概念。（其实可以理解为一个公共的模板bounding box集合，生成的bounding box 都是这个集合里的）所有的预测框其实都是 先验参考框的<strong>偏移</strong>。每个anchor都对应一个预测框，每个预测框只要 预测出 其相对于Anchor的偏移量。例如下图所示，就是5个先验参考框：<br><img src="https://img-blog.csdnimg.cn/img_convert/0152b5d1eba3e64713e8bf34f771fc17.png#pic_center" alt="5个先验参考框（anchor box）"></li><li><p>所以<strong>YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框(bounding box)</strong>。为了使检测所用的特征图分辨率更高，<strong>移除</strong>其中的一个<strong>pool层</strong>。在检测模型中，YOLOv2不是采用 448×448 图片作为输入，而是采用 <strong>416×416 大小</strong>。因为YOLOv2模型下采样的总步长为 32，对于 416×416 大小的图片，<font color="red">最终得到的特征图大小为 13×13 ，维度是<strong>奇数</strong></font>，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。<strong>所以在YOLOv2设计中要保证最终的特征图有奇数个位置。</strong><br><img src="https://img-blog.csdnimg.cn/img_convert/6b72bf553115ff5febd1abb903f7d010.png#pic_center" alt="使用anchor box 假如为2（因为其实就2类物体）"></p></li><li><p>YOLOv2使用了anchor box之后，每个位置的各个anchor box都单独预测一套分类概率值，这和SSD比较类似（但SSD没有预测置信度，而是把background作为一个类别来处理）。</p></li><li>在YOLOv2中，<strong>每个grid cell 预测 5个bounding box</strong> （也就是对应的 5个anchor box）。为什么选5呢后面会说（用的聚类）。所以网络结构输出的特征结果 变为了 <strong>13×13×（5+20）× 5  = 13×13×125</strong> ，不是一个grid cell 的bounding box 共享一套分类概率值（条件概率），现在是每个bounding box 都要有自己单独的一套。如下图所示：<br><img src="https://img-blog.csdnimg.cn/img_convert/a56fa6771fcd8496670697f9005ee125.png" alt="v1与v2特征输出的区别"></li></ul><h3 id="anchor-box-在v2的效果"><a href="#anchor-box-在v2的效果" class="headerlink" title="anchor box 在v2的效果"></a>anchor box 在v2的效果</h3><p>使用anchor boxes之后，<strong>YOLOv2的mAP有稍微下降</strong>（这里下降的原因，有博主猜想是YOLOv2虽然使用了anchor boxes，但是依然采用YOLOv1的训练方法）。YOLOv1只能预测98个边界框（ 7×7×2 ），而YOLOv2使用anchor box之后可以预测更多个边界框（ 13×13×5 ）。所以使用anchor box之后，<strong>YOLOv2的召回率recall大大提升，由原来的81%升至88%</strong>。也就是说yolo检测出物体的能力更强了，但准确性下降了一些。</p><h2 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h2><p> 在Faster R-CNN和SSD中，先验框的维度（长和宽）都是<strong>手动设定</strong>的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。</p><font color="red">这里的这个功能就是 为了选取 对于所训练的数据集，我选择anchor box 为几个？ 才能最好的效果。</font><blockquote><p>那这里 总结一下 v2为什么选择 anchor box =5 呢？<br>答：<strong>所谓的anchor box 的数量 说白了 就是 你的数据集到底有多少个类</strong>，把物体对象框大小差不多的归为一类。<br>v2 采用 的是k-means聚类方法（我们用这个方法确定了<strong>anchor的数量以及长宽比</strong>）对训练集中的边界框标签做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标：</p><script type="math/tex; mode=display">d(box,centroid) = 1 - IoU(box,centroid)</script><font color="green">IoU这部分表示数据集中的某个ground truth框和他所在的聚类中心框的IoU  这个越大说明我的聚类中心选的越好（接近于1 刚好把所有样本分散的好）</font>在**VOC 2007 和COCO数据集上的聚类分析结果**，随着聚类中心数目的增加，平均IOU值（各个边界框与聚类中心的IOU的平均值）是增加的，但是综合考虑模型复杂度和召回率，作者最终选取**5个聚类中心作为先验框** (也就是 anchor box = 5)但是这里先验框的大小具体指什么作者并没有说明，但肯定不是像素点，从代码实现上看，应该是相对于预测的特征图大小（ 13×13）。对比两个数据集，也可以看到COCO数据集上的物体相对小点。这个策略作者并没有单独做实验，但是作者对比了采用聚类分析得到的先验框与手动设置的先验框在平均IOU上的差异，发现前者的平均IOU值更高，因此模型更容易训练学习。 ![数据集VOC和COCO上的边界框聚类分析结果  右侧中 黑框是voc2007 的 长宽比聚类  蓝色的是 coco 的长宽比聚类](https://img-blog.csdnimg.cn/img_convert/5f91f5d74220601c5e3f6d7cb34b81c1.png#pic_center)注：anchor 的长宽比才有意义，至于图上所示，其位置在哪无所谓，其位置没有任何的意义（这里作者调整过了，怕叠在一起不好看）<font color="blue">聚类中心怎么看的呢？这边的先验框大小到底是什么意思呢？</font></blockquote><h2 id="Direct-location-prediction-直接位置预测"><a href="#Direct-location-prediction-直接位置预测" class="headerlink" title="Direct location prediction 直接位置预测"></a>Direct location prediction 直接位置预测</h2><font color="green">Direct location prediction 可以解决v1中bounding box 乱窜野蛮生长的问题（比如：左上角的grid cell 的bounding box 在右下角 偏移量有点大。。）</font><font color="red">**已知 YOLOv2借鉴RPN网络使用anchor box来预测边界框bounding box相对先验框anchor box的偏移量offsets。**</font> 预测边界框 bounding box 的中心位置 $(x,y)$ ，需要根据预测的坐标偏移量<font color="blue">（感觉是个比例）</font> $(t_{x},t_{y})$，先验框 anchor box 的宽高 $(w_{a},h_{a})$ 以及Anchor 中心坐标 $(x_{a},y_{a})$ （特征图（边长是奇数）每个位置的中心点）来计算：![faster R-CNN 中计算预测框的中心坐标的方法](https://img-blog.csdnimg.cn/img_convert/fda3e2548e0e8778e2506eb6bbbdbe56.png#pic_center)这个公式是**无约束的，预测的边界框很容易向任何方向偏移**，如当 $t_{x}=1$ 时边界框将向右偏移先验框的一个宽度大小，而当 $t_{x}=-1$ 时边界框将向左偏移先验框的一个宽度大小，因此每个位置**预测的边界框可以落在图片任何位置**，$t_{x},t_{y}$没有约束而可能移动幅度过大，这导致模型的**不稳定性**，在训练时需要很长时间来预测出正确的offsets。所以，YOLOv2弃用了这种预测方式，而是沿用YOLOv1的方法，就是**预测边界框中心点相对于对应grid cell左上角位置$(c_{x},c_{y})$的相对偏移值**，<font color="blue">为了将边界框 bounding box 中心点约束在当前cell中 </font>，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets $t_{x},t_{y},t_{w},t_{h}$ ，可以按如下公式计算出边界框实际位置和大小$(b_{x},b_{y},b_{w},b_{h})$：![](https://img-blog.csdnimg.cn/img_convert/920c51b2db6727365725e3fb831d027e.png#pic_center)其中 $(c_{x},c_{y})$ 为grid cell的左上角坐标，如下图所示。![边界框位置与大小的计算示例图](https://img-blog.csdnimg.cn/img_convert/9a65ac9119d2ed7d3c1af623efd34b76.png)在计算时每个grid cell的尺度为1，所以当前grid cell的左上角坐标为$(0,0)$。由于 sigmoid函数 的处理  **边界框的中心位置会约束在当前grid cell内部，防止偏移过多**。而 $p_{w}$ 和 $p_{h}$ 是先验框 anchor box 的宽度与长度，前面说过它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1。这里记特征图的大小为 $(W,H)$ （v2中为 (13,13) )，这样我们可以将边界框相对于整张图片的位置和大小计算出来 。![得到的四个 尺度值](https://img-blog.csdnimg.cn/img_convert/92cbcacaeb84e01c3ecde7db7fe3a155.png)**如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的最终位置和大小了**。这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。## Fine-Grained Features 细粒度特征图<font color="green">Fine-Grained Features 细粒度特征图 用于帮助检测小目标以及密集目标</font>YOLOv2的输入图片大小为 416×416 ，经过5次maxpooling之后得到 13×13大小的特征图，并以此特征图采用卷积做预测。 13×13大小的特征图对检测大物体是足够了，但是**对于小物体还需要更精细的特征图（Fine-Grained Features）**。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种pass through层来利用更精细的特征图。如下图为整体的网络结构：![Darknet19检测模型+passthrough操作](https://img-blog.csdnimg.cn/img_convert/989dde6af769da5bd9ba6012e8bef92e.png#pic_center)passthrough层与ResNet网络的shortcut类似，以前面**更高分辨率的特征图为输入**，然后将其**连接到后面的低分辨率特征图**上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个 2×2 的局部区域，然后将其转化为channel维度，对于26×26×512 的特征图，经passthrough层处理之后就变成了 13×13×2048 的新特征图（特征图大小降低4倍，而channles增加4倍。操作如下图所示：![passthrough 增加通道数的操作栗子](https://img-blog.csdnimg.cn/img_convert/4051f9f715281e7658ab01fb6557807b.png#pic_center)这样也就得到了 13×13×2048 的输出，可以与后面的 13×13×1024 低分辨率特征图连接在一起形成 13×13×3072 大小的特征图，然后在此特征图基础上卷积做预测。![就是这样](https://img-blog.csdnimg.cn/img_convert/8a808bf40d6b819b196d7d348c755dca.png)<font color="blue">在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层：</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out = tf.extract_image_patches(<span class="keyword">in</span>, [<span class="number">1</span>, stride, stride, <span class="number">1</span>], [<span class="number">1</span>, stride, stride, <span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">&quot;VALID&quot;</span>)</span><br><span class="line">// <span class="keyword">or</span> use tf.space_to_depth</span><br><span class="line">out = tf.space_to_depth(<span class="keyword">in</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>这是v2论文中刚开始的想法，作者后期借鉴了ResNet网络，**不是直接对高分辨特征图处理**，而是增加了一个**中间1×1卷积层**，先采用64个1×1 卷积核进行卷积，然后再进行passthrough处理，这样 26×26×512的特征图得到 13×13×256 的特征图。最后再高和低分辨率合并，得到13×13×1280的特征图做预测。这算是实现上的一个trick。使用Fine-Grained Features之后YOLOv2的性能有1%的提升。![作者改进后的 也就是真实的代码设计](https://img-blog.csdnimg.cn/img_convert/cd777947365d746debcde922ad285dfb.png#pic_center)## Multi-Scale Training 多尺度训练YOLOv2的一大创新是采用Multi-Scale Training策略，**这样同一个模型其实就可以适应多种大小的图片**。<font color="blue">由于YOLOv2模型中采用了global average pooling 全局平均池化层 ，所以YOLOv2的输入可以不限于 416×416 大小的图片。</font>为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔10个 iterations 之后改变模型的输入图片大小。由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值：{320,352，...608}。在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。![Multi-Scale Training 多尺度输入训练](https://img-blog.csdnimg.cn/img_convert/8a03850706857e9f5a4cd666bae33301.png#pic_center)YOLOv2在 VOC2007和2012的训练集上，可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于 544×544 ，mAP高达78.6%。**注意，这只是训练时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。** 这里也可以看出，多尺度训练的一个副作用：如果你输入的是一个高分辨率的大图片 yolo会预测的比较慢但是比较准 如果你输入的是低分辨率的小图片，yolo会预测的非常快但是精度没有那么高。因此yolo可以通过输入图片的大小尺度，来做到精度和速度的权衡。# Faster##  New Network: Darknet-19YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如下图所示。下面这个是 用于分类的模型， 就是预训练模型啦。![Darknet-19 的分类模型 预训练输入的为 224×224](https://img-blog.csdnimg.cn/img_convert/10c9041bf90f9c2d7a3de23f4b571706.png)Darknet-19与VGG16模型设计原则是一致的，主要采用 3×3 卷积，采用 2×2 的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用**global avgpooling做预测**，并且在 3×3 卷积之间使用 1×1 卷积来压缩特征图channles以降低模型计算量和参数。Darkxnet-19每个卷积层后面同样使用了Batch Normalization层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。# YOLOv2的训练YOLOv2的训练主要包括三个阶段。 1. 第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为 224×224 ，共训练160个epochs。 2. 第二阶段将网络的输入调整为 448×448 ，继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。 3. **第三个阶段就是修改Darknet-19分类模型为检测模型**，并在检测数据集上继续finetune网络。网络修改包括（网路结构可视化）：<font color="red">移除最后一个卷积层、global average pooling层以及softmax层，并且新增了三个 3×3×1024 卷积层，同时增加了一个passthrough层，最后使用 1×1 卷积层输出预测结果，输出的channels数为： $anchor个数 × (5+类别个数)$（在v2中 为 5×（5+20）） ，和训练采用的数据集有关系。</font>由于anchors数为5，对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425（因为COCO是80个分类）。这里以VOC数据集为例，最终的预测矩阵为 $T$（shape为 （batch_size,13,13,125）），可以先将其reshape为 （batch_size,13,13,5,25），<font color="blue">其中 $T[:,:,:,:,0:4]$ 为边界框的位置和大小 $t_x,t_y,t_w,t_h$， $T[:,:,:,:,4]$ 为边界框的置信度，而 $T[:,:,:,:,5:]$为类别预测值。 </font>![YOLOv2训练的三个阶段](https://img-blog.csdnimg.cn/img_convert/533f8c8dcc9da79ea03a5ca90bd00f4a.png#pic_center)![YOLOv2 检测网络结构示意图](https://img-blog.csdnimg.cn/img_convert/89c0c117e6648f93753cc6416d0bbadf.png#pic_center)## YOLOv2的损失函数v2的预测框和类别的想法其实与v1是一致的，对于训练图片中的ground truth，若其中心点落在某个grid cell内，那么该grid cell内的5个先验框 anchor box 所对应的预测框 bounding box 负责预测它，具体是哪个bounding box预测它，需要在训练中确定，即由那个与ground truth的IoU最大的 bounding box 预测它，而剩余的4个 bounding box 不与该ground truth匹配。![YOLOv2的损失函数](https://img-blog.csdnimg.cn/img_convert/68b2ccd52bf5d18fd4a234ac07c12057.png#pic_center) 1. 首先 $W、H$分别指的是特征图（ 13×13 ）的宽13与高13，而 $A$ 指的是先验框anchor box数目（这里是5），各个 $λ$ 值是各个 loss 部分的权重系数（即超参数）。![在这里插入图片描述](https://img-blog.csdnimg.cn/img_convert/2b4666868ab3c549bf793cfa3acf9e59.png#pic_center) 2. <font color="red">第一项loss是计算 该预测框不负责检测物体(background) 的置信度误差(越小越好)</font>，但是哪些预测框不负责检测物体对象而预测背景呢，需要先计算各个预测框和**所有ground truth的IOU值**，并且取**最大值Max_IOU**，如果该值**小于**一定的**阈值**（YOLOv2使用的是0.6），那么这个预测框就标记为background，<font color="blue">需要计算$λ_{noobj}$ 的置信度误差。</font>![在这里插入图片描述](https://img-blog.csdnimg.cn/img_convert/b0b4307ec6cdad8bde6ec679a55b6d8b.png#pic_center) 3. <font color="red">第二项是计算先验框anchor box与预测框bounding box的坐标误差</font>，<font color="blue">但是只在前12800个iterations间计算，该博客博主认为 这项应该是在训练前期使预测框快速学习到先验框的位置和形状。</font> ![在这里插入图片描述](https://img-blog.csdnimg.cn/img_convert/fdd54195cbe12c0fc17d3ac02a61a179.png#pic_center)> Q1: 为什么是 前12800个iterations？ 4. <font color="red">第三大项计算与某个ground truth匹配的预测框各部分loss值，包括定位（坐标）误差、置信度误差以及分类误差。</font>先说一下匹配原则，对于某个ground truth，首先要确定其中心点要落在哪个grid cell上，然后计算这个grid cell的5个先验框 anchor box 与ground truth的IoU值<font color="blue">（YOLOv2中bias_match=1）</font>，**计算IoU值时不考虑坐标，只考虑形状**，所以先将先验框与ground truth的中心点都偏移到同一位置（原点），然后计算出对应的IoU值，IoU值最大的那个先验框与ground truth匹配，对应形状的预测框用来预测这个ground truth。 说白了就是 我先看看 anchorbox 模板里面哪个和 ground truth形状比较像，这样我的框比较对嘛。然后选出这个框 找到 这个grid cell 里面 5个bounding box 和这个anchor 形状一样的那个框 去匹配ground truth。 说白了就是 弄了个中间商去选 形状 匹配。 <font color="blue">在计算obj置信度时，target=1，但与YOLOv1一样而增加了一个控制参数rescore，当其为1时，target取预测框与ground truth的真实IOU值（cfg文件中默认采用这种方式）。 ???什么意思</font> 对于那些没有与ground truth匹配的先验框（与预测框对应），除去那些Max_IOU低于阈值的，其它的就全部忽略，不计算任何误差。这点在YOLOv3论文中也有相关说明：**YOLO中一个ground truth只会与一个先验框匹配（IOU值最好的）**，对于那些IOU值超过一定阈值的先验框，其预测结果就忽略了。 尽管YOLOv2和YOLOv1计算loss处理上有不同，但都是采用均方差来计算loss。另外需要注意的一点是，在计算boxes的 $w$ 和 $h$ 误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，<font color="blue">但是根据ground truth的大小对权重系数进行修正：l.coord_scale * (2 - truth.w*truth.h)（这里w和h都归一化到(0,1))</font>，这样对于尺度较小的boxes其权重系数会更大一些，可以放大误差，起到和YOLOv1计算平方根相似的效果（参考[YOLO v2 损失函数源码分析](https://www.cnblogs.com/YiXiaoZhou/p/7429481.html)）。 最终的YOLOv2模型在速度上比YOLOv1还快（采用了计算量更少的Darknet-19模型），而且模型的准确度比YOLOv1有显著提升，详情见paper。# Yolov2的预测同Yolov1 的预测 同样是消除多余的框。可以看一下Yolov1 是怎么 在网络提取出特征图 后 如何 消除多余的框的。 [Yololv1的详解](https://jks88995656.github.io/2021/09/26/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov1%20%E8%AF%A6%E8%A7%A3/)# Yolo9000YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。# Yolo9000 的训练## WordTree与联合训练这个装逼不成的作者 **选择在COCO检测数据集和ImageNet分类数据集上进行联合训练**，但是遇到的第一问题是<font color="red">两者的类别并不是完全互斥的（而常规的softmax方法认定各神经元是互斥的），</font>比如"Norfolk terrier"明显属于"dog"，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：![基于COCO和ImageNet数据集建立的WordTree](https://img-blog.csdnimg.cn/img_convert/e081b98313870f03c620f2d8fb0ddb7e.png#pic_center)WordTree中的根节点为"physical object"，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个path，然后计算path上各个节点的概率之积。计算某个节点的概率，比如说 Norfolk terrier 节点（根据他的物种从上到下遍历），计算步骤如下所示（分层概率计算完整概率）：![计算Norfolk terrier结点概率](https://img-blog.csdnimg.cn/img_convert/802124190e6b707e7cd21def3a0f0cd7.png#pic_center)上面是 做 分类的时候，概率的计算方式。他用了 ImageNet1000个类别构建的WordTree，当然中间需要一些中间层结点（比如 并不代表物种的  hunting dog这个类等）一共1369个结点。 按同层次的类别softmax的方法如下图，**计算结果其实不如 全部互斥的分类结果。**![ImageNet与WordTree预测的对比](https://img-blog.csdnimg.cn/img_convert/63cf274736bb7b43970301aa7d192431.png#pic_center)![Yolo9000](https://img-blog.csdnimg.cn/img_convert/e6f7edd74a243934d58718fd27e46d23.png#pic_center)<font color="blue">当其是应用于 图像检测上时，需要将上面的 Pr(physical object)替换成 v2的置信度预测。</font><h2 id="实际训练与损失函数"><a href="#实际训练与损失函数" class="headerlink" title="实际训练与损失函数"></a>实际训练与损失函数</h2><p>Yolo9000 使用 ImageNet的分类数据集的前9000个类别和COCO的检测数据集构建WordTree 进行训练，测试用的是 ImageNet检测竞赛数据。</p><p>其用v2输出的 Tensor应该为 （13,13,3,5+9418），做Yolo9000时，作者将anchor box 变成了3个，因为5个的话张量实在太大了。5+9418 也就是 bounding box预测的4个左边偏移量和含有对象置信度，以及9418个分类。</p><p>在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是 $Pr(physical    -object)$ ，同时会给出边界框位置以及一个树状概率图。<strong>在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别（也就是只会有1个）。</strong>而其他与Ground Truth 的IoU 高于0.3的预测框，本不应该具有这么高的置信度（IoU),通通都作为负样本。</p><h2 id="Yolo9000-检测上的实验效果"><a href="#Yolo9000-检测上的实验效果" class="headerlink" title="Yolo9000 检测上的实验效果"></a>Yolo9000 检测上的实验效果</h2><p>通过联合训练策略，YOLO9000可以快速检测出超过9000个类别的物体，总体mAP值为19.7%。事实上，ImageNet检测类别中大部分类别仅在ImageNet分类数据集上见过，并不是检测数据集，（这里可以看做迁移效果）。所以效果一般般，很多类别检测不出来，特别是物体类；但动物类尚可。估计原因就是，COCO检测数据集中大部分都是动物，所以迁移过来的对动物的效果当然也会比较好的。<br><img src="https://img-blog.csdnimg.cn/img_convert/ebf757ed8dcdec3f000ab814355f10b3.png#pic_center" alt="在这里插入图片描述"><br>作者提出的 yolo9000 算是一个开创的想法，但是效果不佳（不然他早就在youtube上装逼了）</p><h1 id="参考论文和博客"><a href="#参考论文和博客" class="headerlink" title="参考论文和博客"></a>参考论文和博客</h1><p>参考博客：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/35325884">转载来源 目标检测|YOLOv2原理与实现</a></li><li><a href="https://www.cnblogs.com/YiXiaoZhou/p/7429481.html">YOLO v2 损失函数源码分析 源码c版的</a></li><li><a href="https://zhuanlan.zhihu.com/p/54073204">Batch Normalization的通俗解释</a></li></ul><p>参考视频：</p><ul><li><a href="https://www.bilibili.com/video/BV1Q64y1s74K?p=1">同济子豪兄v2算法讲解</a></li><li><p><a href="https://www.bilibili.com/video/BV1Q64y1s74K?p=2">同济子豪兄v2论文精讲</a></p><p>keras代码 github：YAD2K-master<br>参考论文：YOLO9000 Better, Faster, Stronger</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;同济子豪兄 之 yolov2 详解&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;font color=&quot;blue&quot;&gt;蓝色： 表示还不明白什么意思&lt;/font&gt; 

&lt;p&gt;v1存在 一些性能和原理上的问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;mAP相比 R-CNN 系列比较低&lt;/</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="http://example.com/tags/yolo/"/>
    
    <category term="目标检测" scheme="http://example.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础 - Batch Normalization</title>
    <link href="http://example.com/2021/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20-%20Batch%20Normalization/"/>
    <id>http://example.com/2021/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20-%20Batch%20Normalization/</id>
    <published>2021-10-05T07:29:01.000Z</published>
    <updated>2021-10-29T15:03:24.311Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="深度学习基础 - Batch Normalization">TOC</a></p><p>在计算机的眼光里，对具有统一规格的数据，更能学习到其数据之间的规律特征。也就是下图所看的的这样，将杂乱的数据标准归一化。<br><img src="https://img-blog.csdnimg.cn/img_convert/7c0360d9080d48dce15371c941baea9b.png#pic_center" alt="数据标准归一化"></p><p>首先我们之前有Normalization（普通数据标准化），一般用作<strong>输入数据</strong>的样本归一化操作。那Batch Normalization 则可用在 每个层上包括隐藏层。</p><h1 id="机器学习中的Feature-Scaling"><a href="#机器学习中的Feature-Scaling" class="headerlink" title="机器学习中的Feature Scaling"></a>机器学习中的Feature Scaling</h1><p>如果特征大小差的比较远的话，loss function会很扁平，数值更大的feature的参数会对结果的影响更大，这样在训练过程中，不同的方向需要设定不同的学习率，这样子会不太方便，这不是我们想看到的，所以我们通常会去做feature scaling。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/196be414d2c05a542644613ab56c6213.png#pic_center" alt="使用Feature Scaling后"><br>具体的操作很简单，对每一维特征，我们对每一个数据减去这维特征的均值，再除以这位特征的标准差，得到缩放后的新的特征值，此时它的均值为0，方差为1。一般经过feature scaling之后，<strong>收敛速度会变快</strong>。</p><p><strong>那么在神经网络中又是什么情况呢？</strong></p><p>我们可以看到，其实在深度网络中，后一层的输入其实是前一层的输出。那么我们在做feature scaling的时候，应该对每一层的输入都去做一个feature scaling, 但是又不像传统的机器学习，由于神经网络每一层的参数都在不断变化，直接使用前面的feature scaling是不太合适的。所以我们需要一个新的技术，就是Batch Normalization了</p><h1 id="Batch-Normalization-放在神经网络的哪个部位？"><a href="#Batch-Normalization-放在神经网络的哪个部位？" class="headerlink" title="Batch Normalization 放在神经网络的哪个部位？"></a>Batch Normalization 放在神经网络的哪个部位？</h1><p>Batch Normalization 放在 线性层的后面，激活函数的前面。如下图所示：<br><img src="https://img-blog.csdnimg.cn/img_convert/be42f4f33882e58d12258995609ccb06.png#pic_center" alt="Batch Normalization 放在 线性层的后面，激活函数的前面"></p><h2 id="Batch-Normalization-基本原理"><a href="#Batch-Normalization-基本原理" class="headerlink" title="Batch Normalization 基本原理"></a>Batch Normalization 基本原理</h2><p>现在一般采用批梯度下降方法对深度学习进行优化，这种方法把数据分为若干组，按组来更新参数，一组中的数据共同决定了本次梯度的方向，下降时减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也下降了很多。</p><p>Batch Normalization(简称BN)中的batch就是批量数据，即每一次优化时的样本数目，通常BN网络层用在卷积层后，用于重新调整数据分布。假设神经网络某层一个batch的输入为X=[x1,x2,…,xn]，其中xi代表一个样本，n为batch size。</p><ol><li>首先，我们需要求得mini-batch里元素的均值：<script type="math/tex; mode=display">μ_{B}=\frac{1}{n}\sum_{i=1}^{n}x_{i}</script></li><li>接下来，求取mini-batch的方差：<script type="math/tex; mode=display">\sigma _{B}^{2} = \frac{1}{n}\sum_{i=1}^{n}(x_{i}-μ_{B})^{2}</script></li><li>这样我们就可以对每个元素进行归一化<script type="math/tex; mode=display">\hat{x_{i}}=\frac{x_{i}-μ_{B}}{\sqrt{\sigma _{B}^{2}+\varepsilon}}</script>这里的分母本来应该是 $\sigma$,但为了防止它为0，我们加上一个$\varepsilon$。</li><li>最后进行尺度缩放和偏移操作，这样可以变换回原始的分布，实现恒等变换，这样的目的是为了补偿网络的非线性表达能力，因为经过标准化之后，偏移量丢失。具体的表达如下，$y_{i}$就是网络的最终输出。$γ$与$β$是神经网络的参数，是网络自己学习的。<script type="math/tex; mode=display">y_{i}=γ\hat{x_{i}}+β</script></li></ol><p>从某种意义上来说，方差和均值代表的其实是输入数据分布的方差和偏移。对于没有BN的网络，这两个值与前一层网络带来的非线性性质有关，而经过变换后，就跟前面一层无关，变成了当前层的一个学习参数，这更加有利于优化并且不会降低网络的能力。</p><h1 id="举个-Batch-Normaliztion-计算栗子"><a href="#举个-Batch-Normaliztion-计算栗子" class="headerlink" title="举个 Batch Normaliztion 计算栗子"></a>举个 Batch Normaliztion 计算栗子</h1><p>他分为训练（训练 $γ$与$β$）和测试阶段（使用训练后得到的参数）。<br><img src="https://img-blog.csdnimg.cn/img_convert/ae36898be0aa225a92aaf50123680551.png#pic_center" alt="同济子豪兄的理解"><br>如下图 这个的 batch_size=8，也就是说输入的图像有8张，即8个输入；看第一个0.9 表示的是 第一张神经元对第4张图片的响应；后面的1.7就是 第一张神经元对第5张图片的响应；26.7 也就是最后一个神经元对 第4张图片的响应。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/a5c5dfbc5e9f3d93b52b31b12dec84c7.png#pic_center" alt=""><br>就是先算出 平均值 再计算 均方差，按公式得到标准化的结果。</p><h1 id="Batch-Normaliztion-的效果"><a href="#Batch-Normaliztion-的效果" class="headerlink" title="Batch Normaliztion 的效果"></a>Batch Normaliztion 的效果</h1><ol><li>因为BN可以把输入都规整化在非饱和区内，所以他可以加快收敛（因为梯度变化的大，不缓慢）比如 sigmoid 和 tanh 函数 非饱和区和饱和区。</li><li>减少了训练时间，而且可以进行深层网络的训练，同时可以使用更大的学习率。</li><li>减轻了对参数初始化的依赖，这是利于调参的</li><li>可以起到正则化的作用 可以防止过拟合</li><li>BN一定程度上增加了泛化能力，dropout等技术可以去掉。</li></ol><p><strong>注意：BN与Dropout不能一起使用。</strong> 为什么有待于补充。</p><h2 id="可以看一下-效果"><a href="#可以看一下-效果" class="headerlink" title="可以看一下 效果"></a>可以看一下 效果</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/275bcf4be1343343b2e006268bed2b95.png#pic_center" alt="用了Batch Normalization 的效果"></p><h1 id="参考论文与博客"><a href="#参考论文与博客" class="headerlink" title="参考论文与博客"></a>参考论文与博客</h1><p>Batch Normalization论文：<a href="http://proceedings.mlr.press/v37/ioffe15.pdf">Batch Normalization论文</a><br>参考的博客：<br><a href="https://zhuanlan.zhihu.com/p/24810318">什么是批标准化？</a><br><a href="https://www.zhihu.com/question/38102762/answer/607815171">知乎第一个留言 深度学习中 Batch Normalization为什么效果好？</a><br><a href="https://zhuanlan.zhihu.com/p/54073204">精选 写的很好  Batch Normalization的通俗解释 </a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;深度学习基础 - Batch Normalization&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在计算机的眼光里，对具有统一规格的数据，更能学习到其数据之间的规律特征。也就是下图所看的的这样，将杂乱的数据标准归一化。&lt;br&gt;&lt;img src=&quot;https://</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>目标检测的中的指标的含义及其实现</title>
    <link href="http://example.com/2021/10/04/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E4%B8%AD%E7%9A%84%E6%8C%87%E6%A0%87%E7%9A%84%E5%90%AB%E4%B9%89%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/"/>
    <id>http://example.com/2021/10/04/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E4%B8%AD%E7%9A%84%E6%8C%87%E6%A0%87%E7%9A%84%E5%90%AB%E4%B9%89%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-10-04T14:01:01.000Z</published>
    <updated>2021-10-29T15:03:28.966Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="目标检测的中的指标的含义及其实现">TOC</a></p><h1 id="Precision和Recall"><a href="#Precision和Recall" class="headerlink" title="Precision和Recall"></a>Precision和Recall</h1><ul><li>Precision是查准率、精确率的意思。预测为正的结果中，有多少真正是正样本。</li><li>Recall是查全率、召回率的意思。对所有正样本有多少预测出来了。</li></ul><p><img src="https://img-blog.csdnimg.cn/img_convert/cdb0fd90e720fb4261f93aa90c4dd2dc.png#pic_center" alt="举个栗子"><br>用另一个图理解<br><img src="https://img-blog.csdnimg.cn/img_convert/3fb660ada123e26a1f646c9962eefb8e.png#pic_center" alt="在这里插入图片描述"><br>快速记忆 左边这个是TF表示这个结果预测的对不对<br>    右边那个PN表示我预测的是正的还是负的（比如二分类 正的：是这个东西 负的：不是这个东西）<br>注意：这两个量都是：第一是你告诉我是正的里面有多少是对的，第二是关注我对ground truth是正的 也就是实际上就是正的的能力。这两个是相反的关系，一个高另一个就低。<br>比如如下图所示：<br><img src="https://img-blog.csdnimg.cn/img_convert/56443acb4016898926b67ac50eb7052a.png#pic_center" alt="提升评选的标准"><br>提升门槛值，那么也就是对的会越对。门槛值高了嘛，那我正样本预测的就越准的，那么我将负样本预测错误成正样本的概率就低了。所以我的 Precison上升了，但Recall 下降了。</p><h1 id="IoU-Intersection-over-Union"><a href="#IoU-Intersection-over-Union" class="headerlink" title="IoU (Intersection over Union)"></a>IoU (Intersection over Union)</h1><p><img src="https://img-blog.csdnimg.cn/img_convert/88aa2935955dddb7556f2dcbd15c4e3e.png#pic_center" alt="IoU示意图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateIoU</span>(<span class="params">candidateBound, groundTruthBound</span>):</span></span><br><span class="line">    cx1 = candidateBound[<span class="number">0</span>]</span><br><span class="line">    cy1 = candidateBound[<span class="number">1</span>]</span><br><span class="line">    cx2 = candidateBound[<span class="number">2</span>]</span><br><span class="line">    cy2 = candidateBound[<span class="number">3</span>]</span><br><span class="line"> </span><br><span class="line">    gx1 = groundTruthBound[<span class="number">0</span>]</span><br><span class="line">    gy1 = groundTruthBound[<span class="number">1</span>]</span><br><span class="line">    gx2 = groundTruthBound[<span class="number">2</span>]</span><br><span class="line">    gy2 = groundTruthBound[<span class="number">3</span>]</span><br><span class="line"> </span><br><span class="line">    carea = (cx2 - cx1) * (cy2 - cy1) <span class="comment">#C的面积</span></span><br><span class="line">    garea = (gx2 - gx1) * (gy2 - gy1) <span class="comment">#G的面积</span></span><br><span class="line"> </span><br><span class="line">    x1 = <span class="built_in">max</span>(cx1, gx1)</span><br><span class="line">    y1 = <span class="built_in">max</span>(cy1, gy1)</span><br><span class="line">    x2 = <span class="built_in">min</span>(cx2, gx2)</span><br><span class="line">    y2 = <span class="built_in">min</span>(cy2, gy2)</span><br><span class="line">    w = <span class="built_in">max</span>(<span class="number">0</span>, x2 - x1)</span><br><span class="line">    h = <span class="built_in">max</span>(<span class="number">0</span>, y2 - y1)</span><br><span class="line">    area = w * h <span class="comment">#C∩G的面积</span></span><br><span class="line"> </span><br><span class="line">    iou = area / (carea + garea - area)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> iou</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="top1、top5-的含义"><a href="#top1、top5-的含义" class="headerlink" title="top1、top5 的含义"></a>top1、top5 的含义</h1><p>在图像分类中：</p><ul><li>Top-1 error<br>的意思是：假如模型预测某张动物图片（一只猫）的类别，且模型只输出1个预测结果，那么这一个结果正好能猜出来这个动物是只猫的概率就是Top-1正确率。猜出来的结果不是猫的概率则成为Top-1错误率。简单来说就是模型猜错的概率。</li><li>Top-5 error<br>的意思是：假如模型预测某张动物图片（还是刚才那只猫），但模型会输出来5个预测结果，那么这五个结果中有猫这个分类的概率成为Top-5正确率，相反，预测输出的这五个结果里没有猫这个分类的概率则成为Top-5错误率。</li></ul><p>一般来说，Top-1和Top-5错误率越低，模型的性能也就越好。且Top-5 error 在数值上会比Top-1 error 的数值要小，毕竟从1个结果猜对的几率总会比从5个结果里猜对的几率要小嘛！</p><p>在目标检测中：</p><ul><li>top-1正确率，就是你预测的label取最后概率向量里面最大的那一个作为预测结果，如过你的预测结果中概率最大的那个分类正确，则预测正确，否则预测错误。</li><li>top5就是最后概率向量最大的前五名中，只要出现了正确概率即为预测正确。否则预测错误。</li></ul><h1 id="Average-Precision-AP-与-mAP"><a href="#Average-Precision-AP-与-mAP" class="headerlink" title="Average Precision (AP)与 mAP"></a>Average Precision (AP)与 mAP</h1><p>多个类别目标检测中，每个类别都可以根据recall（召回率）和 percision（准确率）绘制一条曲线。AP就是该曲线下的面积，mAP意思是对每一类的AP再求平均。<br>mAP计算方法：<br>首先我们要先搞明白AP。<br>AP表示 整个的面积<br><img src="https://img-blog.csdnimg.cn/img_convert/56443acb4016898926b67ac50eb7052a.png#pic_center" alt="AP如右图"><br><img src="https://img-blog.csdnimg.cn/img_convert/245a8e1597441fc997ad6b64144a473b.png#pic_center" alt=""><br>我们一般用F1的值来 找到最适合的点  来均衡 Precison 和 Recall。<br>那什么是mAP呢，我们现在讨论的都是 比如预测有1和0，我们是针对1这个正样本。也可以反过来看0作为正样本呢。所以要加起来一起除以总体 数。<br><img src="https://img-blog.csdnimg.cn/img_convert/2d4feb910a3867622ae4e1c6e85d9aa8.png#pic_center" alt="mAP与AP"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;目标检测的中的指标的含义及其实现&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Precision和Recall&quot;&gt;&lt;a href=&quot;#Precision和Recall&quot; class=&quot;headerlink&quot; title=&quot;Precision和Recall&quot;</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>同济子豪兄 之 yolov1 详解</title>
    <link href="http://example.com/2021/09/26/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov1%20%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2021/09/26/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov1%20%E8%AF%A6%E8%A7%A3/</id>
    <published>2021-09-26T14:01:01.000Z</published>
    <updated>2021-10-29T15:03:50.667Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="同济子豪兄 之 yolov1 详解">TOC</a><br>yolov1 的主旨是 You Only Look Once：Unified，Real-Time Object Detection。<br>yolo 是一个典型的 将 目标检测 转化为  回归问题的方法。yolo与其他网络不同在，他的测试与训练方法不同，所以接下来我们主要 分预测以及训练两个阶段去介绍yolov1。 包括 预测阶段（以及后处理），训练阶段来讲。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/6d831d5df7aff4ccf3dd13027b9fdbbb.png#pic_center" alt="训练阶段以及测试阶段"><br>将输入的图片 划分为 S×S个 grid cell<br><strong>训练阶段：</strong></p><ul><li>将标签 Ground truth 的 框 中心点落在哪个grid cell 中 就由 哪个 grid cell 来预测这个物体对象</li><li>每个 grid cell 可以 预测 B 个 bounding box， 与 Ground truth 的 框 IoU 最大的<br>bounding box 负责预测 这个物体对象。</li><li>每个 grid cell 只能 检测一个物体</li><li>包含和不包含 ground truth 标签的 grid cell 和 bounding box 都要依照损失函数 分别处理</li></ul><p><strong>测试阶段：</strong></p><ul><li>直接获得 S × S ×（S×B+C）向量 进行 NMS 后处理 得到目标检测结果。</li></ul><h1 id="目标检测的基础知识"><a href="#目标检测的基础知识" class="headerlink" title="目标检测的基础知识"></a>目标检测的基础知识</h1><h2 id="目标检测是什么？"><a href="#目标检测是什么？" class="headerlink" title="目标检测是什么？"></a>目标检测是什么？</h2><p>在计算机视觉领域，图像任务主要分为：图像分类，图像检测 和 图像分割（语义分割和实例分割）等<br>语义分割和实例分割的区别在于：语义分割是我对每个像素分类，我不管这个像素是属于哪几个物体的，只管他是属于什么类别的（也就是我只分类，同一个类别的不同实例不区分）；而实例分割是要把同一个类别的不同实例给区分开来</p><p><img src="https://img-blog.csdnimg.cn/img_convert/35b53c4cd1da18a70dca91d65e748d1b.png#pic_center" alt="任务区别图1"><br><img src="https://img-blog.csdnimg.cn/img_convert/636fe4401f3cfa7a5b8fdb3909646d63.png#pic_center" alt="任务区别图2"><br><img src="https://img-blog.csdnimg.cn/img_convert/56310ef740a07f9b2cf502c8a41f28eb.png#pic_center" alt="任务区别图3"></p><h2 id="目标检测主流的数据集来源"><a href="#目标检测主流的数据集来源" class="headerlink" title="目标检测主流的数据集来源"></a>目标检测主流的数据集来源</h2><p>yolo 是在  PASCAL-2007 和 MS-COCO上做的评测<br><img src="https://img-blog.csdnimg.cn/img_convert/0a3e3c44de4fb73523bc7d2026481089.png#pic_center" alt="主流目标检测数据集来源"></p><h2 id="目标检测的发展"><a href="#目标检测的发展" class="headerlink" title="目标检测的发展"></a>目标检测的发展</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/8199110e14455de8cd7de50658a142f6.png#pic_center" alt="目标检测的发展"><br>目前这个主要的流派由两种，上面是单阶段模型 yolo 系列  下面是两阶段模型 RCNN系列。</p><ul><li>两阶段就是先从图像中提取若干候选框，再逐一的对这些候选框进行分类、甄别以及调整它们的坐标最后得出结果。</li><li>单阶段就是 我不提取候选框，我直接把全图喂到算法里面，能直接输出出来目标检测的结果。是一个统一的端到端的系统</li></ul><p>对于 RCNN系列的话，他比较慢但是 正确率比较高</p><ul><li>R-CNN 使用region proposal先提取候选框，再使用卷积神经网络逐一的对每个候选款进行甄别，对 bouning box 位置调整和回归和分类</li><li>Fast R -CNN 是把所有的图片用卷积神经网络过一遍，在生成的feature map上找候选框 投影到 feature map 上面，再进行甄别</li><li>Faster R-CNN 使用了 RPN 网络，也就是找候选框这个事情由 RPN 这个专业户干了</li></ul><p>对于 yolo系列的话，他的优势在于速度，正确率相比没有那么高<font color="blue">（待补充）</font></p><ul><li>yolov1 的缺点在于，单个grid cell 只能识别一种类别。识别小目标或者密集目标 能力不足，例如羊群、人群这种。</li><li>yolov2 是在 yolov1 的基础上 加了<strong>Batch Normalization</strong>、High Resolution Classifer、<strong>Anchor</strong>、Dimension Cluster、Direct location prediction、Fine-Graind Features、Muti-Scale Training</li><li>yolov3</li><li>yolov4 从v4开始 作者变了，前面的大作因为感慨yolo技术被用来干坏事而退出了计算机视觉领域。</li><li>yolov5</li></ul><h1 id="yolo-v1-的-预测阶段"><a href="#yolo-v1-的-预测阶段" class="headerlink" title="yolo v1 的 预测阶段"></a>yolo v1 的 预测阶段</h1><h2 id="yolo-v1-网络架构"><a href="#yolo-v1-网络架构" class="headerlink" title="yolo v1 网络架构"></a>yolo v1 网络架构</h2><p>输出的 7×7×30 这个tensor 就是我们预测阶段所需要的<br><img src="https://img-blog.csdnimg.cn/img_convert/38e7f43fea47802cb9d2b7564377db4b.png#pic_center" alt="yolov1 图像处理的网络架构"><br>我们来分析一下网络结构并写出各层次的padding。<br>计算公式为</p><script type="math/tex; mode=display">outputsize =\lfloor {\frac{inputsize + 2padding -filtersize}{步长}}\rfloor +1</script><p><img src="https://img-blog.csdnimg.cn/img_convert/28059cdcee997d3230d56657217dce43.png#pic_center" alt="算出padding"><br>之后最后两个全连接层。<br>他是怎么转为 4096个输入的呢。也就是输入的为7×7×1024 我们全连接层输出的 是 4096 的一维向量。我们采用 4096个 7×7×1024 的卷积核，然后 变成 1×1×4096  再使用降维 变为 一维的4096。<br>然后我们再将这个 reshap 成 4096×1的列向量 中间的 参数矩阵为 4096×1470  根据 线性回归的公式：</p><script type="math/tex; mode=display">y=w^{T}x</script><p>可以计算得到 1470×4096×4096×1 = 1470×1  然后我们在 reshape 一下变成三维 （7,7,30）</p><h2 id="yolo-v1-预测阶段-内容详解"><a href="#yolo-v1-预测阶段-内容详解" class="headerlink" title="yolo v1 预测阶段 内容详解"></a>yolo v1 预测阶段 内容详解</h2><blockquote><p>Q：为什么 输出的是 7×7×30 呢？<br><img src="https://img-blog.csdnimg.cn/img_convert/c466565d19154b1bc714cafe0d4614c6.png#pic_center" alt="yolo v1核心内容"><br>因为在 yolo v1 中图像被划分成了<strong>7×7的网格</strong>。每个格子叫做 grid cell ，也就是由49个grid cell。</p></blockquote><p>这里的 每个 grid cell 又能预测 B个 bounding box （也就是预测框） yolo v1中 B=2 也就是每个 grid cell 可以预测 2个 bounding box。这两个预测框可能很大也可能很小 也就是这个框是啥样的不一定。（<strong>注意这个框是由前面的网络结构得到的 就是这么神奇</strong>）。这个框覆盖其他的grid cell 是很正常的，只要这个 bounding box 的中心点 是落在这个 grid cell 里就ok。</p><p>每个grid cell 预测 B 个 bounding box。 在yolo v1 的实验中 B=2，也就是 每个 grid cell 由 2 个 bounding box。<br>bounding box 的由 5个 数值来表示 （x,y,h,w,c）</p><ul><li>x，y 表示中心点的位置</li><li>h，w表示这个 bounding box 的 高 和 宽</li><li>c 表示这个 bounding box 框的置信度。在图像中 一般用 框线的粗细来表示置信度的大小。<strong>置信度的意思是，这个bounding box 对自己含有物体对象的自信程度。（注意是识别的对象，而不是具体的类别）</strong></li></ul><p>在yolo v1的实验中，因为由 49 个 grid cell，所以 有 98 个 bounding box 如下图所示（粗细表示置信度高低）<br><img src="https://img-blog.csdnimg.cn/img_convert/049d6ea03b1129753555d38bdb1ed2e4.png#pic_center" alt="98个bounding box"></p><h2 id="grid-cell-输出所有类别的条件概率"><a href="#grid-cell-输出所有类别的条件概率" class="headerlink" title="grid cell 输出所有类别的条件概率"></a>grid cell 输出所有类别的条件概率</h2><p>每个 grid cell 还能生成所有类别的 <strong>条件概率</strong>，并且选择最高的一个概率表明，预测的是这个类别。这也就是 yolo v1 最大的弊端，每个 grid cell 只能预测一个类别，那一个 49个cell 最大只能预测 49个类别；并且如果类别对象很密集，一个 grid cell 中有多个不同的小对象的话，识别效果会很差。</p><p><font color="red">注意这里的概率是 条件概率 比如 p(cat|object) 是在有<strong>存在类别对象的情况下 是 猫的概率</strong>。</font><br>下图可以表示 49个cell 预测的类别情况（也就是各cell 选择的最高类别的 条件概率）</p><p><img src="https://img-blog.csdnimg.cn/img_convert/3dd9cb0d2cf0e1391ba7688e445d4886.png#pic_center" alt="每个grid cell 选择的最高类别的 条件概率"></p><h2 id="7×7×30-这个30是怎么来的呢"><a href="#7×7×30-这个30是怎么来的呢" class="headerlink" title="7×7×30 这个30是怎么来的呢"></a>7×7×30 这个30是怎么来的呢</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/c8cab634a723eaab685cf39f70e88511.png#pic_center" alt="7×7×30 这个30是怎么来的呢"><br>每个grid cell 有 2个bounding box。1个bounding box 中有5个数值 那就是10个数值，然后 yolo v1 预测了20个类别的物体，所以有20个类别的概率 所以是 30个输出。然后有 7×7个 grid cell 所以 输出的 是 7×7×30 的 tensor。</p><blockquote><p>Q：<strong>那 类别真正的概率怎么算出来呢？</strong><br>只有将 对应的 bounding box 置信度 × 这个grid cell 的类别条件概率才是 这个类别真正的概率值</p></blockquote><p> $全概率 = bounding box 置信度 × $</p><p>将这个grid cell 的两个 bouding box 都赋予 这个grid cell 选择的最高类别。再进行一系列的后处理（指的是 置信度很低的先给过滤掉，非极大值抑制（NMS））选择最佳 bounding box</p><p><img src="https://img-blog.csdnimg.cn/img_convert/c928f7b0c5670984b7b21aaeabd0bd5c.png#pic_center" alt=""><br>7×7×30这个tensor中包含了 98个bounding box 49个grid cell 每个grid cell 的类别 bounding box的 5个数值   进行解析和后处理 最后得到了结果<br>以上我们说的都是 预测阶段 也就是 参数啥的已经调好了 我只要跑一跑 拿到个结果。</p><h1 id="预测阶段-的后处理部分"><a href="#预测阶段-的后处理部分" class="headerlink" title="预测阶段 的后处理部分"></a>预测阶段 的后处理部分</h1><p>一个 grid cell 可以得到 下图右侧中两列向量。每个向量 都表示 这个 grid cell 在其一个 bounding box 中 20个类别的全概率。<br>一个98个bounding box。 <strong>这个是由 每个bounding box 的置信度 乘以 tensor 中 输出的 该 grid cell 对20个类别的条件概率得到的。</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/89ba9c3732e7722b15c9eebf175f7634.png#pic_center" alt="grid cell 输出其两个预测框的 20个类别的全概率"><br>现在我们得到如下的框框图像，一共有 98个 bounding box。不同颜色表示不同类别。（因为每个 grid cell 只会预测概率最高的那个类别 比如狗是黄色的 那他的两个bounding box 都是黄色的）</p><p><img src="https://img-blog.csdnimg.cn/img_convert/0b04396864fbcb9abb88839b0ca4cfce.png#pic_center" alt="需要后处理的 98个 预测框"><br>每个 bounding box  把他对20个分类的条件概率拿出来，乘以对应的置信度。得到20个分类的全概率向量。<br><img src="https://img-blog.csdnimg.cn/img_convert/62f2eb8e8436081525012232736fc888.png#pic_center" alt="拿到20个分类的条件概率"><br>我们把刚才 每个bounding box 输出的 20个分类的全概率向量拿出来。 假如第一行的是对狗的预测。我们要把98个列向量按狗的概率，从大到小排列。<font color="red"><strong>（他的意思就是 我按排列之后 每一个类别都按 NMS一遍  ）</strong> </font>整个的过程如下图所示。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/2afbebe5fbb7b772cf1f63a7f2e25127.png#pic_center" alt="如何使用NMS方法"></p><h2 id="NMS方法详解"><a href="#NMS方法详解" class="headerlink" title="NMS方法详解"></a>NMS方法详解</h2><p>假如 我们比较下图中 98个bounding box 狗这一行 的 前两个值， 也就是左边橙色和绿色连的bounding box ，我们计算一下 IoU  这边要预先设定一个门槛值（他这里是0.5，越小排异性越高），如果IoU超过0.5的话，表明这两个框预测的都是同一类别（这里也就是狗）。这样的话，保留值高的那个，低的那个全概率改为0，把这个bounding box 干掉。<br><img src="https://img-blog.csdnimg.cn/img_convert/9b4e674106e87245c29a5712b5c82de8.png#pic_center" alt="NMS 图1"></p><p><img src="https://img-blog.csdnimg.cn/img_convert/9cc8d08e234cc4c22826f68150b8ea98.png#pic_center" alt="结果把这个位置清0"><br>下图这个就是 IoU不到门槛值  说明两个 bounding box 预测的不是一个类别 两者都保留不动。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/dce95176ac7ab31983097137667b0f1c.png#pic_center" alt="IoU不到门槛值的情况"><br>然后第一个和所有比完了 再从剩下里面最高的 再比。<br>最后我们可以得到一个稀疏的 98 列，选择有值的 列向量，找到其最大的概率的那一项，返回他的索引（也就是类别），以及他对于的概率值。然后在图片上打上对应的 bounding box。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/784a171229a817e12ba83f6e69be073a.png#pic_center" alt="然后第一个和所有比完了 再从剩下里面最高的 再比"><br><strong>注意奥 我们讲的是只是对预测阶段 需要 。在训练阶段是不需要NMS的。</strong> 因为每个框不管他是要被打入冷宫的还是负责预测物体的有用框 都要在损失函数中占据一席之地。里面的所有框的一举一动都会影响损失函数。所以不能随随便便在训练阶段 用NMS 把没用的框去掉，或者把概率抹零。</p><h1 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h1><p>首先奥这是个典型的 监督学习。所以我们是需要 ground truth。比如下图就是一个正确的标签：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/9940687bd6b6c2f7cf5235fb14ba8369.png#pic_center" alt="正确的标签"></p><blockquote><p><font color="red">这里有个问题 每个grid cell 都有两个 bounding box 谁来 拟合 ground truth 做损失函数呢? </font><br>答：看谁和 ground truth的 IoU 更大。比如这里 就是由 外面这个大框 负责拟合 ground truth，让其尽量的逼近 调整成 ground truth 的样子，那么另外一个框就被打入冷宫了 ，它什么都不用做，尽量让他置信度降为0。<br><img src="https://img-blog.csdnimg.cn/img_convert/cfafbaa050469d1ce71b47affcdc3341.png#pic_center" alt="每个grid cell 有两个 bounding box"><br><img src="https://img-blog.csdnimg.cn/img_convert/ec519587d6180d710944340ab1713270.png#pic_center" alt="看看选择哪个 bounding box"></p></blockquote><p><strong>如果没有ground truth 中心点落下 的 grid cell 的两个 bounding box 全部打入冷宫。</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/3c083d8df79294e99cf3e223e49ffd9d.png#pic_center" alt="没有ground truth 中心点落下 的 grid cell"></p><h2 id="yolo-v1-的-损失函数-待补充与推理"><a href="#yolo-v1-的-损失函数-待补充与推理" class="headerlink" title="yolo v1 的 损失函数 (待补充与推理)"></a>yolo v1 的 损失函数 (待补充与推理)</h2><p>下面介绍一下 yolov1损失函数误差。主要分为5个部分。<br><img src="https://img-blog.csdnimg.cn/img_convert/b4f54e2a69268e05525cd063dbb06b93.png#pic_center" alt="yolo v1 的 损失函数"><br>现在有一个问题就是 我损失函数要去调整的参数到底是谁。是为了让选中的那个bounding box 框更加拟合 ground truth ，那我们要调整的岂不是 网络结构输出的东西，那也就是改的网络了？所以到底是在调整什么呢？</p><p>没毛病 就是返回框的参数 然后反向传播调整网络的参数</p><h1 id="总结yolov1的缺点"><a href="#总结yolov1的缺点" class="headerlink" title="总结yolov1的缺点"></a>总结yolov1的缺点</h1><ol><li>mAP相比 R-CNN 系列比较低</li><li>定位性能比较差，定位错误占总错误的比例很大</li><li>Recall比较低，就是把全部目标全部检测出来的能力比较差</li><li>检测密集和小目标的能力比较差</li></ol><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li><a href="https://www.bilibili.com/video/BV15w411Z7LG?p=1">牛逼的子豪兄 yolo v1详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/46309428">yolo 损失函数的详解</a></li><li><a href="https://blog.csdn.net/weixin_44523062/article/details/104717799">yolo 置信度的概念</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;同济子豪兄 之 yolov1 详解&quot;&gt;TOC&lt;/a&gt;&lt;br&gt;yolov1 的主旨是 You Only Look Once：Unified，Real-Time Object Detection。&lt;br&gt;yolo 是一个典型的 将 目标检测 转化为  回归问</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="http://example.com/tags/yolo/"/>
    
    <category term="目标检测" scheme="http://example.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达deeplearning.ai学习  之 目标检测</title>
    <link href="http://example.com/2021/09/25/%E5%90%B4%E6%81%A9%E8%BE%BEdeeplearning.ai%E5%AD%A6%E4%B9%A0%20%20%E4%B9%8B%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <id>http://example.com/2021/09/25/%E5%90%B4%E6%81%A9%E8%BE%BEdeeplearning.ai%E5%AD%A6%E4%B9%A0%20%20%E4%B9%8B%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</id>
    <published>2021-09-25T08:37:01.000Z</published>
    <updated>2021-10-29T15:03:19.509Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="吴恩达deeplearning.ai学习  之 目标检测">TOC</a></p><h1 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h1><p>我们对图像的学习过程应该由 图像分类 -&gt; 图像单目标定位 -&gt; 图像多目标检测。<br><img src="https://img-blog.csdnimg.cn/img_convert/3ff5dd940674e17134f4a03d0199478d.png#pic_center" alt="目标检测的演化过程"></p><h2 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h2><p>在图像分类中，例如，输入一张图片到多层卷积神经网络，它会输出一个特征向量，并反馈给 softmax 单元来预测图片类型。比如你正在构建汽车自动驾驶系统，你面前的对象可能包括以下4类：行人、汽车、摩托车和背景，这时 softmax 单元有四个输出（概率高的为判断的类）。如果图片里没有前三类的话，输出结果会是背景。</p><h2 id="图像分类基础上-发展图像内的单目标定位"><a href="#图像分类基础上-发展图像内的单目标定位" class="headerlink" title="图像分类基础上 发展图像内的单目标定位"></a>图像分类基础上 发展图像内的单目标定位</h2><p>在图像分类的基础上，我们可以对图像内的单目标定位<font   color="red">（注意：这边的基础是图像里只有一个目标可以被检测，例如上图的车）</font>。实现定位，需要让神经网络多输出4个数字，标记为$b<em>{x},b</em>{y},b<em>{h},b</em>{w}$用于表示输出的边界框。前两者表示边界框的中心点，后两者表示的为边界框的高度和宽度。<font color="red">注意：我们定义左上角为坐标原点(0,0)，右下角为终点(1,1)</font> 例如下图所示：<br><img src="https://img-blog.csdnimg.cn/img_convert/043ac4d106c620fb7b7d1a3a300173e2.png#pic_center" alt="边界框描述"><br>还是这个例子，神经网络的输出改变为四个边界框描述数字$b<em>{x},b</em>{y},b<em>{h},b</em>{w}$ ，是否含有对象 $p<em>{c}$ ，含有对象属于哪个类标签。可以定义输出  $y$ 为 $[p</em>{c},b<em>{x},b</em>{y},b<em>{h},b</em>{w},c<em>{1},c</em>{2},c_{3}]^{T}$ </p><p><img src="https://img-blog.csdnimg.cn/img_convert/82a6083e2c74937f49a25fc083131ea0.png#pic_center" alt="标签的定义与举例 左图为有车这个目标的图像  右图为没有目标对象的背景图像"><br><strong>注意我们此时假设的图片中只含有一个对象，是单目标的分类定位问题。</strong></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>神经网络的损失函数，其参数为类别$\hat{y}$ 和网络输出 $y$ ，如果采用MSE损失函数，则 </p><script type="math/tex; mode=display">L(\hat{y},y)==(\hat{y_{1}} - y_{1})^{2}+(\hat{y_{2}} - y_{2})^{2}+...+(\hat{y_{8}} - y_{8})^{2}</script><p>损失值等于每个元素相应差值的平方和。<br>​</p><ul><li>如果图片内有对象（即 $p_{c}=1$），损失值就是不同元素的平方和</li><li>如果图片内没有对象（即 $p<em>{c}=0$），我们不用考虑其它元素，只需要关注神经网络输出 $p</em>{c}$ 的准确度。</li></ul><h2 id="图像内的单目标定位基础上-发展多目标定位"><a href="#图像内的单目标定位基础上-发展多目标定位" class="headerlink" title="图像内的单目标定位基础上 发展多目标定位"></a>图像内的单目标定位基础上 发展多目标定位</h2><p>yolo 将图像拆分成多个子图</p><h1 id="特征点检测（Landmark-detection）"><a href="#特征点检测（Landmark-detection）" class="headerlink" title="特征点检测（Landmark detection）"></a>特征点检测（Landmark detection）</h1><p>例如我想做人脸的情感识别，可以根据眼睛的特征点，根据嘴部的关键点输出值来确定嘴的形状，从而判断人物是在微笑还是皱眉，也可以提取鼻子周围的关键特征点。为了便于说明，你可以设定特征点的个数，<strong>假设脸部有64个特征点</strong>，有些点甚至可以帮助你定义脸部轮廓或下颌轮廓。选定特征点个数，并生成包含这些特征点的标签训练集，然后利用神经网络输出脸部关键特征点的位置。<br><img src="https://img-blog.csdnimg.cn/img_convert/2e7d94b4be44001ed6f193bc81b0303e.png#pic_center" alt="人脸情感识别的特征点示意图"><br>具体的想法是，准备一个卷积网络和一些特征点集，将人脸图片输入到卷积网络，输出1或0（1表示有人脸，0表示没有人脸），然后输出 64个特征点坐标位置 $(l<em>{1x},l</em>{1y})$ … $(l<em>{64x},l</em>{64y})$。理论上应该有129个输出（64个特征点 64×2+ 1（表示是否有人脸））。另外在人体姿态检测上也可以使用这样的特征点方法。</p><blockquote><p><strong>Q：注意点 特征点1的特性在所有图片中必须保持一致</strong><br>特征点1始终是右眼的外眼角，特征点2是右眼的内眼角，特征点3是左眼内眼角，特征点4是左眼外眼角等等。所以标签在所有图片中必须保持一致，假如你雇用他人或自己标记了一个足够大的数据集，那么神经网络便可以输出上述所有特征点，你可以利用它们实现其他有趣的效果，比如判断人物的动作姿态，识别图片中的人物表情等等。</p></blockquote><h1 id="滑动窗口目标检测法-（复杂过时）"><a href="#滑动窗口目标检测法-（复杂过时）" class="headerlink" title="滑动窗口目标检测法 （复杂过时）"></a>滑动窗口目标检测法 （复杂过时）</h1><p>这个方法简单的说就两步</p><ul><li>先构建一个卷积神经网络，能够识别出这个对象<br><img src="https://img-blog.csdnimg.cn/img_convert/e00067812bddc65aadedb32741cf0368.png#pic_center" alt="训练得到可以识别车的卷积网络"></li><li>滑动窗口（每次扩大窗口），选定合适的步长，从左上角开始裁剪，将其输入该卷积网络，输出是否为该对象。这样从左上角开始滑动窗口遍历整个图像。<br><img src="https://img-blog.csdnimg.cn/img_convert/4beea107cf1b1f8c359ef591dc421634.png#pic_center" alt="滑动窗口法"><br>这个方法有很明显的缺点就是。如果步长太大，误差会很大，不够精细。单如果步长太精细的话，因为我每滑动一次窗口即裁剪一次图像就要输入卷积网络输出是否是车。卷积网络时间代价很高，这么多次输入，那这个方法需要的时间成本就非常的高。</li></ul><blockquote><p><strong>Q：那为什么这个办法之前是可用的？</strong><br>因为之前神经网络还没兴起，每个裁剪对象判断都是由SVM来完成的。线性分类器速度比较快一些。</p></blockquote><p>解决的办法是不要使用滑动窗口，使用卷积来实现滑动窗口的效果。详见下章节。</p><h1 id="滑动窗口的卷积实现（Convolutional-implementation-of-sliding-windows）"><a href="#滑动窗口的卷积实现（Convolutional-implementation-of-sliding-windows）" class="headerlink" title="滑动窗口的卷积实现（Convolutional implementation of sliding windows）"></a>滑动窗口的卷积实现（Convolutional implementation of sliding windows）</h1><p>首先你要了解，如何将全神经网络转化为用卷积网络来实现，如下图：<br><img src="https://img-blog.csdnimg.cn/img_convert/a9f14a0c1c5045527b4838803ea88b64.png#pic_center" alt="全神经网络转化为用卷积网络来实现"><br><strong>那我们现在来看 为什么要可以使用卷积来简化滑动窗口呢？</strong><br><img src="https://img-blog.csdnimg.cn/img_convert/cf87ee71357e6a873d064bf0bea812b6.png#pic_center" alt="用卷积来简化滑动窗口"><br>如上图所示，假如输入给卷积网络的图像大小为14×14×3，测试集图片是16×16×3。<br>现在给这个输入图片加上黄色条块（padding = 2），在最初的滑动窗口算法中，设置窗口大小为14×14，步长为2，可以滑出4个窗口，对应输入到卷积网络中，输出4个标签。 <strong>可以发现，这4次卷积操作中由很多像素点的计算都是重复的。</strong> 所以执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算。<br>如果我们使用如上的卷积方式的话，最后输出的4个方块，刚好就对应我们的4个窗口。如上面绿色的小框框，假设你剪切出这块区域（编号1），传递给卷积网络，第一层的激活值就是这块区域（编号2），最大池化后的下一层的激活值是这块区域（编号3），这块区域对应着后面几层输出的右上角方块（编号4，5，6）。<br><img src="https://img-blog.csdnimg.cn/img_convert/18e139d467f97830a238b06725a67f68.png#pic_center" alt="更复杂的例子"><br>但是正如在前一页所看到的，我们不能依靠连续的卷积操作来识别图片中的汽车，比如，我们可以对大小为28×28的整张图片进行卷积操作，一次得到所有预测值，如果足够幸运，神经网络便可以识别出汽车的位置。28×28的图像按照 图像14×14的来划定窗口，那么将有 64个窗口（行列均为8 8×8）所以最后输出的也是个 8×8。<br><strong>说白了这个办法，就是用卷积，将最后的输出每个格子表示当初划定的一个窗口。</strong><br>这个算法，效率是提高了，但是仍然存在一个缺点，就是边界框的位置可能不够准确。可以Bounding Box预测解决，详见下一章。<br><img src="https://img-blog.csdnimg.cn/img_convert/b16106fb0d740de6fffa3bd64dec09bc.png#pic_center" alt="识别是正确的，但是框和车的具体位置不符合哎"></p><h1 id="Bounding-Box预测（Bounding-box-predictions）"><a href="#Bounding-Box预测（Bounding-box-predictions）" class="headerlink" title="Bounding Box预测（Bounding box predictions）"></a>Bounding Box预测（Bounding box predictions）</h1><p>如何能精准边界框呢。比较出名的一个算法就是 Yolo 算法。Yolo 算法是这样的，简单的说就是 （如下图）比如你输入的图像是100×100的，然后在图像上放一个3×3网格（自己定应该更精细一点）。一共9个网格，每个网格都采取我们之前说的图像分类再定位的算法。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/15a1fb69f8ac47568bb742c1dfa7a7af.png#pic_center" alt="100×100 化为3×3网格"><br>对于9个格子中的每一个指定一个标签 $y$ ，$y$ 是8维的。$y=[p<em>{c},b</em>{x},b<em>{y},b</em>{h},b<em>{w},c</em>{1},c<em>{2},c</em>{3}]^{T}$。 看这个九宫格的第一个格子，里面没有检测的目标（也就是只有背景），所以左上格子的标签向量 $y=[0,?,?,?,?,?,?,?]^{T}$，同样右边的两个格子也是一样的。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/237ff5b5bf58e20651f7444ba194c017.png#pic_center" alt="取中点的格子"><br>再看第二层，有两辆车，也就是有2个对象。<strong>YOLO算法</strong>做的就是，取两个对象的中点，然后将这个对象分配给<strong>包含对象中点</strong>的格子。所以左边的汽车就分配到左边这个格子上（编号4），然后这辆长条车中点在这里，分配给右侧这个格子（编号6）。所以即使中心格子（编号5）同时有两辆车的一部分，我们就假装中心格子没有任何我们感兴趣的对象，所以对于中心格子，的输出标签向量 就是$y=[0,?,?,?,?,?,?,?]^{T}$。而左右编号为4和6的格子，输出标签向量均为$y=[1,b<em>{x},b</em>{y},b<em>{h},b</em>{w},0,1,0]^{T}$  由此得到，最后这张图片的目标输出为 3×3×8 （因为这里有3×3格子，然后对于每个格子，你都有一个8维向量，所以目标输出尺寸是3×3×8。） </p><h2 id="怎么得到框的精确位置呢？"><a href="#怎么得到框的精确位置呢？" class="headerlink" title="怎么得到框的精确位置呢？"></a>怎么得到框的精确位置呢？</h2><p>因为我们是将每个样本格子都作为考虑对象，所以每个格子的左上角均为（0，0），右下角为（1，1）。如下图所示，右侧长条车为栗子：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/7da7c4e5e6d9c2602b659e5db0342407.png#pic_center" alt="精确外框"><br>可以看到橙色为对象的中点处，而后$b<em>{x},b</em>{y},b<em>{h},b</em>{w}$单位是相对于格子尺寸的比例，所以$b<em>{x},b</em>{y}$必须在0和1之间；$b<em>{h},b</em>{w}$ 可能会大于1，特别是如果有一辆汽车的边界框是这样的（左下大红框），那么边界框的宽度和高度有可能大于1。</p><h1 id="交并比（Intersection-over-union）"><a href="#交并比（Intersection-over-union）" class="headerlink" title="交并比（Intersection over union）"></a>交并比（Intersection over union）</h1><p>业界都用交并比（IOU）来衡量 框框打的对不对。如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/e3544013b01c3b80ee75324e9967a629.png#pic_center" alt="IoU"></p><script type="math/tex; mode=display">IOU = \frac{∩ size}{∪size}</script><p>一般规定≥0.5即可，这个是人为定的，你也可以严格一点，选0.6及其以上。</p><h1 id="非极大值抑制（NMS）减少同一对象的-多个检测结果"><a href="#非极大值抑制（NMS）减少同一对象的-多个检测结果" class="headerlink" title="非极大值抑制（NMS）减少同一对象的 多个检测结果"></a>非极大值抑制（NMS）减少同一对象的 多个检测结果</h1><font color="blue">到目前为止你们学到的对象检测中的一个问题是，你的算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。</font><p><img src="https://img-blog.csdnimg.cn/img_convert/5ccaf433e145ea11d38bd12830c2d86d.png#pic_center" alt="多个检测结果"><br>非极大值抑制这个方法可以确保你的算法对每个对象只检测一次，我们讲一个例子。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/6f0cc70b5cfd976204bc8e5d2d4c7b7c.png#pic_center" alt="图像分成19×19"></p><blockquote><p>就比如上面这个图。<br>在上面放个19×19网格，理论上这辆车只有一个中点，所以它应该只被分配到一个格子里，左边的车子也只有一个中点，所以理论上应该只有一个格子做出有车的预测。<br>实<strong>践中当你运行对象分类和定位算法时，对于每个格子都运行一次，所以这个格子（编号1）可能会认为这辆车中点应该在格子内部，这几个格子（编号2、3）也会这么认为。</strong>对于左边的车子也一样，所以不仅仅是这个格子，如果这是你们以前见过的图像，不仅这个格子（编号4）会认为它里面有车，也许这个格子（编号5）和这个格子（编号6）也会，也许其他格子也会这么认为，觉得它们格子内有车。</p></blockquote><p>我们分步介绍一下非极大抑制是怎么起效的，因为你要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的 $p_{c}$ ，我这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。所以当你运行算法的时候，最后可能会对同一个对象做出多次检测，所以非极大值抑制做的就是清理这些检测结果。这样一辆车只检测一次，而不是每辆车都触发多次检测。</p><p>所以具体上，这个算法做的是，首先看看每次报告每个检测结果相关的概率$p<em>{c}$，实际上$p</em>{c}$ 是 $p<em>{c}=1$ 乘以 $c</em>{1}、c<em>{2}$或 $c</em>{3}$得到的。首先看概率最大的那个，这个例子（右边车辆）中是0.9，然后就说这是最可靠的检测，所以我们就用<strong>高亮标记</strong>，就说我这里找到了一辆车。这么做之后，<strong>非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边框有很高交并比，高度重叠的其他边界框，那么这些输出就会被抑制。</strong><br>因为其他两个矩形 $p_{c}$ 分别是0.6和0.7，这两个矩形和 0.9矩形重叠程度很高，所以会被抑制而变暗。<br>接下来，逐一审视剩下的矩形，找出概率最高，最高的一个，在这种情况下是0.8，我们就认为这里检测出一辆车（左边车辆），然后非极大值抑制算法就会去掉其他 loU 值很高的矩形。所以现在每个矩形都会被高亮显示或者变暗，如果你直接抛弃变暗的矩形，那就剩下高亮显示的那些，这就是最后得到的两个预测结果。</p><p>简单的说就是：先消除$p_{c} ≤ 0.6$的框，当多个检测框重叠面积IOU占据最大框面积的比例超过了这个设定的非最大值抑制这个值的时候，那么就只保留置信度（概率）最高的那个框，冗余的框都去掉。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/ed191a6c686ba1c76835b774e6435eaa.png#pic_center" alt="NMS的操作"></p><h1 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h1><h2 id="为什么要使用-anchor-box"><a href="#为什么要使用-anchor-box" class="headerlink" title="为什么要使用 anchor box"></a>为什么要使用 anchor box</h2><p>到目前为止，对象检测中存在一个问题就是每个格子只能预测一个对象，如果想让一个格子检测出多个对象，你可以这么做，就是使用anchor box这个概念。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/69bf3aff7963180bba13f024ee328d30.png#pic_center" alt="同一个格子对象重叠"><br>假设我们有这样一张图片，对于这个例子，我们使用3x3的网格，可以观察到，行人和汽车的中心几乎在同一个网格，然而我们以前的方法一个格子只能预测一个对象，而且对于 y 输出的向量 $y=[p<em>{c},b</em>{x},b<em>{y},b</em>{h},b<em>{w},c</em>{1},c<em>{2},c</em>{3}]^{T}$ ，你可以检测这三个类别，行人、汽车和摩托车，它将无法输出检测结果，所以我必须从两个检测结果中选一个，这便影响了模型性能，导致一些对象被丢弃无法检测出来。</p><h2 id="anchor-box-的引入和使用"><a href="#anchor-box-的引入和使用" class="headerlink" title="anchor box 的引入和使用"></a>anchor box 的引入和使用</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/2efc6386311540659396db7cb148d63f.png#pic_center" alt="Anchor box"><br>我们按以下图片的方式，重新定义输出即可：（右边的话是因为 anchor box2 的IoU更高）<br><img src="https://img-blog.csdnimg.cn/img_convert/05633034404323320e005d2c553d8b99.png#pic_center" alt="anchor box 的使用"><br>所以，总的来说，anchor box是这么来做的，现在每个对象和以前一样根据中心点分配到一个格子中，然后看和每个anchor box的IoU（交并比），选择IoU最高的那个，用这个anchor box来进行预测。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/1355bdea1f0916386ef754d2ae81a28b.png#pic_center" alt=""></p><h2 id="如何选择-anchor-box？"><a href="#如何选择-anchor-box？" class="headerlink" title="如何选择 anchor box？"></a>如何选择 anchor box？</h2><ol><li>一般手工指定anchor box形状，根据要检测的对象，指定有针对性地anchor box，可选择5-10个anchor box，使其尽可能覆盖到不同形状。</li><li>使用K-means聚类算法获得anchor box。</li></ol><h1 id="anchor-box-NMS-的总过程"><a href="#anchor-box-NMS-的总过程" class="headerlink" title="anchor box + NMS 的总过程"></a>anchor box + NMS 的总过程</h1><p><img src="https://img-blog.csdnimg.cn/img_convert/00001fa607902111940cd50a61e2c0a6.png#pic_center" alt="步骤"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;吴恩达deeplearning.ai学习  之 目标检测&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;目标定位&quot;&gt;&lt;a href=&quot;#目标定位&quot; class=&quot;headerlink&quot; title=&quot;目标定位&quot;&gt;&lt;/a&gt;目标定位&lt;/h1&gt;&lt;p&gt;我们对图像的学</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>argparse模块用法实例详解</title>
    <link href="http://example.com/2021/09/22/argparse%E6%A8%A1%E5%9D%97%E7%94%A8%E6%B3%95%E5%AE%9E%E4%BE%8B%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2021/09/22/argparse%E6%A8%A1%E5%9D%97%E7%94%A8%E6%B3%95%E5%AE%9E%E4%BE%8B%E8%AF%A6%E8%A7%A3/</id>
    <published>2021-09-22T15:31:01.000Z</published>
    <updated>2021-10-29T15:03:15.083Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="argparse模块用法实例详解">TOC</a></p><p>argparse是python的命令行解析的标准模块，内置于python，不需要安装。这个库可以让我们直接在命令行（这边指的是 <strong>python的命令行</strong> 或者是 <strong>Anaconda Prompt</strong>）中就可以向程序中传入参数并让程序运行。<br>其实 argparse 就是一个键值对存储的方式。</p><h1 id="栗子一：传入一个参数并输出"><a href="#栗子一：传入一个参数并输出" class="headerlink" title="栗子一：传入一个参数并输出"></a>栗子一：传入一个参数并输出</h1><p>新建一个文件（如叫：arg_study），在该文件夹中新建一个python文件（如：demo.py）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;命令行中传入一个数字&#x27;</span>)</span><br><span class="line"><span class="comment"># type是要传入的参数的数据类型  help是该参数的提示信息</span></span><br><span class="line"><span class="comment"># integers 相当于键</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;integers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&#x27;传入的数字&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment">#获得传入的参数</span></span><br><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></table></figure><h2 id="查看帮助提示"><a href="#查看帮助提示" class="headerlink" title="查看帮助提示"></a>查看帮助提示</h2><p>命令行中输入<code>python demo.py -h</code>或者 <code>python demo.py --help</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python demo.py -h</span><br><span class="line">python demo.py --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">usage: demo.py [-h] integers</span><br><span class="line"></span><br><span class="line">命令行中传入数字</span><br><span class="line"></span><br><span class="line">positional arguments:</span><br><span class="line">  integers    传入的数字</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>  show this <span class="built_in">help</span> message <span class="keyword">and</span> exit</span><br></pre></td></tr></table></figure><h2 id="输入参数并输出"><a href="#输入参数并输出" class="headerlink" title="输入参数并输出"></a>输入参数并输出</h2><p>如输入5<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python demo.py <span class="number">5</span></span><br></pre></td></tr></table></figure><br>得到的结果<code>print(args)</code>为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Namespace(integers=<span class="string">&#x27;5&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>如何获取其中的数据呢？</strong><br><code>Namespace(integers=&#39;5&#39;)</code> 其实是一个类似于python字典的数据类型。<br>我们可以是哟个 <code>arg.参数名</code>  来提取这个参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获得integers参数</span></span><br><span class="line"><span class="built_in">print</span>(args.integers)</span><br></pre></td></tr></table></figure><h1 id="栗子二：传入多个参数并输出"><a href="#栗子二：传入多个参数并输出" class="headerlink" title="栗子二：传入多个参数并输出"></a>栗子二：传入多个参数并输出</h1><p>nargs是用来说明传入的参数个数，’+’ 表示传入至少一个参数。这时候再重新在命令行中运行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;命令行中传入一个数字&#x27;</span>)</span><br><span class="line"><span class="comment"># nargs是用来说明传入的参数个数，&#x27;+&#x27; 表示传入至少一个参数。</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;integers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, nargs=<span class="string">&#x27;+&#x27;</span>,<span class="built_in">help</span>=<span class="string">&#x27;传入的数字&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(args.integers)</span><br></pre></td></tr></table></figure><p>这时候再重新在命令行中运行<code>python demo.py 1 2 3 4</code>得到<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>]</span><br></pre></td></tr></table></figure></p><h1 id="栗子三：改变数据类型"><a href="#栗子三：改变数据类型" class="headerlink" title="栗子三：改变数据类型"></a>栗子三：改变数据类型</h1><p>add_argument中有type参数可以设置传入参数的数据类型。我们看到代码中有type这个关键词，该关键词可以传入list, str, tuple, set, dict等。例如我们把上面的type=str，改成type=int,这时候我们就可以进行四则运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;命令行中传入一个数字&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;integers&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, nargs=<span class="string">&#x27;+&#x27;</span>,<span class="built_in">help</span>=<span class="string">&#x27;传入的数字&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment">#对传入的数据进行加总</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>(args.integers))</span><br></pre></td></tr></table></figure><p>在命令行中输入 <code>python demo.py 1 2 3 4</code>, 运行结果为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure><h1 id="栗子四：位置参数"><a href="#栗子四：位置参数" class="headerlink" title="栗子四：位置参数"></a>栗子四：位置参数</h1><p>在命令行中传入参数时候，传入的参数的先后顺序不同，运行结果往往会不同，这是因为采用了位置参数,例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;姓名&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;param1&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,<span class="built_in">help</span>=<span class="string">&#x27;姓&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;param2&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,<span class="built_in">help</span>=<span class="string">&#x27;名&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印姓名</span></span><br><span class="line"><span class="built_in">print</span>(args.param1+args.param2)</span><br></pre></td></tr></table></figure><p>输出 张三 ：在命令行中分别输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python demo.py 张 三</span><br></pre></td></tr></table></figure><h2 id="使用可选参数"><a href="#使用可选参数" class="headerlink" title="使用可选参数"></a>使用可选参数</h2><p>为了在命令行中避免上述位置参数的bug（容易忘了顺序），可以使用可选参数，这个有点像关键词传参，但是需要在关键词前面加—，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;姓名&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--family&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,<span class="built_in">help</span>=<span class="string">&#x27;姓&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--name&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,<span class="built_in">help</span>=<span class="string">&#x27;名&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印姓名</span></span><br><span class="line"><span class="built_in">print</span>(args.family+args.name)</span><br></pre></td></tr></table></figure><p>在命令行中输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python demo.py --family=张 --name=三</span><br></pre></td></tr></table></figure><p>结果为 张三 。<br>可选参数虽然写法比较繁琐，但是增加了命令行中的可读性，不容易因为参数传入顺序导致数据错乱。</p><h2 id="设置默认值"><a href="#设置默认值" class="headerlink" title="设置默认值"></a>设置默认值</h2><p>add_argument中有一个<strong>default参数</strong>。有的时候需要对某个参数设置默认值，即如果命令行中没有传入该参数的值，程序使用默认值。如果命令行传入该参数，则程序使用传入的值。具体请看下面的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;姓名&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--family&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;张&#x27;</span>,<span class="built_in">help</span>=<span class="string">&#x27;姓&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--name&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;三&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;名&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印姓名</span></span><br><span class="line"><span class="built_in">print</span>(args.family+args.name)</span><br></pre></td></tr></table></figure><h2 id="设置该参数一定要传入"><a href="#设置该参数一定要传入" class="headerlink" title="设置该参数一定要传入"></a>设置该参数一定要传入</h2><p>add_argument有一个<strong>required参数</strong>可以设置该参数是否必需。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;姓名&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--family&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, <span class="built_in">help</span>=<span class="string">&#x27;姓&#x27;</span>)</span><br><span class="line"><span class="comment"># name 必须传入</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--name&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, required=<span class="literal">True</span>, default=<span class="string">&#x27;&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;名&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印姓名</span></span><br><span class="line"><span class="built_in">print</span>(args.family+args.name)</span><br></pre></td></tr></table></figure><h1 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h1><p><a href="https://zhuanlan.zhihu.com/p/56922793">argparse模块用法实例详解 【非常的详细】</a><br><a href="https://www.cnblogs.com/yymn/p/8059220.html">python中argparse模块用法实例详解 【比较粗糙】</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;argparse模块用法实例详解&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;argparse是python的命令行解析的标准模块，内置于python，不需要安装。这个库可以让我们直接在命令行（这边指的是 &lt;strong&gt;python的命令行&lt;/strong&gt; 或者</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="argparse" scheme="http://example.com/tags/argparse/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch GoogleNet中的Inception</title>
    <link href="http://example.com/2021/09/21/Pytorch%20GoogleNet%E4%B8%AD%E7%9A%84Inception/"/>
    <id>http://example.com/2021/09/21/Pytorch%20GoogleNet%E4%B8%AD%E7%9A%84Inception/</id>
    <published>2021-09-21T15:40:01.000Z</published>
    <updated>2021-10-29T15:03:09.686Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch GoogleNet中的Inception">TOC</a></p><h1 id="GoogleNet-的概念"><a href="#GoogleNet-的概念" class="headerlink" title="GoogleNet 的概念"></a>GoogleNet 的概念</h1><p>是基于AlexNet，VGG 后的模型</p><h1 id="特殊点：Inception"><a href="#特殊点：Inception" class="headerlink" title="特殊点：Inception"></a>特殊点：Inception</h1><h2 id="为什么要提出-Inception"><a href="#为什么要提出-Inception" class="headerlink" title="为什么要提出 Inception"></a>为什么要提出 Inception</h2><p>一般来说，提升网络性能最直接的办法是<strong>增加网络深度和宽度</strong>，但一味地增加，会带来诸多问题：</p><ol><li>参数太多，如果训练数据集有限，很容易产生过拟合； </li><li>网络越大、参数越多，计算复杂度越大，难以应用；</li><li>网络越深，容易出现梯度消失问题（梯度越往后穿越容易消失），难以优化模型。</li></ol><blockquote><p><strong>梯度消失和梯度爆炸</strong> 是什么？<br>查看文章   <a href="https://www.jianshu.com/p/ece360b7fabb">如何理解梯度消失和梯度爆炸</a><br>以及本博客文章 <a href="https://jks88995656.github.io/2021/09/20/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E7%90%86%E8%A7%A3/">梯度消失和梯度爆炸的理解</a></p></blockquote><p>我们希望在增加网络深度和宽度的同时减少参数，为了减少参数，自然就想到将<strong>全连接变成稀疏连接</strong>。但是在实现上，全连接变成稀疏连接后实际计算量并不会有质的提升，因为<font color="blue">大部分硬件是针对密集矩阵计算优化的，稀疏矩阵虽然数据量少，但是计算所消耗的时间却很难减少。</font><br>在这种需求和形势下，Google研究人员提出了Inception的方法。</p><h2 id="Inception-模块的结构"><a href="#Inception-模块的结构" class="headerlink" title="Inception 模块的结构"></a>Inception 模块的结构</h2><p>Inception 模块 是GoogleNet 重复使用的重要部分。其最大的特点是 引入了 1×1 的卷积核。其目的是用于 缩小通道数，将像素信息融合，也叫做 <strong>通道压缩</strong>。<br><img src="https://img-blog.csdnimg.cn/img_convert/9f3c90923dc228229c3f6c1695777fa3.png#pic_center" alt="在这里插入图片描述"><br>同时其可以减少卷积核的参数数量 例如:如下的操作数对比（28×28表示 卷积的时候图片像素点也要乘的啊）<br><img src="https://img-blog.csdnimg.cn/img_convert/7e7182fa5f7155ca6f80ac1cc6689c08.png#pic_center" alt="参数变化 用1×1后"><br>Inception 模块的内容如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/d98aae54e7979a26c11393e6c0fb7ad3.png#pic_center" alt="Inception 模块结构"></p><h2 id="Pytorch-实现-Inception-模块"><a href="#Pytorch-实现-Inception-模块" class="headerlink" title="Pytorch 实现 Inception 模块"></a>Pytorch 实现 Inception 模块</h2><p>下图中为每个部分的代码模块。 每个部分的 上侧是 pytorch中网络初始化部分，下侧是 pytorch中网络前馈实现的部分。<br><img src="https://img-blog.csdnimg.cn/img_convert/065b0f6abeae7092776e18a5b4755ac3.png#pic_center" alt="各部分实现图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionA</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一个部分</span></span><br><span class="line">        self.branch_pool = torch.nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个部分</span></span><br><span class="line">        self.branch1x1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三个部分</span></span><br><span class="line">        self.branch5x5_1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = torch.nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第四个部分</span></span><br><span class="line">        self.branch3x3_1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = torch.nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_3 = torch.nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 第一个部分</span></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个部分</span></span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三个部分</span></span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第四个部分</span></span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">        branch3x3 = self.branch3x3_3(branch3x3)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通道整合</span></span><br><span class="line">        outputs = [branch_pool, branch1x1, branch5x5, branch3x3]</span><br><span class="line">        <span class="comment"># 整合通道 通道的位置在1处  （batch,通道,宽度,长度）</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="GoogleNet-网络的整体模型"><a href="#GoogleNet-网络的整体模型" class="headerlink" title="GoogleNet 网络的整体模型"></a>GoogleNet 网络的整体模型</h1><p><img src="https://img-blog.csdnimg.cn/img_convert/42c1625cbe9b8e8c37de67bfbf3d319a.png#pic_center" alt="GoogleNet 全貌"></p><h1 id="使用Mnist-使用Inception"><a href="#使用Mnist-使用Inception" class="headerlink" title="使用Mnist 使用Inception"></a>使用Mnist 使用Inception</h1><h2 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">88</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.incep1 = InceptionA(in_channels=<span class="number">10</span>)</span><br><span class="line">        self.incep2 = InceptionA(in_channels=<span class="number">20</span>)</span><br><span class="line">        self.mp = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">1408</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.mp(self.conv1(x)))</span><br><span class="line">        x = self.incep1(x)</span><br><span class="line">        x = F.relu(self.mp(self.conv2(x)))</span><br><span class="line">        x = self.incep2(x)</span><br><span class="line">        <span class="comment"># 变成列向量</span></span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="Mnist-数据集-训练-整体代码"><a href="#Mnist-数据集-训练-整体代码" class="headerlink" title="Mnist 数据集 训练 整体代码"></a>Mnist 数据集 训练 整体代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步 准备数据</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 用均值和方差进行归一化</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">])</span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                               train=<span class="literal">True</span>,</span><br><span class="line">                               download=<span class="literal">True</span>,</span><br><span class="line">                               transform=transform)</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                              train=<span class="literal">False</span>,</span><br><span class="line">                              download=<span class="literal">True</span>,</span><br><span class="line">                              transform=transform)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset,</span><br><span class="line">                          shuffle=<span class="literal">True</span>,</span><br><span class="line">                          batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(test_dataset,</span><br><span class="line">                         shuffle=<span class="literal">False</span>,</span><br><span class="line">                         batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionA</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一个部分</span></span><br><span class="line">        self.branch_pool = torch.nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个部分</span></span><br><span class="line">        self.branch1x1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三个部分</span></span><br><span class="line">        self.branch5x5_1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = torch.nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第四个部分</span></span><br><span class="line">        self.branch3x3_1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = torch.nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_3 = torch.nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 第一个部分</span></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个部分</span></span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三个部分</span></span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第四个部分</span></span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">        branch3x3 = self.branch3x3_3(branch3x3)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通道整合</span></span><br><span class="line">        outputs = [branch_pool, branch1x1, branch5x5, branch3x3]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">88</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.incep1 = InceptionA(in_channels=<span class="number">10</span>)</span><br><span class="line">        self.incep2 = InceptionA(in_channels=<span class="number">20</span>)</span><br><span class="line">        self.mp = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">1408</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.mp(self.conv1(x)))</span><br><span class="line">        x = self.incep1(x)</span><br><span class="line">        x = F.relu(self.mp(self.conv2(x)))</span><br><span class="line">        x = self.incep2(x)</span><br><span class="line">        <span class="comment"># 变成列向量</span></span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个网络模型</span></span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 定义损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义测试函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            outputs = model(data)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">            correct += predicted.eq(target.view_as(predicted)).<span class="built_in">sum</span>().item()</span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%) \n&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss, correct, <span class="built_in">len</span>(test_loader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    )) \</span><br><span class="line"> \</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epochs</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">                <span class="comment"># prepare data</span></span><br><span class="line">                inputs, labels = data</span><br><span class="line">                <span class="comment"># 前馈</span></span><br><span class="line">                y_predict = model(inputs)</span><br><span class="line">                loss = criterion(y_predict, labels)</span><br><span class="line">                <span class="comment"># 反馈</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 更新</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                <span class="keyword">if</span> (batch_idx + <span class="number">1</span>) % <span class="number">30</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                        epoch, batch_idx * <span class="built_in">len</span>(data), <span class="built_in">len</span>(train_loader.dataset),</span><br><span class="line">                               <span class="number">100.</span> * batch_idx / <span class="built_in">len</span>(train_loader), loss.item()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">    test()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h1><p><a href="https://zhuanlan.zhihu.com/p/73857137">深度学习|经典网络：GoogLeNet（一）</a><br><a href="https://zhuanlan.zhihu.com/p/89002063">GoogLeNet</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch GoogleNet中的Inception&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;GoogleNet-的概念&quot;&gt;&lt;a href=&quot;#GoogleNet-的概念&quot; class=&quot;headerlink&quot; title=&quot;GoogleNet 的</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="GoogleNet" scheme="http://example.com/tags/GoogleNet/"/>
    
  </entry>
  
  <entry>
    <title>梯度消失和梯度爆炸的理解</title>
    <link href="http://example.com/2021/09/20/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>http://example.com/2021/09/20/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2021-09-20T11:31:01.000Z</published>
    <updated>2021-10-29T15:03:05.247Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="梯度消失和梯度爆炸的理解">TOC</a></p><h1 id="根本原因"><a href="#根本原因" class="headerlink" title="根本原因"></a>根本原因</h1><p>梯度消失和梯度爆炸的根本原因是由于深度神经网络过长的链，在反向传播通过链式法则求导过程中产生的。 换句话说，就是<strong>反向传播先天就有一定的毛病</strong>。</p><h1 id="根本原因的理解"><a href="#根本原因的理解" class="headerlink" title="根本原因的理解"></a>根本原因的理解</h1><p><img src="https://img-blog.csdnimg.cn/beb6cdfcd994483c9133ee88c0763034.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="前馈和反向传播过程"><br>左上角为 正向反馈的步骤（σ 表示 激活函数 $sigmoid$ ）。 Loss 使用的是 MSE损失函数。 根据反向传播，我们可以看到最后 loss 对 b1 参数的梯度。 可以看到 连乘的情况。</p><ul><li>看这个式子里的 $w<em>{i}$ 中，一般我们初始化权重参数 $w</em>{i}$ 时，通常都小于1。</li><li>激活函数 $sigmoid$  的求导如下所示 <img src="https://img-blog.csdnimg.cn/9f175038b5e74cfda605e8906a194416.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="求导sigmoid函数"></li><li>激活函数的导数 图像如下图所示：<img src="https://img-blog.csdnimg.cn/fe08669e4f3648ea89dec0b560ba422b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="sigmoid函数求导"></li><li>所以$|σ’(z) × w| ≤ 0.25$，多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说梯度几乎为0，导致参数基本不更新，这就是梯度消失的根本原因。<br>梯度爆炸的原因，也就是说如果$|σ’(z) × w| ≥ 1$，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。<br>但 $sigmoid$ 的函数是不可能大于1了，上图看的很清楚，那只能是参数 $w<em>{i}$了，故只有当 $abs(w)&gt;4$ 时才可能出现梯度爆炸，<strong>这也就是经常看到别人博客里的一句话，初始权重过大</strong>。<br>但梯度爆炸的情况一般不会发生，对于$sigmoid$ 函数来说，$σ’(z)$的大小也与 $w</em>{i}$ 有关。<br><strong>其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。</strong></li></ul><h1 id="如何解决梯度消失的问题（待补充）"><a href="#如何解决梯度消失的问题（待补充）" class="headerlink" title="如何解决梯度消失的问题（待补充）"></a>如何解决梯度消失的问题（待补充）</h1><ol><li>用 $ReLU、LeakyRelu、Elu$ 等激活函数激活函数取代 $sigmoid$ 激活函数。<br>将输出不要固定在0-1之间。$sigmoid$函数的梯度随着 $x$ 的增大或减小和消失，而 $ReLU$ 不会。<br><img src="https://img-blog.csdnimg.cn/3dc7f13adf6c402a94e53d27765e8e92.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_14,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="sigmoid与relu"></li><li>Batch Normalization 就是通过对每一层的输出规范为均值和方差一致的方法，消除了$w$带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。</li><li>ResNet残差结构<br>具体待补充完善，查看 ResNet</li><li>LSTM结构<br>LSTM不太容易发生梯度消失，主要原因在于LSTM内部复杂的“门（gates）”，具体看LSTM基本原理解析</li><li>预训练加finetunning<br>此方法来自Hinton在06年发表的论文上，其基本思想是每次训练一层隐藏层节点，将上一层隐藏层的输出作为输入，而本层的输出作为下一层的输入，这就是逐层预训练。<br>训练完成后，再对整个网络进行“微调（fine-tunning）”。<br>此方法相当于是找全局最优，然后整合起来寻找全局最优，但是现在基本都是直接拿imagenet的预训练模型直接进行finetunning。</li><li>梯度剪切、正则<br>这个方案主要是针对梯度爆炸提出的，其思想是设值一个剪切阈值，如果更新梯度时，梯度超过了这个阈值，那么就将其强制限制在这个范围之内。这样可以防止梯度爆炸。<br>另一种防止梯度爆炸的手段是采用权重正则化，正则化主要是通过对网络权重做正则来限制过拟合，但是根据正则项在损失函数中的形式：<br>可以看出，如果发生梯度爆炸，那么权值的范数就会变的非常大，反过来，通过限制正则化项的大小，也可以在一定程度上限制梯度爆炸的发生。</li></ol><h1 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/25631496">神经网络训练中的梯度消失与梯度爆炸</a></li><li><a href="https://www.jianshu.com/p/3f35e555d5ba">梯度消失和梯度爆炸问题详解</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;梯度消失和梯度爆炸的理解&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;根本原因&quot;&gt;&lt;a href=&quot;#根本原因&quot; class=&quot;headerlink&quot; title=&quot;根本原因&quot;&gt;&lt;/a&gt;根本原因&lt;/h1&gt;&lt;p&gt;梯度消失和梯度爆炸的根本原因是由于深度神经网络</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Keras 实现 Kaggle 数据集 Titanic 预测</title>
    <link href="http://example.com/2021/09/17/Keras%20%E5%AE%9E%E7%8E%B0%20Kaggle%20%E6%95%B0%E6%8D%AE%E9%9B%86%20Titanic%20%E9%A2%84%E6%B5%8B/"/>
    <id>http://example.com/2021/09/17/Keras%20%E5%AE%9E%E7%8E%B0%20Kaggle%20%E6%95%B0%E6%8D%AE%E9%9B%86%20Titanic%20%E9%A2%84%E6%B5%8B/</id>
    <published>2021-09-17T12:24:01.000Z</published>
    <updated>2021-10-29T15:02:16.646Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras 实现 Kaggle 数据集 Titanic 预测">TOC</a></p><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><blockquote><p>使用乘客数据(如姓名、年龄、性别、社会经济阶层等)，建立一个模型预测泰坦尼克号沉船上哪些乘客能够幸存。<br>数据被分成训练集和测试集两组，它们分别在train.csv和test.csv文档中。我们的模型将基于训练集的乘客的性别和阶级等特征建立。在测试集中每个乘客是否幸存的信息是缺省的，其将由我们模型预测出来作为答案提交。</p></blockquote><h1 id="加载本地下载的-Titanic-数据集"><a href="#加载本地下载的-Titanic-数据集" class="headerlink" title="加载本地下载的 Titanic 数据集"></a>加载本地下载的 Titanic 数据集</h1><p>这里使用的 是 pandas 的 read_csv() 方法。 读取的格式为 DataFrame。<br><img src="https://img-blog.csdnimg.cn/0c79c9053cc34cd39d609475f2ac37f3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># xlsx训练数据导入</span></span><br><span class="line">train_filepath = <span class="string">r&quot;../dataset/Titanic/train.csv&quot;</span></span><br><span class="line">train_data = pd.read_csv(train_filepath)</span><br><span class="line">test_filepath = <span class="string">r&quot;../dataset/Titanic/test.csv&quot;</span></span><br><span class="line">test_data = pd.read_csv(test_filepath)</span><br></pre></td></tr></table></figure><br>同样可以用 shape函数查看 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#(891,12)</span></span><br><span class="line"><span class="built_in">print</span>(train_data.shape)</span><br></pre></td></tr></table></figure><h1 id="数据分析与预处理"><a href="#数据分析与预处理" class="headerlink" title="数据分析与预处理"></a>数据分析与预处理</h1><p>在预处理数据前，首先整体分析各项数据对预测模型的重要性</p><p>（1）PassengerID：乘客的ID<br>（2）Survived：乘客是否幸存，取值为0或1，是我们预测/分类的目标。<br>（3）Pclass：客舱等级，可能蕴含着乘客的阶层、乘客客舱的位置等信息，比较重要。<br>（4）Name： 姓名，是无关信息。<br>（5）Sex：性别。灾难来临时常让妇女儿童先走，而同等条件女性体力普遍弱于男性，这些因素都会影响到一名乘客幸存的可能性，因此比较重要。<br>（6）Age：年龄，较为重要，理由同上。<br>（7）Parch：直系亲友数目，比较重要。<br>（8）SibSp：旁系亲友数目，比较重要。<br>（9）Ticket：票编号，是无关信息。<br>（10）Fare：票价，可能会反映乘客的社会阶层等。<br>（11）Cabin：客舱编号，可能会反映客舱位置等，但由于缺省太多，数据量很小不具有代表性，可以视为噪音剔除。<br>（12）Embarked：上船的港口编号。</p><p>在剔除了一些数据后，是否会因信息损失而降低模型的准确度？例如乘客的姓名可能暗含船上乘客之间家庭的关系。实际上我们的模型本来就是建立在不完全观测上（比如我们不知道船上的一对男女乘客有没有发生像Jack和Rose那样的故事），不确定性是必然存在的。把握主要矛盾，舍弃噪音信息是建立模型的一个好思路。</p><h2 id="训练数据预处理方法"><a href="#训练数据预处理方法" class="headerlink" title="训练数据预处理方法"></a>训练数据预处理方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># 训练数据预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PreprocessTrainData</span>(<span class="params">train_data</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理1：筛除无关特征</span></span><br><span class="line">    <span class="comment"># 无关的有 乘客的ID 姓名 票编号客舱编号（数据量太少，当噪声剔除）</span></span><br><span class="line">    <span class="comment"># 是否幸存 客舱等级 性别 年龄 旁系亲友数目  直系亲友数目 票价  上船港口编号</span></span><br><span class="line">    cols=[<span class="string">&#x27;Survived&#x27;</span>, <span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;SibSp&#x27;</span>, <span class="string">&#x27;Parch&#x27;</span>, <span class="string">&#x27;Fare&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>]</span><br><span class="line">    <span class="comment"># colums表示列名  index 表示行名</span></span><br><span class="line">    train_data = pd.DataFrame(train_data, columns=cols)</span><br><span class="line">    <span class="comment">#(891,8)</span></span><br><span class="line">    <span class="built_in">print</span>(train_data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理2：填充缺失特征并标准化特征</span></span><br><span class="line">    age_mean = train_data[<span class="string">&#x27;Age&#x27;</span>].mean()</span><br><span class="line">    <span class="comment"># fillna 为 无值的数据填充</span></span><br><span class="line">    train_data[<span class="string">&#x27;Age&#x27;</span>] = train_data[<span class="string">&#x27;Age&#x27;</span>].fillna(age_mean)</span><br><span class="line"></span><br><span class="line">    fare_mean = train_data[<span class="string">&#x27;Fare&#x27;</span>].mean()</span><br><span class="line">    train_data[<span class="string">&#x27;Fare&#x27;</span>] = train_data[<span class="string">&#x27;Fare&#x27;</span>].fillna(fare_mean)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理3：性别编码0-1  将&#123;&#x27;female&#x27;: 0, &#x27;male&#x27;: 1&#125;</span></span><br><span class="line">    train_data[<span class="string">&#x27;Sex&#x27;</span>]= train_data[<span class="string">&#x27;Sex&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;female&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;male&#x27;</span>: <span class="number">1</span>&#125;).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理4：登港地点转换为one-hot编码</span></span><br><span class="line">    <span class="comment"># 这是将 Embarked这一列 分成 one-hot形式 共有三个港口 所以分成了3列</span></span><br><span class="line">    x_OneHot_df = pd.get_dummies(data=train_data,columns=[<span class="string">&quot;Embarked&quot;</span>])</span><br><span class="line">    ndarray = x_OneHot_df.values</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;ndarray&#x27;</span>,ndarray)</span><br><span class="line">    <span class="comment">#(891,10)</span></span><br><span class="line">    <span class="built_in">print</span>(ndarray.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理5：全体特征标准化，标签向量化</span></span><br><span class="line">    <span class="comment"># &#x27;Survived&#x27;</span></span><br><span class="line">    label = ndarray[:,:<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># label的shape： (891,1)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;label的shape：&quot;</span>,label.shape)</span><br><span class="line">    <span class="comment"># 除了&#x27;Survived&#x27; 其他全部特征</span></span><br><span class="line">    features = ndarray[:,<span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># features的shape： (891, 9)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;features的shape：&quot;</span>, features.shape)</span><br><span class="line">    <span class="comment"># 求一个所有列的平均值</span></span><br><span class="line">    mean = features.mean(axis=<span class="number">0</span>)</span><br><span class="line">    features -= mean</span><br><span class="line">    <span class="comment"># 求一个所有列的方差</span></span><br><span class="line">    std = features.std(axis=<span class="number">0</span>)</span><br><span class="line">    features /= std</span><br><span class="line">    <span class="keyword">return</span> features,label</span><br></pre></td></tr></table></figure><h2 id="测试数据预处理方法"><a href="#测试数据预处理方法" class="headerlink" title="测试数据预处理方法"></a>测试数据预处理方法</h2><p>本质上与训练数据相同，只是少了一列 标签</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试数据预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PreprocessTestData</span>(<span class="params">test_data</span>):</span></span><br><span class="line">    <span class="comment"># 预处理1：筛除无关特征</span></span><br><span class="line">    <span class="comment"># 客舱等级 性别 年龄 旁系亲友数目 直系亲友数目 票价  上船港口编号</span></span><br><span class="line">    cols=[ <span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;SibSp&#x27;</span>, <span class="string">&#x27;Parch&#x27;</span>, <span class="string">&#x27;Fare&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>]</span><br><span class="line">    test_data = test_data[cols]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理2：填充缺失特征并标准化特征</span></span><br><span class="line">    age_mean = test_data[<span class="string">&#x27;Age&#x27;</span>].mean()</span><br><span class="line">    test_data[<span class="string">&#x27;Age&#x27;</span>] = test_data[<span class="string">&#x27;Age&#x27;</span>].fillna(age_mean)</span><br><span class="line"></span><br><span class="line">    fare_mean = test_data[<span class="string">&#x27;Fare&#x27;</span>].mean()</span><br><span class="line">    test_data[<span class="string">&#x27;Fare&#x27;</span>] = test_data[<span class="string">&#x27;Fare&#x27;</span>].fillna(fare_mean)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理3：性别编码0-1</span></span><br><span class="line">    test_data[<span class="string">&#x27;Sex&#x27;</span>]= test_data[<span class="string">&#x27;Sex&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;female&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;male&#x27;</span>: <span class="number">1</span>&#125;).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理4：登港地点转换为one-hot编码</span></span><br><span class="line">    x_OneHot_df = pd.get_dummies(data=test_data,columns=[<span class="string">&quot;Embarked&quot;</span>])</span><br><span class="line">    ndarray = x_OneHot_df.values</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理5：全体特征标准化，标签向量化</span></span><br><span class="line">    features = ndarray</span><br><span class="line">    mean = features.mean(axis=<span class="number">0</span>)</span><br><span class="line">    features -= mean</span><br><span class="line">    std = features.std(axis=<span class="number">0</span>)</span><br><span class="line">    features /= std</span><br><span class="line">    <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure><h2 id="拿到处理后的数据"><a href="#拿到处理后的数据" class="headerlink" title="拿到处理后的数据"></a>拿到处理后的数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train, y_train = PreprocessTrainData(train_data)</span><br><span class="line">x_test = PreprocessTestData(test_data)</span><br></pre></td></tr></table></figure><h1 id="构建网络模型"><a href="#构建网络模型" class="headerlink" title="构建网络模型"></a>构建网络模型</h1><p>构建网络时需要注意控制网络的大小。模型中容量（模型可学习的参数）不足可能导致欠拟合；但模型也不是越大越好，因为模型过大可能导致过拟合，泛化能力下降。其他降低过拟合的方法包括添加dropout正则化、权重正则化等。此外还需要在评估模型（将在下文阐述）的过程中尝试不同的超参数（学习率等）以找到最佳配置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三步 构建网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TitanicModel</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建网络-模型定义</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(input_dim=<span class="number">9</span>,units=<span class="number">64</span>, kernel_regularizer=regularizers.l1_l2(l1=<span class="number">0.001</span>,l2=<span class="number">0.001</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(layers.Dense(units=<span class="number">64</span>, kernel_regularizer=regularizers.l1_l2(l1=<span class="number">0.001</span>,l2=<span class="number">0.001</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(layers.Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建网络-编译模型</span></span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;rmsprop&#x27;</span>, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h1 id="训练模型（带验证集）"><a href="#训练模型（带验证集）" class="headerlink" title="训练模型（带验证集）"></a>训练模型（带验证集）</h1><h2 id="划分测试集和验证集"><a href="#划分测试集和验证集" class="headerlink" title="划分测试集和验证集"></a>划分测试集和验证集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 留出验证集</span></span><br><span class="line">num_val = <span class="number">300</span></span><br><span class="line"><span class="comment"># 将训练集样本顺序 随机打乱</span></span><br><span class="line">np.random.shuffle([x_train,y_train])</span><br><span class="line"><span class="comment"># 取出 前300个 训练样本作为 验证集</span></span><br><span class="line">x_val = x_train[:num_val]</span><br><span class="line"><span class="comment"># 其他的部分作为 真实的训练集</span></span><br><span class="line">partial_x_train = x_train[num_val:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取出前三百个的标签 作为 验证集</span></span><br><span class="line">y_val = y_train[:num_val]</span><br><span class="line"><span class="comment"># 其他的 作为真实的训练集</span></span><br><span class="line">partial_y_train = y_train[num_val:]</span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model = TitanicModel()</span><br><span class="line">model.fit(partial_x_train, partial_y_train, epochs = <span class="number">150</span>, batch_size=<span class="number">16</span>, validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure><h1 id="预测测试样本并评估"><a href="#预测测试样本并评估" class="headerlink" title="预测测试样本并评估"></a>预测测试样本并评估</h1><h2 id="预测测试样本"><a href="#预测测试样本" class="headerlink" title="预测测试样本"></a>预测测试样本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_test2 = model.predict_classes(x_test)</span><br><span class="line"><span class="built_in">print</span>(y_test2)</span><br></pre></td></tr></table></figure><h2 id="读取正确答案"><a href="#读取正确答案" class="headerlink" title="读取正确答案"></a>读取正确答案</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">real_label_filepath = <span class="string">r&quot;../dataset/Titanic/gender_submission.csv&quot;</span></span><br><span class="line">real_label = pd.read_csv(real_label_filepath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取正确数值一列</span></span><br><span class="line">onehot = pd.get_dummies(data=real_label)</span><br><span class="line">xarray = onehot.values</span><br><span class="line">real = xarray[:,<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(real.shape,y_test2.shape)</span><br></pre></td></tr></table></figure><h2 id="输出正确率与保存预测答案"><a href="#输出正确率与保存预测答案" class="headerlink" title="输出正确率与保存预测答案"></a>输出正确率与保存预测答案</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> (y_pre,y_rel) <span class="keyword">in</span> <span class="built_in">zip</span>(y_test2,real):</span><br><span class="line">    <span class="keyword">if</span> y_pre == y_rel:</span><br><span class="line">        count = count+<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;count:&quot;</span>,count)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;这个模型的正确率是：&quot;</span> ,count/y_test2.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;../dataset/Titanic/gender_submission_predict.csv&quot;</span>,<span class="string">&#x27;w+&#x27;</span>,newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    csv_file = csv.writer(f)</span><br><span class="line">    csv_file.writerows(y_test2)</span><br></pre></td></tr></table></figure><h1 id="这个模型的优点"><a href="#这个模型的优点" class="headerlink" title="这个模型的优点"></a>这个模型的优点</h1><p>这个模型 来自 [<a href="https://zhuanlan.zhihu.com/p/278057962?utm_source=wechat_session&amp;ivk_sa=1024320u">https://zhuanlan.zhihu.com/p/278057962?utm_source=wechat_session&amp;ivk_sa=1024320u</a>]<br>此模型在kaggle上排名前12%。总结其优点如下：</p><p>（1）几乎完全没有人工干预。我们并不需要深入理解和分析每种因素对乘客幸存可能性的影响，而只需将数据几乎交由机器自己来学习便能得到准确度极高的预测结果。</p><p>（2）几乎没有引入数据集以外的新信息。引入新信息的行为包括将已知的乘客生存信息填入预测结果（kaggle上实现100%准确率的来源）等。此模型仅在数据处理阶段，引入部分常识判断的信息。</p><p>（3）<strong>模型泛化能力强</strong>。这里的“泛化”是指在模型建立过程中没有对该问题“过拟合”。实质上一味追求此问题的预测准确率是没有意义的。过度分析并设计复杂的特征工程也许可以提高测试集的准确率，但实质上很可能是对该问题的过拟合，不能在其他类似问题上泛化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras 实现 Kaggle 数据集 Titanic 预测&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h1&gt;&lt;blockqu</summary>
      
    
    
    
    <category term="机器学习基础" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="Kaggle" scheme="http://example.com/tags/Kaggle/"/>
    
  </entry>
  
</feed>
