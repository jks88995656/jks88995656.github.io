<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一只柴犬</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-09-21T15:24:29.376Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>凯凯超人</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Pytorch GoogleNet中的Inception</title>
    <link href="http://example.com/2021/09/21/Pytorch%20GoogleNet%E4%B8%AD%E7%9A%84Inception/"/>
    <id>http://example.com/2021/09/21/Pytorch%20GoogleNet%E4%B8%AD%E7%9A%84Inception/</id>
    <published>2021-09-21T15:40:01.000Z</published>
    <updated>2021-09-21T15:24:29.376Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch GoogleNet中的Inception">TOC</a></p><h1 id="GoogleNet-的概念"><a href="#GoogleNet-的概念" class="headerlink" title="GoogleNet 的概念"></a>GoogleNet 的概念</h1><p>是基于AlexNet，VGG 后的模型</p><h1 id="特殊点：Inception"><a href="#特殊点：Inception" class="headerlink" title="特殊点：Inception"></a>特殊点：Inception</h1><h2 id="为什么要提出-Inception"><a href="#为什么要提出-Inception" class="headerlink" title="为什么要提出 Inception"></a>为什么要提出 Inception</h2><p>一般来说，提升网络性能最直接的办法是<strong>增加网络深度和宽度</strong>，但一味地增加，会带来诸多问题：</p><ol><li>参数太多，如果训练数据集有限，很容易产生过拟合； </li><li>网络越大、参数越多，计算复杂度越大，难以应用；</li><li>网络越深，容易出现梯度消失问题（梯度越往后穿越容易消失），难以优化模型。</li></ol><blockquote><p><strong>梯度消失和梯度爆炸</strong> 是什么？<br>查看文章   <a href="https://www.jianshu.com/p/ece360b7fabb">如何理解梯度消失和梯度爆炸</a><br>以及本博客文章 <a href="https://jks88995656.github.io/2021/09/20/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E7%90%86%E8%A7%A3/">梯度消失和梯度爆炸的理解</a></p></blockquote><p>我们希望在增加网络深度和宽度的同时减少参数，为了减少参数，自然就想到将<strong>全连接变成稀疏连接</strong>。但是在实现上，全连接变成稀疏连接后实际计算量并不会有质的提升，因为<font color="blue">大部分硬件是针对密集矩阵计算优化的，稀疏矩阵虽然数据量少，但是计算所消耗的时间却很难减少。</font><br>在这种需求和形势下，Google研究人员提出了Inception的方法。</p><h2 id="Inception-模块的结构"><a href="#Inception-模块的结构" class="headerlink" title="Inception 模块的结构"></a>Inception 模块的结构</h2><p>Inception 模块 是GoogleNet 重复使用的重要部分。其最大的特点是 引入了 1×1 的卷积核。其目的是用于 缩小通道数，将像素信息融合，也叫做 <strong>通道压缩</strong>。<br><img src="https://img-blog.csdnimg.cn/img_convert/9f3c90923dc228229c3f6c1695777fa3.png#pic_center" alt="在这里插入图片描述"><br>同时其可以减少卷积核的参数数量 例如:如下的操作数对比（28×28表示 卷积的时候图片像素点也要乘的啊）<br><img src="https://img-blog.csdnimg.cn/img_convert/7e7182fa5f7155ca6f80ac1cc6689c08.png#pic_center" alt="参数变化 用1×1后"><br>Inception 模块的内容如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/d98aae54e7979a26c11393e6c0fb7ad3.png#pic_center" alt="Inception 模块结构"></p><h2 id="Pytorch-实现-Inception-模块"><a href="#Pytorch-实现-Inception-模块" class="headerlink" title="Pytorch 实现 Inception 模块"></a>Pytorch 实现 Inception 模块</h2><p>下图中为每个部分的代码模块。 每个部分的 上侧是 pytorch中网络初始化部分，下侧是 pytorch中网络前馈实现的部分。<br><img src="https://img-blog.csdnimg.cn/img_convert/065b0f6abeae7092776e18a5b4755ac3.png#pic_center" alt="各部分实现图"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionA</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一个部分</span></span><br><span class="line">        self.branch_pool = torch.nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个部分</span></span><br><span class="line">        self.branch1x1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三个部分</span></span><br><span class="line">        self.branch5x5_1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = torch.nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第四个部分</span></span><br><span class="line">        self.branch3x3_1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = torch.nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_3 = torch.nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 第一个部分</span></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个部分</span></span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三个部分</span></span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第四个部分</span></span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">        branch3x3 = self.branch3x3_3(branch3x3)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通道整合</span></span><br><span class="line">        outputs = [branch_pool, branch1x1, branch5x5, branch3x3]</span><br><span class="line">        <span class="comment"># 整合通道 通道的位置在1处  （batch,通道,宽度,长度）</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="GoogleNet-网络的整体模型"><a href="#GoogleNet-网络的整体模型" class="headerlink" title="GoogleNet 网络的整体模型"></a>GoogleNet 网络的整体模型</h1><p><img src="https://img-blog.csdnimg.cn/img_convert/42c1625cbe9b8e8c37de67bfbf3d319a.png#pic_center" alt="GoogleNet 全貌"></p><h1 id="使用Mnist-使用Inception"><a href="#使用Mnist-使用Inception" class="headerlink" title="使用Mnist 使用Inception"></a>使用Mnist 使用Inception</h1><h2 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">88</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.incep1 = InceptionA(in_channels=<span class="number">10</span>)</span><br><span class="line">        self.incep2 = InceptionA(in_channels=<span class="number">20</span>)</span><br><span class="line">        self.mp = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">1408</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.mp(self.conv1(x)))</span><br><span class="line">        x = self.incep1(x)</span><br><span class="line">        x = F.relu(self.mp(self.conv2(x)))</span><br><span class="line">        x = self.incep2(x)</span><br><span class="line">        <span class="comment"># 变成列向量</span></span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="Mnist-数据集-训练-整体代码"><a href="#Mnist-数据集-训练-整体代码" class="headerlink" title="Mnist 数据集 训练 整体代码"></a>Mnist 数据集 训练 整体代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步 准备数据</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 用均值和方差进行归一化</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">])</span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                               train=<span class="literal">True</span>,</span><br><span class="line">                               download=<span class="literal">True</span>,</span><br><span class="line">                               transform=transform)</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                              train=<span class="literal">False</span>,</span><br><span class="line">                              download=<span class="literal">True</span>,</span><br><span class="line">                              transform=transform)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset,</span><br><span class="line">                          shuffle=<span class="literal">True</span>,</span><br><span class="line">                          batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(test_dataset,</span><br><span class="line">                         shuffle=<span class="literal">False</span>,</span><br><span class="line">                         batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InceptionA</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第一个部分</span></span><br><span class="line">        self.branch_pool = torch.nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个部分</span></span><br><span class="line">        self.branch1x1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三个部分</span></span><br><span class="line">        self.branch5x5_1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = torch.nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第四个部分</span></span><br><span class="line">        self.branch3x3_1 = torch.nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = torch.nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_3 = torch.nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 第一个部分</span></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二个部分</span></span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三个部分</span></span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第四个部分</span></span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">        branch3x3 = self.branch3x3_3(branch3x3)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通道整合</span></span><br><span class="line">        outputs = [branch_pool, branch1x1, branch5x5, branch3x3]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">88</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.incep1 = InceptionA(in_channels=<span class="number">10</span>)</span><br><span class="line">        self.incep2 = InceptionA(in_channels=<span class="number">20</span>)</span><br><span class="line">        self.mp = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">1408</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.mp(self.conv1(x)))</span><br><span class="line">        x = self.incep1(x)</span><br><span class="line">        x = F.relu(self.mp(self.conv2(x)))</span><br><span class="line">        x = self.incep2(x)</span><br><span class="line">        <span class="comment"># 变成列向量</span></span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个网络模型</span></span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 定义损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义测试函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            outputs = model(data)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">            correct += predicted.eq(target.view_as(predicted)).<span class="built_in">sum</span>().item()</span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%) \n&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss, correct, <span class="built_in">len</span>(test_loader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    )) \</span><br><span class="line"> \</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epochs</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">                <span class="comment"># prepare data</span></span><br><span class="line">                inputs, labels = data</span><br><span class="line">                <span class="comment"># 前馈</span></span><br><span class="line">                y_predict = model(inputs)</span><br><span class="line">                loss = criterion(y_predict, labels)</span><br><span class="line">                <span class="comment"># 反馈</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 更新</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                <span class="keyword">if</span> (batch_idx + <span class="number">1</span>) % <span class="number">30</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                        epoch, batch_idx * <span class="built_in">len</span>(data), <span class="built_in">len</span>(train_loader.dataset),</span><br><span class="line">                               <span class="number">100.</span> * batch_idx / <span class="built_in">len</span>(train_loader), loss.item()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">    test()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h1><p><a href="https://zhuanlan.zhihu.com/p/73857137">深度学习|经典网络：GoogLeNet（一）</a><br><a href="https://zhuanlan.zhihu.com/p/89002063">GoogLeNet</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch GoogleNet中的Inception&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;GoogleNet-的概念&quot;&gt;&lt;a href=&quot;#GoogleNet-的概念&quot; class=&quot;headerlink&quot; title=&quot;GoogleNet 的</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="GoogleNet" scheme="http://example.com/tags/GoogleNet/"/>
    
  </entry>
  
  <entry>
    <title>梯度消失和梯度爆炸的理解</title>
    <link href="http://example.com/2021/09/20/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>http://example.com/2021/09/20/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2021-09-20T11:31:01.000Z</published>
    <updated>2021-09-21T15:06:43.194Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="梯度消失和梯度爆炸的理解">TOC</a></p><h1 id="根本原因"><a href="#根本原因" class="headerlink" title="根本原因"></a>根本原因</h1><p>梯度消失和梯度爆炸的根本原因是由于深度神经网络过长的链，在反向传播通过链式法则求导过程中产生的。 换句话说，就是<strong>反向传播先天就有一定的毛病</strong>。</p><h1 id="根本原因的理解"><a href="#根本原因的理解" class="headerlink" title="根本原因的理解"></a>根本原因的理解</h1><p><img src="https://img-blog.csdnimg.cn/beb6cdfcd994483c9133ee88c0763034.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="前馈和反向传播过程"><br>左上角为 正向反馈的步骤（σ 表示 激活函数 $sigmoid$ ）。 Loss 使用的是 MSE损失函数。 根据反向传播，我们可以看到最后 loss 对 b1 参数的梯度。 可以看到 连乘的情况。</p><ul><li>看这个式子里的 $w<em>{i}$ 中，一般我们初始化权重参数 $w</em>{i}$ 时，通常都小于1。</li><li>激活函数 $sigmoid$  的求导如下所示 <img src="https://img-blog.csdnimg.cn/9f175038b5e74cfda605e8906a194416.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="求导sigmoid函数"></li><li>激活函数的导数 图像如下图所示：<img src="https://img-blog.csdnimg.cn/fe08669e4f3648ea89dec0b560ba422b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="sigmoid函数求导"></li><li>所以$|σ’(z) × w| ≤ 0.25$，多个小于1的数连乘之后，那将会越来越小，导致靠近输入层的层的权重的偏导几乎为0，也就是说梯度几乎为0，导致参数基本不更新，这就是梯度消失的根本原因。<br>梯度爆炸的原因，也就是说如果$|σ’(z) × w| ≥ 1$，连乘下来就会导致梯度过大，导致梯度更新幅度特别大，可能会溢出，导致模型无法收敛。<br>但 $sigmoid$ 的函数是不可能大于1了，上图看的很清楚，那只能是参数 $w<em>{i}$了，故只有当 $abs(w)&gt;4$ 时才可能出现梯度爆炸，<strong>这也就是经常看到别人博客里的一句话，初始权重过大</strong>。<br>但梯度爆炸的情况一般不会发生，对于$sigmoid$ 函数来说，$σ’(z)$的大小也与 $w</em>{i}$ 有关。<br><strong>其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。</strong></li></ul><h1 id="如何解决梯度消失的问题（待补充）"><a href="#如何解决梯度消失的问题（待补充）" class="headerlink" title="如何解决梯度消失的问题（待补充）"></a>如何解决梯度消失的问题（待补充）</h1><ol><li>用 $ReLU、LeakyRelu、Elu$ 等激活函数激活函数取代 $sigmoid$ 激活函数。<br>将输出不要固定在0-1之间。$sigmoid$函数的梯度随着 $x$ 的增大或减小和消失，而 $ReLU$ 不会。<br><img src="https://img-blog.csdnimg.cn/3dc7f13adf6c402a94e53d27765e8e92.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_14,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="sigmoid与relu"></li><li>Batch Normalization 就是通过对每一层的输出规范为均值和方差一致的方法，消除了$w$带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。</li><li>ResNet残差结构<br>具体待补充完善，查看 ResNet</li><li>LSTM结构<br>LSTM不太容易发生梯度消失，主要原因在于LSTM内部复杂的“门（gates）”，具体看LSTM基本原理解析</li><li>预训练加finetunning<br>此方法来自Hinton在06年发表的论文上，其基本思想是每次训练一层隐藏层节点，将上一层隐藏层的输出作为输入，而本层的输出作为下一层的输入，这就是逐层预训练。<br>训练完成后，再对整个网络进行“微调（fine-tunning）”。<br>此方法相当于是找全局最优，然后整合起来寻找全局最优，但是现在基本都是直接拿imagenet的预训练模型直接进行finetunning。</li><li>梯度剪切、正则<br>这个方案主要是针对梯度爆炸提出的，其思想是设值一个剪切阈值，如果更新梯度时，梯度超过了这个阈值，那么就将其强制限制在这个范围之内。这样可以防止梯度爆炸。<br>另一种防止梯度爆炸的手段是采用权重正则化，正则化主要是通过对网络权重做正则来限制过拟合，但是根据正则项在损失函数中的形式：<br>可以看出，如果发生梯度爆炸，那么权值的范数就会变的非常大，反过来，通过限制正则化项的大小，也可以在一定程度上限制梯度爆炸的发生。</li></ol><h1 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/25631496">神经网络训练中的梯度消失与梯度爆炸</a></li><li><a href="https://www.jianshu.com/p/3f35e555d5ba">梯度消失和梯度爆炸问题详解</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;梯度消失和梯度爆炸的理解&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;根本原因&quot;&gt;&lt;a href=&quot;#根本原因&quot; class=&quot;headerlink&quot; title=&quot;根本原因&quot;&gt;&lt;/a&gt;根本原因&lt;/h1&gt;&lt;p&gt;梯度消失和梯度爆炸的根本原因是由于深度神经网络</summary>
      
    
    
    
    <category term="深度学习基础" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Keras 实现 Kaggle 数据集 Titanic 预测</title>
    <link href="http://example.com/2021/09/17/Keras%20%E5%AE%9E%E7%8E%B0%20Kaggle%20%E6%95%B0%E6%8D%AE%E9%9B%86%20Titanic%20%E9%A2%84%E6%B5%8B/"/>
    <id>http://example.com/2021/09/17/Keras%20%E5%AE%9E%E7%8E%B0%20Kaggle%20%E6%95%B0%E6%8D%AE%E9%9B%86%20Titanic%20%E9%A2%84%E6%B5%8B/</id>
    <published>2021-09-17T12:24:01.000Z</published>
    <updated>2021-09-17T12:25:41.234Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras 实现 Kaggle 数据集 Titanic 预测">TOC</a></p><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><blockquote><p>使用乘客数据(如姓名、年龄、性别、社会经济阶层等)，建立一个模型预测泰坦尼克号沉船上哪些乘客能够幸存。<br>数据被分成训练集和测试集两组，它们分别在train.csv和test.csv文档中。我们的模型将基于训练集的乘客的性别和阶级等特征建立。在测试集中每个乘客是否幸存的信息是缺省的，其将由我们模型预测出来作为答案提交。</p></blockquote><h1 id="加载本地下载的-Titanic-数据集"><a href="#加载本地下载的-Titanic-数据集" class="headerlink" title="加载本地下载的 Titanic 数据集"></a>加载本地下载的 Titanic 数据集</h1><p>这里使用的 是 pandas 的 read_csv() 方法。 读取的格式为 DataFrame。<br><img src="https://img-blog.csdnimg.cn/0c79c9053cc34cd39d609475f2ac37f3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># xlsx训练数据导入</span></span><br><span class="line">train_filepath = <span class="string">r&quot;../dataset/Titanic/train.csv&quot;</span></span><br><span class="line">train_data = pd.read_csv(train_filepath)</span><br><span class="line">test_filepath = <span class="string">r&quot;../dataset/Titanic/test.csv&quot;</span></span><br><span class="line">test_data = pd.read_csv(test_filepath)</span><br></pre></td></tr></table></figure><br>同样可以用 shape函数查看 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#(891,12)</span></span><br><span class="line"><span class="built_in">print</span>(train_data.shape)</span><br></pre></td></tr></table></figure><h1 id="数据分析与预处理"><a href="#数据分析与预处理" class="headerlink" title="数据分析与预处理"></a>数据分析与预处理</h1><p>在预处理数据前，首先整体分析各项数据对预测模型的重要性</p><p>（1）PassengerID：乘客的ID<br>（2）Survived：乘客是否幸存，取值为0或1，是我们预测/分类的目标。<br>（3）Pclass：客舱等级，可能蕴含着乘客的阶层、乘客客舱的位置等信息，比较重要。<br>（4）Name： 姓名，是无关信息。<br>（5）Sex：性别。灾难来临时常让妇女儿童先走，而同等条件女性体力普遍弱于男性，这些因素都会影响到一名乘客幸存的可能性，因此比较重要。<br>（6）Age：年龄，较为重要，理由同上。<br>（7）Parch：直系亲友数目，比较重要。<br>（8）SibSp：旁系亲友数目，比较重要。<br>（9）Ticket：票编号，是无关信息。<br>（10）Fare：票价，可能会反映乘客的社会阶层等。<br>（11）Cabin：客舱编号，可能会反映客舱位置等，但由于缺省太多，数据量很小不具有代表性，可以视为噪音剔除。<br>（12）Embarked：上船的港口编号。</p><p>在剔除了一些数据后，是否会因信息损失而降低模型的准确度？例如乘客的姓名可能暗含船上乘客之间家庭的关系。实际上我们的模型本来就是建立在不完全观测上（比如我们不知道船上的一对男女乘客有没有发生像Jack和Rose那样的故事），不确定性是必然存在的。把握主要矛盾，舍弃噪音信息是建立模型的一个好思路。</p><h2 id="训练数据预处理方法"><a href="#训练数据预处理方法" class="headerlink" title="训练数据预处理方法"></a>训练数据预处理方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># 训练数据预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PreprocessTrainData</span>(<span class="params">train_data</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理1：筛除无关特征</span></span><br><span class="line">    <span class="comment"># 无关的有 乘客的ID 姓名 票编号客舱编号（数据量太少，当噪声剔除）</span></span><br><span class="line">    <span class="comment"># 是否幸存 客舱等级 性别 年龄 旁系亲友数目  直系亲友数目 票价  上船港口编号</span></span><br><span class="line">    cols=[<span class="string">&#x27;Survived&#x27;</span>, <span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;SibSp&#x27;</span>, <span class="string">&#x27;Parch&#x27;</span>, <span class="string">&#x27;Fare&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>]</span><br><span class="line">    <span class="comment"># colums表示列名  index 表示行名</span></span><br><span class="line">    train_data = pd.DataFrame(train_data, columns=cols)</span><br><span class="line">    <span class="comment">#(891,8)</span></span><br><span class="line">    <span class="built_in">print</span>(train_data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理2：填充缺失特征并标准化特征</span></span><br><span class="line">    age_mean = train_data[<span class="string">&#x27;Age&#x27;</span>].mean()</span><br><span class="line">    <span class="comment"># fillna 为 无值的数据填充</span></span><br><span class="line">    train_data[<span class="string">&#x27;Age&#x27;</span>] = train_data[<span class="string">&#x27;Age&#x27;</span>].fillna(age_mean)</span><br><span class="line"></span><br><span class="line">    fare_mean = train_data[<span class="string">&#x27;Fare&#x27;</span>].mean()</span><br><span class="line">    train_data[<span class="string">&#x27;Fare&#x27;</span>] = train_data[<span class="string">&#x27;Fare&#x27;</span>].fillna(fare_mean)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理3：性别编码0-1  将&#123;&#x27;female&#x27;: 0, &#x27;male&#x27;: 1&#125;</span></span><br><span class="line">    train_data[<span class="string">&#x27;Sex&#x27;</span>]= train_data[<span class="string">&#x27;Sex&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;female&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;male&#x27;</span>: <span class="number">1</span>&#125;).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理4：登港地点转换为one-hot编码</span></span><br><span class="line">    <span class="comment"># 这是将 Embarked这一列 分成 one-hot形式 共有三个港口 所以分成了3列</span></span><br><span class="line">    x_OneHot_df = pd.get_dummies(data=train_data,columns=[<span class="string">&quot;Embarked&quot;</span>])</span><br><span class="line">    ndarray = x_OneHot_df.values</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;ndarray&#x27;</span>,ndarray)</span><br><span class="line">    <span class="comment">#(891,10)</span></span><br><span class="line">    <span class="built_in">print</span>(ndarray.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理5：全体特征标准化，标签向量化</span></span><br><span class="line">    <span class="comment"># &#x27;Survived&#x27;</span></span><br><span class="line">    label = ndarray[:,:<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># label的shape： (891,1)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;label的shape：&quot;</span>,label.shape)</span><br><span class="line">    <span class="comment"># 除了&#x27;Survived&#x27; 其他全部特征</span></span><br><span class="line">    features = ndarray[:,<span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># features的shape： (891, 9)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;features的shape：&quot;</span>, features.shape)</span><br><span class="line">    <span class="comment"># 求一个所有列的平均值</span></span><br><span class="line">    mean = features.mean(axis=<span class="number">0</span>)</span><br><span class="line">    features -= mean</span><br><span class="line">    <span class="comment"># 求一个所有列的方差</span></span><br><span class="line">    std = features.std(axis=<span class="number">0</span>)</span><br><span class="line">    features /= std</span><br><span class="line">    <span class="keyword">return</span> features,label</span><br></pre></td></tr></table></figure><h2 id="测试数据预处理方法"><a href="#测试数据预处理方法" class="headerlink" title="测试数据预处理方法"></a>测试数据预处理方法</h2><p>本质上与训练数据相同，只是少了一列 标签</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试数据预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PreprocessTestData</span>(<span class="params">test_data</span>):</span></span><br><span class="line">    <span class="comment"># 预处理1：筛除无关特征</span></span><br><span class="line">    <span class="comment"># 客舱等级 性别 年龄 旁系亲友数目 直系亲友数目 票价  上船港口编号</span></span><br><span class="line">    cols=[ <span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;SibSp&#x27;</span>, <span class="string">&#x27;Parch&#x27;</span>, <span class="string">&#x27;Fare&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>]</span><br><span class="line">    test_data = test_data[cols]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理2：填充缺失特征并标准化特征</span></span><br><span class="line">    age_mean = test_data[<span class="string">&#x27;Age&#x27;</span>].mean()</span><br><span class="line">    test_data[<span class="string">&#x27;Age&#x27;</span>] = test_data[<span class="string">&#x27;Age&#x27;</span>].fillna(age_mean)</span><br><span class="line"></span><br><span class="line">    fare_mean = test_data[<span class="string">&#x27;Fare&#x27;</span>].mean()</span><br><span class="line">    test_data[<span class="string">&#x27;Fare&#x27;</span>] = test_data[<span class="string">&#x27;Fare&#x27;</span>].fillna(fare_mean)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理3：性别编码0-1</span></span><br><span class="line">    test_data[<span class="string">&#x27;Sex&#x27;</span>]= test_data[<span class="string">&#x27;Sex&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;female&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;male&#x27;</span>: <span class="number">1</span>&#125;).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理4：登港地点转换为one-hot编码</span></span><br><span class="line">    x_OneHot_df = pd.get_dummies(data=test_data,columns=[<span class="string">&quot;Embarked&quot;</span>])</span><br><span class="line">    ndarray = x_OneHot_df.values</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预处理5：全体特征标准化，标签向量化</span></span><br><span class="line">    features = ndarray</span><br><span class="line">    mean = features.mean(axis=<span class="number">0</span>)</span><br><span class="line">    features -= mean</span><br><span class="line">    std = features.std(axis=<span class="number">0</span>)</span><br><span class="line">    features /= std</span><br><span class="line">    <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure><h2 id="拿到处理后的数据"><a href="#拿到处理后的数据" class="headerlink" title="拿到处理后的数据"></a>拿到处理后的数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train, y_train = PreprocessTrainData(train_data)</span><br><span class="line">x_test = PreprocessTestData(test_data)</span><br></pre></td></tr></table></figure><h1 id="构建网络模型"><a href="#构建网络模型" class="headerlink" title="构建网络模型"></a>构建网络模型</h1><p>构建网络时需要注意控制网络的大小。模型中容量（模型可学习的参数）不足可能导致欠拟合；但模型也不是越大越好，因为模型过大可能导致过拟合，泛化能力下降。其他降低过拟合的方法包括添加dropout正则化、权重正则化等。此外还需要在评估模型（将在下文阐述）的过程中尝试不同的超参数（学习率等）以找到最佳配置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三步 构建网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TitanicModel</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建网络-模型定义</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(input_dim=<span class="number">9</span>,units=<span class="number">64</span>, kernel_regularizer=regularizers.l1_l2(l1=<span class="number">0.001</span>,l2=<span class="number">0.001</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(layers.Dense(units=<span class="number">64</span>, kernel_regularizer=regularizers.l1_l2(l1=<span class="number">0.001</span>,l2=<span class="number">0.001</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">    model.add(layers.Dense(units=<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建网络-编译模型</span></span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;rmsprop&#x27;</span>, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h1 id="训练模型（带验证集）"><a href="#训练模型（带验证集）" class="headerlink" title="训练模型（带验证集）"></a>训练模型（带验证集）</h1><h2 id="划分测试集和验证集"><a href="#划分测试集和验证集" class="headerlink" title="划分测试集和验证集"></a>划分测试集和验证集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 留出验证集</span></span><br><span class="line">num_val = <span class="number">300</span></span><br><span class="line"><span class="comment"># 将训练集样本顺序 随机打乱</span></span><br><span class="line">np.random.shuffle([x_train,y_train])</span><br><span class="line"><span class="comment"># 取出 前300个 训练样本作为 验证集</span></span><br><span class="line">x_val = x_train[:num_val]</span><br><span class="line"><span class="comment"># 其他的部分作为 真实的训练集</span></span><br><span class="line">partial_x_train = x_train[num_val:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取出前三百个的标签 作为 验证集</span></span><br><span class="line">y_val = y_train[:num_val]</span><br><span class="line"><span class="comment"># 其他的 作为真实的训练集</span></span><br><span class="line">partial_y_train = y_train[num_val:]</span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model = TitanicModel()</span><br><span class="line">model.fit(partial_x_train, partial_y_train, epochs = <span class="number">150</span>, batch_size=<span class="number">16</span>, validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure><h1 id="预测测试样本并评估"><a href="#预测测试样本并评估" class="headerlink" title="预测测试样本并评估"></a>预测测试样本并评估</h1><h2 id="预测测试样本"><a href="#预测测试样本" class="headerlink" title="预测测试样本"></a>预测测试样本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_test2 = model.predict_classes(x_test)</span><br><span class="line"><span class="built_in">print</span>(y_test2)</span><br></pre></td></tr></table></figure><h2 id="读取正确答案"><a href="#读取正确答案" class="headerlink" title="读取正确答案"></a>读取正确答案</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">real_label_filepath = <span class="string">r&quot;../dataset/Titanic/gender_submission.csv&quot;</span></span><br><span class="line">real_label = pd.read_csv(real_label_filepath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取正确数值一列</span></span><br><span class="line">onehot = pd.get_dummies(data=real_label)</span><br><span class="line">xarray = onehot.values</span><br><span class="line">real = xarray[:,<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(real.shape,y_test2.shape)</span><br></pre></td></tr></table></figure><h2 id="输出正确率与保存预测答案"><a href="#输出正确率与保存预测答案" class="headerlink" title="输出正确率与保存预测答案"></a>输出正确率与保存预测答案</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> (y_pre,y_rel) <span class="keyword">in</span> <span class="built_in">zip</span>(y_test2,real):</span><br><span class="line">    <span class="keyword">if</span> y_pre == y_rel:</span><br><span class="line">        count = count+<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;count:&quot;</span>,count)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;这个模型的正确率是：&quot;</span> ,count/y_test2.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">r&quot;../dataset/Titanic/gender_submission_predict.csv&quot;</span>,<span class="string">&#x27;w+&#x27;</span>,newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    csv_file = csv.writer(f)</span><br><span class="line">    csv_file.writerows(y_test2)</span><br></pre></td></tr></table></figure><h1 id="这个模型的优点"><a href="#这个模型的优点" class="headerlink" title="这个模型的优点"></a>这个模型的优点</h1><p>这个模型 来自 [<a href="https://zhuanlan.zhihu.com/p/278057962?utm_source=wechat_session&amp;ivk_sa=1024320u">https://zhuanlan.zhihu.com/p/278057962?utm_source=wechat_session&amp;ivk_sa=1024320u</a>]<br>此模型在kaggle上排名前12%。总结其优点如下：</p><p>（1）几乎完全没有人工干预。我们并不需要深入理解和分析每种因素对乘客幸存可能性的影响，而只需将数据几乎交由机器自己来学习便能得到准确度极高的预测结果。</p><p>（2）几乎没有引入数据集以外的新信息。引入新信息的行为包括将已知的乘客生存信息填入预测结果（kaggle上实现100%准确率的来源）等。此模型仅在数据处理阶段，引入部分常识判断的信息。</p><p>（3）<strong>模型泛化能力强</strong>。这里的“泛化”是指在模型建立过程中没有对该问题“过拟合”。实质上一味追求此问题的预测准确率是没有意义的。过度分析并设计复杂的特征工程也许可以提高测试集的准确率，但实质上很可能是对该问题的过拟合，不能在其他类似问题上泛化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras 实现 Kaggle 数据集 Titanic 预测&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h1&gt;&lt;blockqu</summary>
      
    
    
    
    <category term="Keras" scheme="http://example.com/categories/Keras/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="Kaggle" scheme="http://example.com/tags/Kaggle/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 封装函数</title>
    <link href="http://example.com/2021/09/17/Pytorch%20%E5%B0%81%E8%A3%85%E5%87%BD%E6%95%B0/"/>
    <id>http://example.com/2021/09/17/Pytorch%20%E5%B0%81%E8%A3%85%E5%87%BD%E6%95%B0/</id>
    <published>2021-09-17T11:40:01.000Z</published>
    <updated>2021-09-17T12:23:25.342Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 封装函数">TOC</a></p><h1 id="torch-tensor-view"><a href="#torch-tensor-view" class="headerlink" title="torch.tensor.view"></a>torch.tensor.view</h1><p>Tensor.view(*shape) → Tensor<br> view()的作用相当于numpy中的reshape，重新定义矩阵的形状。<br> <img src="https://img-blog.csdnimg.cn/95e47412ecb142e1b2f5dfcb469d9762.png#pic_center" alt="在这里插入图片描述"><br>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">a = torch.<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">&gt;&gt;a:tensor([ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>,</span><br><span class="line">        <span class="number">15.</span>, <span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>, <span class="number">20.</span>, <span class="number">21.</span>, <span class="number">22.</span>, <span class="number">23.</span>, <span class="number">24.</span>, <span class="number">25.</span>, <span class="number">26.</span>, <span class="number">27.</span>, <span class="number">28.</span>,</span><br><span class="line">        <span class="number">29.</span>, <span class="number">30.</span>])</span><br><span class="line"></span><br><span class="line">b = a.view(<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line">&gt;&gt;b:tensor([[[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">         [ <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">         [<span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>, <span class="number">20.</span>],</span><br><span class="line">         [<span class="number">21.</span>, <span class="number">22.</span>, <span class="number">23.</span>, <span class="number">24.</span>, <span class="number">25.</span>],</span><br><span class="line">         [<span class="number">26.</span>, <span class="number">27.</span>, <span class="number">28.</span>, <span class="number">29.</span>, <span class="number">30.</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(b.view(b.size(<span class="number">0</span>),-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(b.view(b.size(<span class="number">1</span>),-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(b.view(b.size(<span class="number">2</span>),-<span class="number">1</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>b是一个2组3行5列，<br>b.size(0)就是留下2组，后面3行5列拉直成15个数，行成2行15列；<br>b.size(1)就是留下3行，2组5列拉成10个数，行成3行10列；<br>b.size(2)就是留下5列，2组3行拉成6个数，行成5行6列</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 封装函数&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;torch-tensor-view&quot;&gt;&lt;a href=&quot;#torch-tensor-view&quot; class=&quot;headerlink&quot; title=&quot;torch.tensor.view&quot;&gt;</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>数据集加载的各种方式方法</title>
    <link href="http://example.com/2021/09/17/%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E7%9A%84%E5%90%84%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%96%B9%E6%B3%95/"/>
    <id>http://example.com/2021/09/17/%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E7%9A%84%E5%90%84%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%96%B9%E6%B3%95/</id>
    <published>2021-09-17T11:37:01.000Z</published>
    <updated>2021-09-17T12:21:53.678Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="数据集加载的各种方式方法">TOC</a></p><h1 id="np-loadtxt-读取本地-csv-文件"><a href="#np-loadtxt-读取本地-csv-文件" class="headerlink" title="np.loadtxt 读取本地 csv 文件"></a>np.loadtxt 读取本地 csv 文件</h1><p>读入本地 csv 文件内容。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    numpy读取本地文件</span></span><br><span class="line"><span class="string">    delimiter 数据的分割号</span></span><br><span class="line"><span class="string">    dtype 读取数据类型  一般机器学习类的都是float32 </span></span><br><span class="line"><span class="string">    因为显卡一般他内核里面是按32位工作的</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;./dataset/diabetes.csv&#x27;</span>,delimiter=<span class="string">&quot;,&quot;</span>,dtype=np.float32)</span><br></pre></td></tr></table></figure><p>用Pytorch使用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">x_train = torch.from_numpy(xy[:-<span class="number">1</span>,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line">y_train = torch.from_numpy(xy[:-<span class="number">1</span>,[-<span class="number">1</span>]])</span><br></pre></td></tr></table></figure></p><h1 id="批次读取-本地-csv-文件"><a href="#批次读取-本地-csv-文件" class="headerlink" title="批次读取 本地 csv 文件"></a>批次读取 本地 csv 文件</h1><p>使用 DataLoader+Dataset </p><h2 id="Step1：引入包"><a href="#Step1：引入包" class="headerlink" title="Step1：引入包"></a>Step1：引入包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dataset 是个抽象类，所以其不可以被实例化</span></span><br><span class="line"><span class="comment"># Dataset 可以为其他的子类所继承的</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br></pre></td></tr></table></figure><h2 id="Step2：定义一个自己的Dataset类-并继承原有的抽象类Dataset"><a href="#Step2：定义一个自己的Dataset类-并继承原有的抽象类Dataset" class="headerlink" title="Step2：定义一个自己的Dataset类 并继承原有的抽象类Dataset"></a>Step2：定义一个自己的Dataset类 并继承原有的抽象类Dataset</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DiabetesDataset 继承 Dataset</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filepath</span>):</span></span><br><span class="line">        xy = np.loadtxt(filepath, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">        <span class="comment"># 一共有几个样本</span></span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">        self.x_train = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_train = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 继承Dataset的方法并重写</span></span><br><span class="line">    <span class="comment"># 其实用来帮助找索引位置的  dataset[index]</span></span><br><span class="line">    <span class="comment"># 函数功能是根据index索引去返回数据样本以及标签label</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.x_train[index], self.y_train[index]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 函数功能是用来查看数据的长度，也就是 dataset 样本的数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个类</span></span><br><span class="line">diabetesdataset = DiabetesDataset(<span class="string">&#x27;../dataset/diabetes.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Step3：使用DataLoader"><a href="#Step3：使用DataLoader" class="headerlink" title="Step3：使用DataLoader"></a>Step3：使用DataLoader</h2><p>其返回的是 对应索引的数据<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个 loader</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    batch_size 就是批次大小</span></span><br><span class="line"><span class="string">    shuffle True的话就表示要打乱数据</span></span><br><span class="line"><span class="string">    num_workers  读取Mni-batch的时候要多进程</span></span><br><span class="line"><span class="string">                2就表示由2个进程进行读取</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 返回 （x,y）</span></span><br><span class="line">train_loader = DataLoader(dataset=diabetesdataset, batch_size=<span class="number">32</span>, num_workers=<span class="number">2</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></p><h2 id="Step4：用迭代的方式拿到数据"><a href="#Step4：用迭代的方式拿到数据" class="headerlink" title="Step4：用迭代的方式拿到数据"></a>Step4：用迭代的方式拿到数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            <span class="comment"># 1. Prepare data</span></span><br><span class="line">            inputs, labels = data</span><br><span class="line">            <span class="comment"># 2. Forward</span></span><br><span class="line">            y_pred = FullLeanerModel(inputs)</span><br><span class="line">            loss = criterion(y_pred, labels)</span><br><span class="line">            <span class="comment"># 3. Backward</span></span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">            loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">            <span class="comment"># 4. Update</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure><h1 id="用-pandas-的-read-csv-方法"><a href="#用-pandas-的-read-csv-方法" class="headerlink" title="用 pandas 的 read_csv 方法"></a>用 pandas 的 read_csv 方法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_filepath = <span class="string">r&quot;../dataset/Titanic/train.csv&quot;</span></span><br><span class="line">train_data = pd.read_csv(train_filepath)</span><br></pre></td></tr></table></figure><p>pandas需要使用 DataFrame 的形式<br><img src="https://img-blog.csdnimg.cn/cb5ee55c852d4a66a120888969fdb170.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>可用 pandas.DataFrame()来转换成 DataFrame 格式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否幸存 客舱等级 性别 年龄 旁系亲友数目  直系亲友数目 票价  上船港口编号</span></span><br><span class="line">cols=[<span class="string">&#x27;Survived&#x27;</span>, <span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;SibSp&#x27;</span>, <span class="string">&#x27;Parch&#x27;</span>, <span class="string">&#x27;Fare&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>]</span><br><span class="line"><span class="comment"># colums表示列名  index 表示行名   选择需要的列</span></span><br><span class="line">train_data = pd.DataFrame(train_data, columns=cols)</span><br></pre></td></tr></table></figure><p>可以用 ndarray 将DataFrame 转为 普通的numpy 读取出来。如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#x27;Survived&#x27;</span></span><br><span class="line">label = ndarray[:,:<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 除了&#x27;Survived&#x27; 其他全部特征</span></span><br><span class="line">features = ndarray[:,<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;数据集加载的各种方式方法&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;np-loadtxt-读取本地-csv-文件&quot;&gt;&lt;a href=&quot;#np-loadtxt-读取本地-csv-文件&quot; class=&quot;headerlink&quot; title=&quot;np.loadtx</summary>
      
    
    
    
    <category term="数据集加载" scheme="http://example.com/categories/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8A%A0%E8%BD%BD/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="数据集" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch CNN概念性解决</title>
    <link href="http://example.com/2021/09/16/Pytorch%20CNN%E6%A6%82%E5%BF%B5%E6%80%A7%E8%A7%A3%E5%86%B3/"/>
    <id>http://example.com/2021/09/16/Pytorch%20CNN%E6%A6%82%E5%BF%B5%E6%80%A7%E8%A7%A3%E5%86%B3/</id>
    <published>2021-09-16T12:40:01.000Z</published>
    <updated>2021-09-17T12:19:07.363Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch CNN概念性解决">TOC</a><br>需要知道的几点</p><ol><li>卷积核里的参数都是超参数，是通过训练进行调整的</li></ol><h1 id="CNN的流程"><a href="#CNN的流程" class="headerlink" title="CNN的流程"></a>CNN的流程</h1><p>input——&gt;convolution——&gt;max pooling——&gt;…——&gt;flatten（压平）——&gt;fully connected network——&gt;output</p><h1 id="CNN的卷积操作运算"><a href="#CNN的卷积操作运算" class="headerlink" title="CNN的卷积操作运算"></a>CNN的卷积操作运算</h1><h2 id="单通道图像"><a href="#单通道图像" class="headerlink" title="单通道图像"></a>单通道图像</h2><p><img src="https://img-blog.csdnimg.cn/f37680551d064da8b5dfe82c2e998a8d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="卷积计算方式"></p><h2 id="多通道图像-如三通道RGB"><a href="#多通道图像-如三通道RGB" class="headerlink" title="多通道图像  如三通道RGB"></a>多通道图像  如三通道RGB</h2><p>用三个卷积核，分别对3个通道各自卷积，然后再相加整合输出。<br><img src="https://img-blog.csdnimg.cn/379e12b328b14abda21ae5a551f94f9a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="RGB卷积"></p><h1 id="如何看待卷积核的个数和通道数"><a href="#如何看待卷积核的个数和通道数" class="headerlink" title="如何看待卷积核的个数和通道数"></a>如何看待卷积核的个数和通道数</h1><p><img src="https://img-blog.csdnimg.cn/21b1a72bd38e4c7984d1b65e3e0f1d2f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="卷积核的个数和通道数"><br>记住两条规律就好：</p><ul><li>卷积核通道数 = 输入通道数</li><li>卷积核个数 = 输出通道数</li></ul><p>注意卷积层一般都是4维的 包括：批次，输入通道数，卷积核大小（宽，高）<br>可以参考下面的卷积程序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入通道数为5  输出通道数为10</span></span><br><span class="line">in_channels,out_channels = <span class="number">5</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 初始化图像高度</span></span><br><span class="line">width,height = <span class="number">100</span>,<span class="number">100</span></span><br><span class="line"><span class="comment"># 卷积核的大小</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line"><span class="comment"># 批次</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"><span class="comment"># 随机生成100×100的 图像</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(batch_size,in_channels,width,height)</span><br><span class="line"><span class="comment"># 设置卷积层 输入维度，输出维度，卷积核大小</span></span><br><span class="line">conv_layer = torch.nn.Conv2d(in_channels,out_channels,kernel_size=kernel_size)</span><br><span class="line"><span class="comment"># 将图片放入卷积层 输出结果</span></span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([1, 5, 100, 100])</span></span><br><span class="line"><span class="comment"># 分别为 batch_size,图片通道数，图像宽100，图像高100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"><span class="comment"># torch.Size([1, 10, 98, 98])</span></span><br><span class="line"><span class="comment"># 分别为 batch_size,输出通道数10（其实就是卷积核的个数），图像卷积后（由于卷积核是3×3 所以减去2）</span></span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="comment"># torch.Size([10, 5, 3, 3])</span></span><br><span class="line"><span class="comment"># 输出通道（卷积核数量） 输入通道（卷积核通道数） 卷积核大小</span></span><br><span class="line"><span class="built_in">print</span>(conv_layer.weight.shape)</span><br></pre></td></tr></table></figure><h1 id="如何看待-padding"><a href="#如何看待-padding" class="headerlink" title="如何看待 padding"></a>如何看待 padding</h1><p><strong>padding的目的是，为了规定输出图像的大小。</strong><br>例如 原来是  5×5 的原图，通过卷积核为 3×3 那 输出的图 是 3×3 的<br>但如果设置 padding 为1，那么相当于把原图扩充为 7×7 了。<br><img src="https://img-blog.csdnimg.cn/d6da81ad35f544e19c1b32e73f70714f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="padding为1时候的卷积"><br>可以参考一下如下的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 假设的灰度图片像素值 图片为5×5</span></span><br><span class="line"><span class="built_in">input</span> = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">7</span>,</span><br><span class="line">         <span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">2</span>,</span><br><span class="line">         <span class="number">1</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">4</span>,</span><br><span class="line">         <span class="number">9</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">2</span>,</span><br><span class="line">         <span class="number">3</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 将输入的图片变成Tensor类型，并且reshape一下</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; 参数从左到右分别为</span></span><br><span class="line"><span class="string">    batch_size,in_channels,width,height</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积核的大小</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; 从左到右分别为 </span></span><br><span class="line"><span class="string">    in_channels,out_channels,kernel_size</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">conv_layer = torch.nn.Conv2d(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,padding=<span class="number">1</span>,bias=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    自定义一个tensor向量 再将其 reshape</span></span><br><span class="line"><span class="string">    batch_size,in_channels,width,height</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">kernel = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]).view(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 初始化卷积层</span></span><br><span class="line">conv_layer.weight.data = kernel.data</span><br><span class="line"></span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># torch.Size([1, 1, 5, 5])</span></span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/8123cd0423f9446ba513f9e8b03bb0d0.png#pic_center" alt="输出结果"></p><h1 id="如何看待步长-stride"><a href="#如何看待步长-stride" class="headerlink" title="如何看待步长 stride"></a>如何看待步长 stride</h1><p>设置一下 stride就可以了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_layer = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, bias=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><br>改变步长的意义，其实就是为了缩小输出尺寸</p><h1 id="如何看待Max-Pooling"><a href="#如何看待Max-Pooling" class="headerlink" title="如何看待Max Pooling"></a>如何看待Max Pooling</h1><p><img src="https://img-blog.csdnimg.cn/84eff2cc1acd44c79a6cf251af071e3d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="MaxPooling示意图"><br>可以参考如下的代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = [ <span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">5</span>,</span><br><span class="line">          <span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,</span><br><span class="line">          <span class="number">1</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,</span><br><span class="line">          <span class="number">9</span>,<span class="number">7</span>,<span class="number">4</span>,<span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入的 batch_size为1 通道数为1 也就是灰度图像 大小是4×4</span></span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">#  默认的步长 stride也是2</span></span><br><span class="line">maxpooling_layer = torch.nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">output = maxpooling_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    这里只进行了一个 maxpooling操作</span></span><br><span class="line"><span class="string">    input = [ 3,4,6,5,</span></span><br><span class="line"><span class="string">            2,4,6,8,</span></span><br><span class="line"><span class="string">            1,6,7,8,</span></span><br><span class="line"><span class="string">            9,7,4,6] </span></span><br><span class="line"><span class="string">    变为</span></span><br><span class="line"><span class="string">       [4,8</span></span><br><span class="line"><span class="string">        9,8]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="设计一个网络并实现"><a href="#设计一个网络并实现" class="headerlink" title="设计一个网络并实现"></a>设计一个网络并实现</h1><h2 id="如图所示为-要设计的网络模型"><a href="#如图所示为-要设计的网络模型" class="headerlink" title="如图所示为 要设计的网络模型"></a>如图所示为 要设计的网络模型</h2><p><img src="https://img-blog.csdnimg.cn/b352a2f5626544ae924453466305321f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="要设计的网络模型"></p><h2 id="转换成流程图"><a href="#转换成流程图" class="headerlink" title="转换成流程图"></a>转换成流程图</h2><p><img src="https://img-blog.csdnimg.cn/5bbf95bc996345c9a8ae13238acaa8ad.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h2 id="Pytorch-代码实现这个网络"><a href="#Pytorch-代码实现这个网络" class="headerlink" title="Pytorch 代码实现这个网络"></a>Pytorch 代码实现这个网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设计网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNNNet</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNNNet, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = torch.nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># Flatten data from (n, 1, 28, 28) to (n, 784)</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = self.pooling(torch.relu(self.conv1(x)))</span><br><span class="line">        x = self.pooling(torch.relu(self.conv2(x)))</span><br><span class="line">        <span class="comment"># flatten (n,320)</span></span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 实例化这个网络</span></span><br><span class="line">CNNmodel = CNNNet()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch CNN概念性解决&quot;&gt;TOC&lt;/a&gt;&lt;br&gt;需要知道的几点&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;卷积核里的参数都是超参数，是通过训练进行调整的&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;CNN的流程&quot;&gt;&lt;a href=&quot;#CNN的流程&quot; class=&quot;h</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="CNN" scheme="http://example.com/tags/CNN/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 多分类问题的解决</title>
    <link href="http://example.com/2021/09/15/Pytorch%20%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/"/>
    <id>http://example.com/2021/09/15/Pytorch%20%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/</id>
    <published>2021-09-15T11:40:01.000Z</published>
    <updated>2021-09-17T12:20:57.433Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 多分类问题的解决">TOC</a></p><h1 id="多分类问题-激活函数的选择-Softmax"><a href="#多分类问题-激活函数的选择-Softmax" class="headerlink" title="多分类问题 激活函数的选择 Softmax"></a>多分类问题 激活函数的选择 Softmax</h1><p><img src="https://img-blog.csdnimg.cn/87f8f15a7fc0480d95ba08f55f1c6a4c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="应该解决的条件"><br>选择 Softmax 函数的关键点是：</p><ul><li>确保输出的每一个概率都是 ≥ 0 的</li><li>所有的概率之和应该为 1</li></ul><p>Softmax 激活函数完美解决了这个问题：</p><p><img src="https://img-blog.csdnimg.cn/a3a45fd36d8d4f45a7c7347e26e7d744.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="Softmax公式"><br>举个简单的例子<br><img src="https://img-blog.csdnimg.cn/799dea5f0b8348cea9c294567d552202.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="算数案例"></p><h1 id="CrossEntropyLoss-和-NLLLoss区别"><a href="#CrossEntropyLoss-和-NLLLoss区别" class="headerlink" title="CrossEntropyLoss 和 NLLLoss区别"></a>CrossEntropyLoss 和 NLLLoss区别</h1><p>NLLLoss全称是Negative Log Likelyhood Loss，负对数似然损失函数。<br>softmax + NLLLoss = CrossEntropyLoss</p><p>在Pytorch中</p><ul><li>CrossEntropyLoss可以直接接到模型结果之后，直接得出交叉熵损失。</li><li>NLLLoss需要在模型结果后先接一个Softmax，将模型结果变成概率，再用NLLLoss求预测损失。</li></ul><h1 id="实现梯度下降"><a href="#实现梯度下降" class="headerlink" title="实现梯度下降"></a>实现梯度下降</h1><h2 id="Step1：准备数据"><a href="#Step1：准备数据" class="headerlink" title="Step1：准备数据"></a>Step1：准备数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure><h2 id="Step2：初始化参数w"><a href="#Step2：初始化参数w" class="headerlink" title="Step2：初始化参数w"></a>Step2：初始化参数w</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始值w为1</span></span><br><span class="line">w = <span class="number">1.0</span></span><br></pre></td></tr></table></figure><h2 id="Step3：定义模型"><a href="#Step3：定义模型" class="headerlink" title="Step3：定义模型"></a>Step3：定义模型</h2><p><img src="https://img-blog.csdnimg.cn/45983f52cebc4ab2b4d3cf4e0ee7e16b.png#pic_center" alt="在这里插入图片描述"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x_data</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x_data * w</span><br></pre></td></tr></table></figure></p><h2 id="Step4：定义损失函数"><a href="#Step4：定义损失函数" class="headerlink" title="Step4：定义损失函数"></a>Step4：定义损失函数</h2><p><img src="https://img-blog.csdnimg.cn/fb6f96943d1b46a696049f6c07965484.png#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x_data, y_data</span>):</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        loss = loss + (y_pred - y) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> loss / <span class="built_in">len</span>(x_data)</span><br></pre></td></tr></table></figure><h2 id="Step5：定义梯度"><a href="#Step5：定义梯度" class="headerlink" title="Step5：定义梯度"></a>Step5：定义梯度</h2><p><img src="https://img-blog.csdnimg.cn/845af8941e0c4ee08fe574a29c869950.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求梯度 也就是 损失函数对w的偏导</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x_data, y_data</span>):</span></span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    x = [1, 2, 3]</span></span><br><span class="line"><span class="string">    y = [4, 5, 6, 7]</span></span><br><span class="line"><span class="string">    xy = zip(x, y)</span></span><br><span class="line"><span class="string">    print xy</span></span><br><span class="line"><span class="string">    运行的结果是： [(1, 4), (2, 5), (3, 6)]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        grad = grad + <span class="number">2</span> * x * (x * w - y)</span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(x_data)</span><br></pre></td></tr></table></figure><h2 id="Step6：训练并更新参数-w"><a href="#Step6：训练并更新参数-w" class="headerlink" title="Step6：训练并更新参数 w"></a>Step6：训练并更新参数 w</h2><p><img src="https://img-blog.csdnimg.cn/731d4beb873c41938a42b58461044c56.png#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 没训练过的时候 w是初始值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"><span class="comment"># 这批数据样本 训练 99次</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 每次都是计算一整个数据集的 平均loss</span></span><br><span class="line">    loss_val = loss(x_data, y_data)</span><br><span class="line">    grad_val = gradient(x_data, y_data)</span><br><span class="line">    w = w - <span class="number">0.01</span> * grad_val</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, loss_val)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><h1 id="实现多分类"><a href="#实现多分类" class="headerlink" title="实现多分类"></a>实现多分类</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步 准备数据</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 用均值和方差进行归一化</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">])</span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                               train=<span class="literal">True</span>,</span><br><span class="line">                               download=<span class="literal">True</span>,</span><br><span class="line">                               transform=transform)</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;../dataset/mnist/&#x27;</span>,</span><br><span class="line">                              train=<span class="literal">False</span>,</span><br><span class="line">                              download=<span class="literal">True</span>,</span><br><span class="line">                              transform=transform)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset,</span><br><span class="line">                          shuffle=<span class="literal">True</span>,</span><br><span class="line">                          batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(test_dataset,</span><br><span class="line">                         shuffle=<span class="literal">False</span>,</span><br><span class="line">                         batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步 设计模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 这个是必须写的</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 构建层次模型</span></span><br><span class="line">        self.l1 = torch.nn.Linear(<span class="number">784</span>, <span class="number">512</span>)</span><br><span class="line">        self.l2 = torch.nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.l3 = torch.nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.l4 = torch.nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.l5 = torch.nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">784</span>)</span><br><span class="line">        x = F.relu(self.l1(x))</span><br><span class="line">        x = F.relu(self.l2(x))</span><br><span class="line">        x = F.relu(self.l3(x))</span><br><span class="line">        x = F.relu(self.l4(x))</span><br><span class="line">        <span class="keyword">return</span> self.l5(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个网络模型</span></span><br><span class="line">model = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 定义损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步 训练模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 这个0 的意思表示 索引从0开始计</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        inputs, target = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + update</span></span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch + <span class="number">1</span>, batch_idx + <span class="number">1</span>, running_loss / <span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五步 测试并检验</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            <span class="comment">#输出每行最大的那个</span></span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, dim=<span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set: %d %%&#x27;</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 多分类问题的解决&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;多分类问题-激活函数的选择-Softmax&quot;&gt;&lt;a href=&quot;#多分类问题-激活函数的选择-Softmax&quot; class=&quot;headerlink&quot; title=&quot;多分类问题 激</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="分类问题" scheme="http://example.com/tags/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 数据读取 DataLoader与Dataset 概念</title>
    <link href="http://example.com/2021/09/13/Pytorch%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%20DataLoader%E4%B8%8EDataset%20%E6%A6%82%E5%BF%B5/"/>
    <id>http://example.com/2021/09/13/Pytorch%20%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%20DataLoader%E4%B8%8EDataset%20%E6%A6%82%E5%BF%B5/</id>
    <published>2021-09-13T13:17:01.000Z</published>
    <updated>2021-09-13T13:17:51.173Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 数据读取 DataLoader与Dataset 概念">TOC</a></p><p>在机器学习中，我们对数据的处理主要分为4个阶段，如下图所示：<br><img src="https://img-blog.csdnimg.cn/b32a357d69624036babcbab934d6ea0f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><ol><li>第一步，收集需要的数据，数据包括原始样本 和 对应标签；</li><li>第二步，对数据集进行划分，把数据集划分为 <strong>训练集、验证集和测试集</strong> ；训练集用于训练模型，验证集用于验证模型是否过拟合（也可以理解为用验证集挑选模型的超参数），测试集用于测试模型的性能，测试模型的泛化能力；</li><li><p>第三步，从本地读取数据，要按 Mini-batch 分批训练。一起训练，内存不够。使用的方法为 <strong>DataLoader</strong> ，其又分为两个部分：</p><ol><li><strong>Sample</strong>  用于生成索引，即样本的序号；</li><li><strong>Dataset</strong> 是根据索引去读取图片以及对应的标签；</li></ol></li><li><p>第四步，数据预处理，把数据读取进来往往还需要对数据进行一系列的图像预处理，比如说<strong>数据的中心化，标准化，旋转或者翻转</strong>等等。Pytorch 中数据预处理是通过 <strong>transforms</strong> 进行处理的；</p></li></ol><h1 id="DataLoader-和-Dataset"><a href="#DataLoader-和-Dataset" class="headerlink" title="DataLoader 和 Dataset"></a>DataLoader 和 Dataset</h1><h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p><img src="https://img-blog.csdnimg.cn/d48e19e8e3a44038a1ee9e38c54ae4dd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>torch.utils.data.DataLoader</strong></p><p>功能：构建可迭代的数据装载器；<br>dataset: Dataset类，决定数据从哪里读取及如何读取；<br>batchsize：批次样本数量大小；<br>num_works:是否多进程读取数据； 可以设置为几个进程<br>shuffle：每个epoch是否乱序；<br>drop_last：当样本数不能被 batchsize 整除时，是否舍弃最后一批数据；</p></blockquote><p>Epoch，Iteration，Batchsize的区别</p><blockquote><p>Epoch：所有训练样本都已输入到模型中，称为一个Epoch<br>Iteration：一批样本输入到模型中，称之为一个Iteration；<br>Batchsize：批次样本数量大小，决定一个Epoch中有多少个Iteration； Iteration = Epoch ➗ Batchsize<br><img src="https://img-blog.csdnimg.cn/9819b23c3821402ca0fe804a641bfd00.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">样本总数：<span class="number">87</span>，Batchsize=<span class="number">8</span> （样本不能被Batchsize整除）</span><br><span class="line"><span class="number">1</span> Epoch = <span class="number">10</span> Iteration，drop_last = <span class="literal">True</span></span><br><span class="line"><span class="number">1</span> Epoch = <span class="number">11</span> Iteration， drop_last = <span class="literal">False</span></span><br></pre></td></tr></table></figure><br>用法例如：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_loader = DataLoader(dataset=diabetesdataset,batch_size=<span class="number">32</span>,num_workers=<span class="number">2</span>,shuffle=<span class="literal">True</span>,drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p><img src="https://img-blog.csdnimg.cn/4cb1ec48dac047f3b025dd652a96e11b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><blockquote><p><strong>torch.utils.data.Dataset</strong><br>Dataset是用来定义数据从哪里读取，以及如何读取的问题；</p><p><font color="red">功能：Dataset抽象类，所有自定义的Dataset需要继承它，并且重写<strong>getitem</strong>()； </font><br><strong> getitem </strong>()：接收一个索引，返回一个样本</p></blockquote><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DiabetesDataset 继承 Dataset</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,filepath</span>):</span></span><br><span class="line">        xy = np.loadtxt(filepath,delimiter=<span class="string">&#x27;,&#x27;</span>,dtype=np.float32)</span><br><span class="line">        <span class="comment"># 一共有几个样本</span></span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">        self.x_train = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_train = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 继承Dataset的方法并重写</span></span><br><span class="line">    <span class="comment"># 其实用来帮助找索引位置的  dataset[index]</span></span><br><span class="line">    <span class="comment"># 函数功能是根据index索引去返回数据样本以及标签label</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.x_train[index],self.y_train[index]</span><br><span class="line">    <span class="comment"># 函数功能是用来查看数据的长度，也就是 dataset 样本的数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br></pre></td></tr></table></figure><h2 id="Dataloader的运行机制"><a href="#Dataloader的运行机制" class="headerlink" title="Dataloader的运行机制"></a>Dataloader的运行机制</h2><h3 id="数据读取的三个问题：1、读哪些数据；2、从哪读数据；3、怎么读数据？"><a href="#数据读取的三个问题：1、读哪些数据；2、从哪读数据；3、怎么读数据？" class="headerlink" title="数据读取的三个问题：1、读哪些数据；2、从哪读数据；3、怎么读数据？"></a>数据读取的三个问题：1、读哪些数据；2、从哪读数据；3、怎么读数据？</h3><ol><li>从代码中可以发现，index 是从 sampler.py 中输出的，所以读哪些数据是由sampler得到的；</li><li>从代码中看，是从Dataset中的 文件地址参数 告诉我们 Pytorch 是从硬盘中的哪一个文件夹获取数据；</li><li>从代码中可以发现，Pytorch是从Dataset的getitem()中具体实现的，根据索引去读取数据；</li></ol><h3 id="DataLoader数据读取流程"><a href="#DataLoader数据读取流程" class="headerlink" title="DataLoader数据读取流程"></a>DataLoader数据读取流程</h3><p><img src="https://img-blog.csdnimg.cn/323718d3de4e415d9ba5af77011315cc.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>简单描述一下流程图，首先在for循环中去使用DataLoader，进入DataLoader之后是否采用多进程进入DataLoaderlter，进入DataLoaderIter之后会使用sampler去获取Index，拿到索引之后传输到DatasetFetcher，在DatasetFetcher中会调用Dataset，Dataset根据给定的Index，在getitem中从硬盘里面去读取实际的Img和Label，读取了一个batch_size的数据之后，通过一个collate_fn将数据进行整理，整理成batch_Data的形式，接着就可以输入到模型中训练；</p><p><strong>读哪些是由Sampler决定的，从哪读是由Dataset决定的，怎么读是由getitem决定的</strong></p><h1 id="详细原文转载"><a href="#详细原文转载" class="headerlink" title="详细原文转载"></a>详细原文转载</h1><p><a href="https://blog.csdn.net/qq_37388085/article/details/102663166">https://blog.csdn.net/qq_37388085/article/details/102663166</a></p><h1 id="实验代码"><a href="#实验代码" class="headerlink" title="实验代码"></a>实验代码</h1><p>糖尿病案例。 数据读取采用 批处理 DataLoader。<br>理论上 测试集和训练集 都应该分别有一个 DataLoader</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Dataset 是个抽象类，所以其不可以被实例化</span></span><br><span class="line"><span class="comment"># Dataset 可以为其他的子类所继承的</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;../dataset/diabetes.csv&#x27;</span>, delimiter=<span class="string">&quot;,&quot;</span>, dtype=np.float32)</span><br><span class="line"><span class="comment"># (759,9) 759行 9列</span></span><br><span class="line"><span class="comment"># 当测试样本 的 一组数据</span></span><br><span class="line">x_test = torch.from_numpy(xy[-<span class="number">1</span>:, :-<span class="number">1</span>])</span><br><span class="line">y_test = torch.from_numpy(xy[-<span class="number">1</span>:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DiabetesDataset 继承 Dataset</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DiabetesDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, filepath</span>):</span></span><br><span class="line">        xy = np.loadtxt(filepath, delimiter=<span class="string">&#x27;,&#x27;</span>, dtype=np.float32)</span><br><span class="line">        <span class="comment"># 一共有几个样本</span></span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">        self.x_train = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">        self.y_train = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 继承Dataset的方法并重写</span></span><br><span class="line">    <span class="comment"># 其实用来帮助找索引位置的  dataset[index]</span></span><br><span class="line">    <span class="comment"># 函数功能是根据index索引去返回数据样本以及标签label</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.x_train[index], self.y_train[index]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 函数功能是用来查看数据的长度，也就是 dataset 样本的数量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个类</span></span><br><span class="line">diabetesdataset = DiabetesDataset(<span class="string">&#x27;../dataset/diabetes.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个 loader</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    batch_size 就是批次大小</span></span><br><span class="line"><span class="string">    shuffle True的话就表示要打乱数据</span></span><br><span class="line"><span class="string">    num_workers  读取Mni-batch的时候要多线程</span></span><br><span class="line"><span class="string">                2就表示由2个线程进行读取</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 返回 （x,y）</span></span><br><span class="line">train_loader = DataLoader(dataset=diabetesdataset, batch_size=<span class="number">32</span>, num_workers=<span class="number">2</span>, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二部 定义模型</span></span><br><span class="line"><span class="comment"># 这个其实有点类似与 神经网络的结构了 每个线性层后面都加了一个sigmoid函数 做了次非线性变换</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullLeanerModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 定义多层线性模型的结构</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FullLeanerModel, self).__init__()</span><br><span class="line">        <span class="comment"># 第一个线性转换 将8维转换为6维</span></span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前馈 运行</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 用一个变量比较 简单</span></span><br><span class="line">        x = self.sigmoid(self.linear1(x))</span><br><span class="line">        x = self.sigmoid(self.linear2(x))</span><br><span class="line">        x = self.sigmoid(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">FullLeanerModel = FullLeanerModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 定义 损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(FullLeanerModel.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">            <span class="comment"># 1. Prepare data</span></span><br><span class="line">            inputs, labels = data</span><br><span class="line">            <span class="comment"># 2. Forward</span></span><br><span class="line">            y_pred = FullLeanerModel(inputs)</span><br><span class="line">            loss = criterion(y_pred, labels)</span><br><span class="line">            <span class="comment"># 3. Backward</span></span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">            loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">            <span class="comment"># 4. Update</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第五步 评估模型</span></span><br><span class="line">    <span class="comment"># 输出各层  权重w 和 偏置b</span></span><br><span class="line">    <span class="keyword">for</span> weight, bias <span class="keyword">in</span> FullLeanerModel.state_dict().items():  <span class="comment"># param is weight or bias(Tensor)</span></span><br><span class="line">        <span class="built_in">print</span>(weight, bias)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    y_yuce = FullLeanerModel(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;测试样本的预测值为&quot;</span>, y_yuce.data, <span class="string">&quot;实际样本的标签值为&quot;</span>, y_test.data)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://img-blog.csdnimg.cn/6df0d9baee164ff69400094abdfff61a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 数据读取 DataLoader与Dataset 概念&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在机器学习中，我们对数据的处理主要分为4个阶段，如下图所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/b32a</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="DataLoader" scheme="http://example.com/tags/DataLoader/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 实现 多维特征的输入——糖尿病预测</title>
    <link href="http://example.com/2021/09/13/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E7%9A%84%E8%BE%93%E5%85%A5%E2%80%94%E2%80%94%E7%B3%96%E5%B0%BF%E7%97%85%E9%A2%84%E6%B5%8B/"/>
    <id>http://example.com/2021/09/13/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E7%9A%84%E8%BE%93%E5%85%A5%E2%80%94%E2%80%94%E7%B3%96%E5%B0%BF%E7%97%85%E9%A2%84%E6%B5%8B/</id>
    <published>2021-09-13T03:30:01.000Z</published>
    <updated>2021-09-13T13:24:52.355Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 实现 多维特征的输入——糖尿病预测">TOC</a></p><p>整体的设计思路<br>大致的设计步骤 分为5步 如下所示：<br><img src="https://img-blog.csdnimg.cn/66ed2d3a9e914f229e8c67a8248b1dee.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_9,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="实现的步骤"><br>第五步：是进行 评估模型并预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h1 id="Step1：准备数据"><a href="#Step1：准备数据" class="headerlink" title="Step1：准备数据"></a>Step1：准备数据</h1><p>数据格式包含759个样本，其中有8个特征已经其是否会加剧糖尿病的预测标签（1或0）。<br><img src="https://img-blog.csdnimg.cn/7475f37ead424166967915beceedd5ec.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="数据维度预览"><br>我们将759个样本，分为758个训练样本以及1个测试样本。</p><p>先读入本地 csv 文件内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    numpy读取本地文件</span></span><br><span class="line"><span class="string">    delimiter 分割号</span></span><br><span class="line"><span class="string">    dtype 读取数据类型  一般机器学习类的都是float32 </span></span><br><span class="line"><span class="string">    因为显卡一般他内核里面是按32位工作的</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;./dataset/diabetes.csv&#x27;</span>,delimiter=<span class="string">&quot;,&quot;</span>,dtype=np.float32)</span><br></pre></td></tr></table></figure><p>可以查看一下 读入的数据维度情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (759,9) 759行 9列</span></span><br><span class="line"><span class="built_in">print</span>(xy.shape)</span><br></pre></td></tr></table></figure><p>其中758个样本作为测试集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">x_train = torch.from_numpy(xy[:-<span class="number">1</span>,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line">y_train = torch.from_numpy(xy[:-<span class="number">1</span>,[-<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p>最后一个样本作为训练集，用于预测结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当测试样本 的 一组数据</span></span><br><span class="line">x_test = torch.from_numpy(xy[-<span class="number">1</span>:,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_test.shape)</span><br><span class="line">y_test = torch.from_numpy(xy[-<span class="number">1</span>:,[-<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><h1 id="Step2：定义模型"><a href="#Step2：定义模型" class="headerlink" title="Step2：定义模型"></a>Step2：定义模型</h1><p>同样每一层都为 逻辑回归模型。但这边有8个特征，所以导入的应该是个矩阵。<br><img src="https://img-blog.csdnimg.cn/79a71c6a9f0245d9914e4fcef6607793.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="怎么构建线性模型"><br><img src="https://img-blog.csdnimg.cn/ad19f89d0a57481e8f2bdb1e37e8d7ee.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="数据维度的变换 输入n行8个特征得到n行1列的预测"></p><h2 id="torch-nn-Linear-in-features-out-features-bias-True-方法"><a href="#torch-nn-Linear-in-features-out-features-bias-True-方法" class="headerlink" title="torch.nn.Linear(in_features,out_features,bias=True) 方法"></a>torch.nn.Linear(in_features,out_features,bias=True) 方法</h2><p><img src="https://img-blog.csdnimg.cn/0e75527101244e56a7687acb4eaa9315.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="torch.nn.Linear函数详解"><br>下面这个函数 使得8维度线性变换为6维度<br>注：1.整个模型都是以 列向量为操作单位的（这么做其实是为了利用计算机的并行计算能力加快训练速度），所以维度指的是有多少列，比如左下角这个维度就是8。<br>2.激活函数的引入，其实就是为了引入非线性的因素。这样就可以使得我们可以非线性的变换矩阵维度。<br><img src="https://img-blog.csdnimg.cn/1c73a9cbdba047818bda33c1c9ab517f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="8D到6D的真正含义"><br>下面是我们 构建的整体的 线性网络模型<br><img src="https://img-blog.csdnimg.cn/06e1c8bb27054087bce6ccb4c0eee8c0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="整体网络模型"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二部 定义模型</span></span><br><span class="line"><span class="comment"># 这个其实有点类似与 神经网络的结构了 每个线性层后面都加了一个sigmoid函数 做了次非线性变换</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullLeanerModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 定义多层线性模型的结构</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FullLeanerModel,self).__init__()</span><br><span class="line">        <span class="comment"># 第一个线性转换 将8维转换为6维</span></span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>,<span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>,<span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前馈 运行输出结果</span></span><br><span class="line">    <span class="comment"># 会被自动调用 是方法的重写</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="comment"># 用一个变量比较 简单</span></span><br><span class="line">        x = self.sigmoid(self.linear1(x))</span><br><span class="line">        x = self.sigmoid(self.linear2(x))</span><br><span class="line">        x = self.sigmoid(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span>  x</span><br></pre></td></tr></table></figure><p>实例化这个模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">FullLeanerModel = FullLeanerModel()</span><br></pre></td></tr></table></figure><h1 id="Step3：定义损失函数和优化器"><a href="#Step3：定义损失函数和优化器" class="headerlink" title="Step3：定义损失函数和优化器"></a>Step3：定义损失函数和优化器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三步 定义 损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(FullLeanerModel.parameters(),lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h1 id="Step4：训练模型"><a href="#Step4：训练模型" class="headerlink" title="Step4：训练模型"></a>Step4：训练模型</h1><p>训练模型100次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>):</span><br><span class="line">    y_pred = FullLeanerModel(x_train)</span><br><span class="line">    loss = criterion(y_pred,y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br></pre></td></tr></table></figure><h1 id="评估模型并预测"><a href="#评估模型并预测" class="headerlink" title="评估模型并预测"></a>评估模型并预测</h1><h2 id="输出所有层次的-权重和偏置"><a href="#输出所有层次的-权重和偏置" class="headerlink" title="输出所有层次的 权重和偏置"></a>输出所有层次的 权重和偏置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># 输出各层  权重w 和 偏置b</span></span><br><span class="line"><span class="keyword">for</span> weight, bias <span class="keyword">in</span> FullLeanerModel.state_dict().items():  <span class="comment"># param is weight or bias(Tensor)</span></span><br><span class="line"><span class="built_in">print</span>( weight,bias)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/5c3c93e1544a4005873d7fabd4f88e51.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="各层次的权重和偏置值"></p><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_yuce = FullLeanerModel(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试样本的预测值为&quot;</span>,y_yuce.data,<span class="string">&quot;实际样本的标签值为&quot;</span>,y_test.data)</span><br></pre></td></tr></table></figure><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    numpy读取本地文件</span></span><br><span class="line"><span class="string">    delimiter 分割号</span></span><br><span class="line"><span class="string">    dtype 读取数据类型  一般机器学习类的都是float32 因为显卡一般他内核里面是按32位工作的</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">xy = np.loadtxt(<span class="string">&#x27;./dataset/diabetes.csv&#x27;</span>,delimiter=<span class="string">&quot;,&quot;</span>,dtype=np.float32)</span><br><span class="line"><span class="comment"># (759,9) 759行 9列</span></span><br><span class="line"><span class="built_in">print</span>(xy.shape)</span><br><span class="line"><span class="comment"># numpy 转 张量  最后一组样本我们当测试样本吧  其余的当训练样本</span></span><br><span class="line">x_train = torch.from_numpy(xy[:-<span class="number">1</span>,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line">y_train = torch.from_numpy(xy[:-<span class="number">1</span>,[-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当测试样本 的 一组数据</span></span><br><span class="line">x_test = torch.from_numpy(xy[-<span class="number">1</span>:,:-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_test.shape)</span><br><span class="line">y_test = torch.from_numpy(xy[-<span class="number">1</span>:,[-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二部 定义模型</span></span><br><span class="line"><span class="comment"># 这个其实有点类似与 神经网络的结构了 每个线性层后面都加了一个sigmoid函数 做了次非线性变换</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullLeanerModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 定义多层线性模型的结构</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FullLeanerModel,self).__init__()</span><br><span class="line">        <span class="comment"># 第一个线性转换 将8维转换为6维</span></span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>,<span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>,<span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = torch.nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前馈 运行</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="comment"># 用一个变量比较 简单</span></span><br><span class="line">        x = self.sigmoid(self.linear1(x))</span><br><span class="line">        x = self.sigmoid(self.linear2(x))</span><br><span class="line">        x = self.sigmoid(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span>  x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">FullLeanerModel = FullLeanerModel()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 定义 损失函数和优化器</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(FullLeanerModel.parameters(),lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>):</span><br><span class="line">    y_pred = FullLeanerModel(x_train)</span><br><span class="line">    loss = criterion(y_pred,y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss.item())</span><br><span class="line">![在这里插入图片描述](https://img-blog.csdnimg.cn/f600a0b6fbe24b60b35a0f2bc26f9444.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16<span class="comment">#pic_center)</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五步 评估模型</span></span><br><span class="line">    <span class="comment"># 输出各层  权重w 和 偏置b</span></span><br><span class="line"><span class="keyword">for</span> weight, bias <span class="keyword">in</span> FullLeanerModel.state_dict().items():  <span class="comment"># param is weight or bias(Tensor)</span></span><br><span class="line">    <span class="built_in">print</span>( weight,bias)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_yuce = FullLeanerModel(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试样本的预测值为&quot;</span>,y_yuce.data,<span class="string">&quot;实际样本的标签值为&quot;</span>,y_test.data)</span><br></pre></td></tr></table></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="https://img-blog.csdnimg.cn/d4fac45272664d249443217900408b79.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="实验结果"><br>可以看到 最后一个样本模型预测概率是 0.6529 。&gt;0.5 我们可以推测其标签就是1，而实际标签也是1，所以这个模型预测结果目前看是正确的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 实现 多维特征的输入——糖尿病预测&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;整体的设计思路&lt;br&gt;大致的设计步骤 分为5步 如下所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/66ed2d3a9e914f</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="多维度特征" scheme="http://example.com/tags/%E5%A4%9A%E7%BB%B4%E5%BA%A6%E7%89%B9%E5%BE%81/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 实现 简易逻辑回归模型 —— 刘二</title>
    <link href="http://example.com/2021/09/12/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%AE%80%E6%98%93%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/"/>
    <id>http://example.com/2021/09/12/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%AE%80%E6%98%93%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/</id>
    <published>2021-09-12T03:30:01.000Z</published>
    <updated>2021-09-13T13:25:10.303Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 实现 简易逻辑回归模型 —— 刘二">TOC</a></p><p>用Pytorch 实现 简单的逻辑回归。整个流程图可以如下图所示：<br><img src="https://img-blog.csdnimg.cn/ed9e3efcda894426bc9e66c6c0c5d408.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_9,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="逻辑回归预测流程"></p><p>大致的设计步骤 分为5步 如下所示：<br><img src="https://img-blog.csdnimg.cn/66ed2d3a9e914f229e8c67a8248b1dee.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_9,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>第五步：是进行 评估模型并预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><h1 id="Step1：准备数据"><a href="#Step1：准备数据" class="headerlink" title="Step1：准备数据"></a>Step1：准备数据</h1><p><img src="https://img-blog.csdnimg.cn/91dc702ff784417993b5d0e5f63f75ed.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><h1 id="Step2：设计模型"><a href="#Step2：设计模型" class="headerlink" title="Step2：设计模型"></a>Step2：设计模型</h1><p>内涵的线性模型比较简单 为 $y=Wx+b$ 只有两个超参数 W 和 b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二步 设计模型</span></span><br><span class="line"><span class="comment"># 构建一个线性模型类 所有的模型类都必须继承torch.nn.Module</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 调用父类的构造 这步必须得有</span></span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        <span class="comment"># Linear 是一个模型类 这边实例化他给 linear</span></span><br><span class="line">        <span class="comment"># w 权重 = 1  b 偏置 = 1</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 线性模型之后 在外套sigmoid激活函数</span></span><br><span class="line">        y_pred = torch.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure><h2 id="torch-nn-Linear-in-features-out-features-bias-True-方法"><a href="#torch-nn-Linear-in-features-out-features-bias-True-方法" class="headerlink" title="torch.nn.Linear(in_features,out_features,bias=True) 方法"></a>torch.nn.Linear(in_features,out_features,bias=True) 方法</h2><p><img src="https://img-blog.csdnimg.cn/0e75527101244e56a7687acb4eaa9315.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><p>实例化这个模型为 model</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化这个模型</span></span><br><span class="line">model = LogisticRegressionModel()</span><br></pre></td></tr></table></figure><h1 id="Step3：构建损失函数和优化器"><a href="#Step3：构建损失函数和优化器" class="headerlink" title="Step3：构建损失函数和优化器"></a>Step3：构建损失函数和优化器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三步 构建损失函数和优化器</span></span><br><span class="line"><span class="comment"># BCELoss  Binary Cross Entropy</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>) <span class="comment"># size_average = True 的话 就 乘以 1/N  默认为true</span></span><br><span class="line"><span class="comment">#model.parameters() 可以找到模型所有需要训练的参数</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>) </span><br></pre></td></tr></table></figure><h2 id="torch-nn-BCELoss-size-average-False-方法"><a href="#torch-nn-BCELoss-size-average-False-方法" class="headerlink" title="torch.nn.BCELoss(size_average=False) 方法"></a>torch.nn.BCELoss(size_average=False) 方法</h2><p>用于创建一个 BCE 损失函数<br><img src="https://img-blog.csdnimg.cn/c901445a0aad460d8c7c50e0a9287cd8.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h2 id="torch-optim-SGD-…-方法"><a href="#torch-optim-SGD-…-方法" class="headerlink" title="torch.optim.SGD(…) 方法"></a>torch.optim.SGD(…) 方法</h2><p>优化器选择 SGD   可调整学习率<br>$w^{*} = w - α\frac{\partial L}{\partial W}$<br><img src="https://img-blog.csdnimg.cn/aab4fe33c3844ea386a7c14cc9350865.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="Step4-训练模型"><a href="#Step4-训练模型" class="headerlink" title="Step4: 训练模型"></a>Step4: 训练模型</h1><p> 前两步 就是正向传播 forward</p><ol><li>预测 标签</li><li>预测 与 实际 算出损失值</li><li>反向传播 backward 优化参数</li><li>更新参数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss.item() )</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br></pre></td></tr></table></figure><h1 id="Step5-评估模型并预测"><a href="#Step5-评估模型并预测" class="headerlink" title="Step5: 评估模型并预测"></a>Step5: 评估模型并预测</h1><p>这边没有准备 测试集及其标签</p><p>输出 超参数  权重w 和 偏置b<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出 超参数  权重w 和 偏置b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>,model.linear.weight.item() )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>,model.linear.bias.item())</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测 </span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x为4.0 预测的 y值为：&#x27;</span>,y_test.data)</span><br></pre></td></tr></table></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><font color="red"> 这边少了 怎么输出 准确率？ </font><br><img src="https://img-blog.csdnimg.cn/e9bb2b08573e4016bb049d6289513ec9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="可以画一下图"><a href="#可以画一下图" class="headerlink" title="可以画一下图"></a>可以画一下图</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">200</span>)</span><br><span class="line">x_t = torch.Tensor(x).view((<span class="number">200</span>, <span class="number">1</span>))</span><br><span class="line">y_t = model(x_t)</span><br><span class="line">y = y_t.data.numpy()</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">10</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>], c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability of Pass&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="实现源码"><a href="#实现源码" class="headerlink" title="实现源码"></a>实现源码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 第一步 载入数据</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步 设计模型</span></span><br><span class="line"><span class="comment"># 构建一个线性模型类 所有的模型类都必须继承torch.nn.Module</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 调用父类的构造 这步必须得有</span></span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, self).__init__()</span><br><span class="line">        <span class="comment"># Linear 是一个模型类 这边实例化他给 linear</span></span><br><span class="line">        <span class="comment"># w 权重 = 1  b 偏置 = 1</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 线性模型之后 在外套sigmoid激活函数</span></span><br><span class="line">        y_pred = torch.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个模型</span></span><br><span class="line">model = LogisticRegressionModel()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 构建损失函数和优化器</span></span><br><span class="line"><span class="comment"># BCELoss  Binary Cross Entropy</span></span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>) <span class="comment"># size_average = True 的话 就 乘以 1/N  默认为true</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>) <span class="comment">#model.parameters() 可以找到</span></span><br><span class="line"><span class="comment"># 模型所有需要训练的参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss.item() )</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五步 评估模型</span></span><br><span class="line"><span class="comment"># 输出 超参数  权重w 和 偏置b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>,model.linear.weight.item() )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>,model.linear.bias.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x为4.0 预测的 y值为：&#x27;</span>,y_test.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">200</span>)</span><br><span class="line">x_t = torch.Tensor(x).view((<span class="number">200</span>, <span class="number">1</span>))</span><br><span class="line">y_t = model(x_t)</span><br><span class="line">y = y_t.data.numpy()</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">10</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>], c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability of Pass&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 实现 简易逻辑回归模型 —— 刘二&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;用Pytorch 实现 简单的逻辑回归。整个流程图可以如下图所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/ed9e3efcda</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
    <category term="逻辑回归" scheme="http://example.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>神经网络与深度学习 邱锡鹏 学习笔记（机器学习）</title>
    <link href="http://example.com/2021/09/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E9%82%B1%E9%94%A1%E9%B9%8F%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%89/"/>
    <id>http://example.com/2021/09/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E9%82%B1%E9%94%A1%E9%B9%8F%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%89/</id>
    <published>2021-09-11T07:46:01.000Z</published>
    <updated>2021-09-13T04:58:39.371Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="神经网络与深度学习 邱锡鹏 学习笔记（机器学习）">TOC</a></p><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><p>机器学习看作一个从有限、高维、有噪声的数据上得到更一般性规律的泛化问题．<br><img src="https://img-blog.csdnimg.cn/b5f2ae1f8a044527984d6b7a4e1d2190.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="机器学习过程"></p><h2 id="机器学习的三个基本要素"><a href="#机器学习的三个基本要素" class="headerlink" title="机器学习的三个基本要素"></a>机器学习的三个基本要素</h2><p><img src="https://img-blog.csdnimg.cn/0e48590c034949c6917b8eeeb555ad30.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="机器学习三要素"></p><ul><li>模型</li><li>学习准则</li><li>优化算法</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>我们不知道 输入和输出是如何对应联系的，也就是数学函数究竟是什么。 我们假设一个可能的函数集合 $ℱ$ ，称为 假设空间 。 我们根据 观测 各函数在测试集 $𝒟$ 的表现，从中选择更理想的假设模型。<br>常用的假设空间分为 <strong>线性和非线性</strong> 两种</p><h2 id="学习准则"><a href="#学习准则" class="headerlink" title="学习准则"></a>学习准则</h2><p>令训练集 是由 $𝑁$ 个独立同分布的样本组成，即每个样本 (𝒙, 𝑦) ∈ 𝒳 × 𝒴 是从 𝒳  和 𝒴  的联合空间中按照某个未知分布 $𝑝_{r}(𝒙, 𝑦)$ 独立地随机产生的 。</p><p>学习准则包括：</p><ul><li>经验风险最小化</li><li>结构风险最小化</li><li>最大似然估计</li><li>最大后验估计</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>用来量化模型预测和真实标签之间的差异</p><h4 id="0-1损失函数"><a href="#0-1损失函数" class="headerlink" title="0-1损失函数"></a>0-1损失函数</h4><h4 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h4><p>经常用在预测标签 $𝑦$ 为实数值的任务中</p><script type="math/tex; mode=display">L(y,f(x;θ)) = \frac{1}{2}(y-f(x;θ))</script><p><strong>平方损失函数一般不适用于分类问题</strong> </p><h4 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h4><p>一般用于分类问题</p><script type="math/tex; mode=display">L(y,f(x;θ)) =-\sum_{c=1}^{C}y_{c}\log f_{c}(x;θ)</script><p>例如，对于一个三分类的问题，一个样本的标签向量为 $𝒚 = [0, 0, 1]^{T}$，模型预测的标签分布为 $f(x;θ) = [0.3, 0.3, 0.4]^{T}$。<br>则它们的交叉熵为 $−(0 × log(0.3) + 0 ×log(0.3) + 1 × log(0.4)) = − log(0.4)$</p><h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><blockquote><p><strong>过拟合</strong><br>模型在训练集上错误率 很低，但是在未知数据上错误率很高，这就是所谓的过拟合。<br>过拟合的 3种 方法： 参数在过拟合之前就停止更新；正则化Regularization；<br>dropout</p><p><strong>欠拟合</strong><br>模型不能很好地拟合训练数据，在训练集上的错误率比较高．欠拟合一般是由于模型能力不足造成的。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/0bb493818f134cfe924b74c3f5e9a772.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><h4 id="参数与超参数"><a href="#参数与超参数" class="headerlink" title="参数与超参数"></a>参数与超参数</h4><ul><li>参数：模型 𝑓(𝒙; 𝜃)中的 𝜃 称为模型的参数，可以通过优化算法进行学习</li><li>超参数：用来定义模型结构或优化策略的。 <font color="blue">常见的超参数有：聚类算法中的类别个数、梯度下降法中的步长、正则化项的系数、神经网络的层数、支持向量机中的核函数等 </font></li></ul><p>参数的优化一般都是由优化器优化，而超参数优化是机器学习的 一个经验性很强的技术 ，通常是按照人的经验设定，或者通过搜索的方法对一组超参数组合进行不断试错调整。</p><h4 id="批量梯度下降法"><a href="#批量梯度下降法" class="headerlink" title="批量梯度下降法"></a>批量梯度下降法</h4><p>缺点在于 局部最优问题<br>用下面的迭代公式来计算计算训练集 𝒟 上风险函数的最小值：<br><img src="https://img-blog.csdnimg.cn/4703fad601d345569a9950cc173f436d.png#pic_center" alt="批量梯度下降法"><br>𝛼 一般称为学习率（Learning Rate）．</p><blockquote><p>批量梯度下降法在  每次迭代时<strong>需要计算每个样本（也就是所有样本）</strong>上损失函数的梯度并求和。当训练集中的样本数量 𝑁 很大时，空间复杂度比较高，每次迭代的计算开销也很大．</p></blockquote><h4 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h4><p>缺点在于 局部最优问题，但相比于批量，更容易脱离局部最优</p><blockquote><p>为了减少每次迭代的计算复杂度，我们也可以在每次迭代时<strong>只采集一个样本</strong>，计算这个样本损失函数的梯度并更新参数，即随机梯度下降法。当经过足够次数的迭代时，随机梯度下降 也可以收敛到局部最优解。<br><img src="https://img-blog.csdnimg.cn/4770f90631bb4ef3aa53847f7242a206.png#pic_center" alt="随机梯度下降法"></p><h4 id="小批量梯度下降法"><a href="#小批量梯度下降法" class="headerlink" title="小批量梯度下降法"></a>小批量梯度下降法</h4><p><font color="red">现在 大规模的机器学习 常用这个。</font><br><strong>利用了计算机的并行计算能力</strong><br>每次迭代时，我们随机选取一小部分训练样本来计算梯度并更新参数，这样既可以兼顾随机梯度下降法的优点，也可以提高训练效率。<br><img src="https://img-blog.csdnimg.cn/36fa218d071f4a399d141fa6641232ae.png#pic_center" alt="小批量梯度下降法"></p><h1 id="机器学习的简单示例——线性回归"><a href="#机器学习的简单示例——线性回归" class="headerlink" title="机器学习的简单示例——线性回归"></a>机器学习的简单示例——线性回归</h1><p>自变量数量 为1时称为 <strong>简单回归</strong>， 自变量数量大于1时称为 <strong>多元回归</strong>．</p><script type="math/tex; mode=display">𝑓(𝒙; 𝒘) = 𝒘^{T}𝒙</script></blockquote><h2 id="不明白-先跳过"><a href="#不明白-先跳过" class="headerlink" title="不明白 先跳过"></a>不明白 先跳过</h2><h1 id="偏差-方差分解"><a href="#偏差-方差分解" class="headerlink" title="偏差- 方差分解"></a>偏差- 方差分解</h1><p>其用在，对如何对模型的拟合能力和复杂度之间取得一个良好的平衡，偏差与方差起这很好的分析和指导作用。</p><p>对于单个样本 𝒙，不同训练集 𝒟 得到模型 $𝑓_{D}(𝒙)$ 和最优模型 $𝑓^{*}(𝒙)$ 的期望  差距为：<br><img src="https://img-blog.csdnimg.cn/0568101873af46f9ac794e591d7cfb19.png#pic_center" alt="在这里插入图片描述"></p><ul><li>偏差：指一个模型在 <font color="red">不同训练集</font> 上的<strong>平均性能和最优模型的差异</strong>，可以用来衡量一个模型的<strong>拟合能力</strong></li><li>方差：一个模型在不同训练集上的差异 ，可以用来衡量一个模型是否容易过拟合 ．</li></ul><p><img src="https://img-blog.csdnimg.cn/99942f3b15814cacbb5dcfb5a53cb056.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_9,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="机器学习模型的四种偏差和方差组合情况"><br>每个图的中心点为<br>最优模型 $𝑓^{*}(𝒙)$，黑点为不同训练集𝐷 上得到的模型 $𝑓_{D}(𝒙)$</p><ul><li>高偏差<strong>低方差</strong>的情况，表示模型的泛化能力很好，但<strong>拟合能力不足</strong></li><li>低偏差<strong>高方差</strong>的情况，表示模型的<strong>拟合能力很好</strong>，但泛化能力比较差．当训练数据比较少时会导致过拟合</li></ul><p>总结的来看：<br>模型在<strong>训练集</strong>上的错误率比较高时，说明模型的<strong>拟合能力不够</strong>，<strong>偏差比较高</strong>。这种情况可以通过</p><ul><li>增加数据特征</li><li>提高模型复杂度</li><li>减小正则化系数</li></ul><p>模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高．这种情况可以通过</p><ul><li>降低模型复杂度</li><li>加大正则化系数</li><li>引入先验</li><li>此外，还有一种有效降低方差的方法为<strong>集成模型</strong>，即通过多个高方差模型的平均来降低方差．</li></ul><h1 id="学习算法分类"><a href="#学习算法分类" class="headerlink" title="学习算法分类"></a>学习算法分类</h1><p>按照训练样本提供的信息以及反馈方式的不同，将机器学习算法分为以下几类：</p><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>机器学习的特征 𝒙 和标签 𝑦 之间可以用 数学模型表示出来，并且训练集中每个样本都有标签。（数据集标签一般都需要由人工进行标注，成本很高）<br>根据标签的类型还可以分为：</p><ul><li>回归。 标签 𝑦 与 模型的输出 都是连续值</li><li>分类。 标签 𝑦 是离散的类别（符号）。这种模型又叫做 分类器 。 分类问题可以分为 二分类 多分类。</li><li>结构化学习。 特殊的分类问题。标签 𝒚 通常是结构化的对象，比如序列、树或图等。 由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将 𝒙 ,  𝒚 映射为该空间中的联合特征向量 𝜙(𝒙, 𝒚)，预测模型可以写为 <img src="https://img-blog.csdnimg.cn/e6f1ad9f44424166b35a7bf4def1dbb4.png#pic_center" alt="在这里插入图片描述"><br>其中 Gen(𝒙) 表示输入 𝒙 的所有可能的输出目标集合．计算 arg max 的过程也称为<strong>解码</strong>（Decoding）过程，一般通过<strong>动态规划</strong>的方法来计算。</li></ul><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p> 是指从不包含目标标签的训练样本中自动学习到一些有价值的信息。典型的无监督学习问题有 </p><ul><li>聚类</li><li>密度估计</li><li>特征学习</li><li>降维</li></ul><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p> 是一类通过交互来学习的机器学习算法．在强化学习中，智能体根据环境的状态做出一个动作，并得到即时或延时的奖励．智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报．</p><h2 id="弱监督和半监督"><a href="#弱监督和半监督" class="headerlink" title="弱监督和半监督"></a>弱监督和半监督</h2><p>弱监督学习和半监督学习的方法，希望从大规模的无标注数据中充分挖掘有用的信息，降低对标注样本数量的要求。</p><p>强化学习和监督学习的不同在于，强化学习不需要显式地以“输入/输出对”的方式给出训练样本，是一种在线的学习机制。</p><h1 id="数据的特征表示"><a href="#数据的特征表示" class="headerlink" title="数据的特征表示"></a>数据的特征表示</h1><p><img src="https://img-blog.csdnimg.cn/deb2a7eee01647089fa86ebbd5f6acd8.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><p>需要将这些不同类型的数据转换为向量表示 。</p><p>如何选取有效的特征，具体可分为两种：特征选择和特征抽取。 <strong>传统的是和预测模型的学习分离的。</strong></p><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>特征选择就是保留有用特征，移除冗余或无关的特征。<br>方法包括 子集搜索和 L1正则化 </p><h3 id="子集搜索"><a href="#子集搜索" class="headerlink" title="子集搜索"></a>子集搜索</h3><ul><li>过滤式方法：不依赖具体机器学习模型的特征选择方法。每次增加最有信息量的特征，或删除最没有信息量的特征。</li><li>包裹式方法（Wrapper Method）是使用后续机器学习模型的准确率作为评价来选择一个特征子集的方法．每次增加对后续机器学习模型最有用的特征，或删除对后续机器学习任务最无用的特征。</li></ul><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>由于 L1 正则化会导致稀疏特征，因此间接实现了特征选择．</p><h2 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h2><p> 构造一个新的特征空间，并将原始特征投影在新的空间中得到新的表示。 其方法分为 <strong>有监督和无监督</strong>两类。</p><ul><li><strong>监督</strong>的特征学习的目标是抽取对一个特定的预测任务最有用的特征，比如线性判别分析。</li><li><strong>无监督</strong>的特征学习和具体任务无关，其目标通常是减少冗余信息和噪声，比如<strong>主成分分析PCA</strong>和<strong>自编码器 AE</strong>。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://img-blog.csdnimg.cn/502589206ad64f6898d51126bd9233ba.png#pic_center" alt="传统的特征选择和特征抽取方法"><br>特征选择和特征抽取的优点是可以用较少的特征来表示原始特征中的大部分相关信息，去掉噪声信息，并进而提高计算效率和减小维度灾难。<br>对于很多没有正则化的模型，特征选择和特征抽取非常必要。 经过特征选择或特征抽取后，特征的数量一般会减少，因此特征选择和特征抽取。 也经常称为<strong>维数约减或降维</strong>。</p><h1 id="模型的评价指标"><a href="#模型的评价指标" class="headerlink" title="模型的评价指标"></a>模型的评价指标</h1><ul><li>准确率（Accuracy）</li><li>错误率（Error Rate）： 1 - 准确率</li><li>查准率（Precision）</li><li>查全率（Recall）</li><li>F值（F Measure）</li></ul><h2 id="如何理解-精确率、召回率和F值"><a href="#如何理解-精确率、召回率和F值" class="headerlink" title="如何理解 精确率、召回率和F值"></a>如何理解 精确率、召回率和F值</h2><p>模型在测试集上的结果可以分为以下四种情况：<br>比如我们现在 预测标签 C</p><ul><li>真正例（TP）：样本的预测与实际标签相同</li><li>假负例（FN）：样本实际标签为C，模型预测错了</li><li>假正例（FP）：样本实际标签不是C，但模型预测成C了</li><li>真负例（TN）：样本实际为其他类，模型也预测为其他类</li></ul><p><img src="https://img-blog.csdnimg.cn/e157318baff446ce9215cf7854120df2.png#pic_center" alt="在这里插入图片描述"></p><h3 id="查准率"><a href="#查准率" class="headerlink" title="查准率"></a>查准率</h3><p>类别 𝑐 的查准率  是所有预测为 类别C 的样本中 预测正确  的比例<br>精确率 $P_{C}$ 的计算 公式为：</p><script type="math/tex; mode=display">P_{C} = \frac{TP_{c}}{TP_{c}+FP_{c}}</script><h3 id="查全率"><a href="#查全率" class="headerlink" title="查全率"></a>查全率</h3><p>类别𝑐的查全率 是所有真实标签为 类别𝑐 的样本中预测正确的比例：</p><script type="math/tex; mode=display">R_{C} = \frac{TP_{c}}{TP_{c}+FN_{c}}</script><h3 id="F值"><a href="#F值" class="headerlink" title="F值"></a>F值</h3><p>F值（F Measure）是一个综合指标，为精确率和召回率的调和平均：</p><script type="math/tex; mode=display">F_{C} = \frac{(1+β^{2})×P_{C}×R_{C}}{β^{2}×P_{C}+R_{C}}</script><p>其中 𝛽 用于平衡精确率和召回率的重要性，一般取值为1．𝛽 = 1时的F值称为 <strong>F1 值</strong>，是精确率和召回率的调和平均．</p><h2 id="宏平均和微平均"><a href="#宏平均和微平均" class="headerlink" title="宏平均和微平均"></a>宏平均和微平均</h2><p>为了计算分类算法在<strong>所有类别上的总体查准率、查全率和 F1值</strong>，经常使用两种平均方法，分别称为 宏平均 和 微平均</p><ul><li>宏平均是每一类的性能指标的算术平均值</li><li>微平均是每一个样本的性能指标的算术平均值</li></ul><h1 id="理论和定理"><a href="#理论和定理" class="headerlink" title="理论和定理"></a>理论和定理</h1><h2 id="PAC学习理论："><a href="#PAC学习理论：" class="headerlink" title="PAC学习理论："></a>PAC学习理论：</h2><p>指该学习算法能够在多项式时间内从合理数量的训练数据中学习到一个近似正确的𝑓(𝒙)．</p><h2 id="没有免费午餐定理"><a href="#没有免费午餐定理" class="headerlink" title="没有免费午餐定理"></a>没有免费午餐定理</h2><p>没有免费午餐定理   就是不存在一种机器学习算法适合于任何领域或任务。</p><h2 id="奥卡姆剃刀原理"><a href="#奥卡姆剃刀原理" class="headerlink" title="奥卡姆剃刀原理"></a>奥卡姆剃刀原理</h2><p>简单的模型泛化能力更好．如果有两个性能相近的模型，我们应该选择<strong>更简单的模型</strong>。<br>因此，在机器学习的学习准则上，我们经常会引入参数正则化来限制模型能力，避免过拟合．</p><h2 id="丑小鸭定理"><a href="#丑小鸭定理" class="headerlink" title="丑小鸭定理"></a>丑小鸭定理</h2><p>世界上不存在相似性的客观标准，一切相似性的标准都是主观的</p><h2 id="归纳偏置"><a href="#归纳偏置" class="headerlink" title="归纳偏置"></a>归纳偏置</h2><p>预测模型前，先假设。<br>比如在最近邻分类器中，我们会假设在特征空间中，一个小的局部区域中的大部分样本同属一类。<br>在朴素贝叶斯分类器中，我们会假设每个特征的条件概率是互相独立的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;神经网络与深度学习 邱锡鹏 学习笔记（机器学习）&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;机器学习&quot;&gt;&lt;a href=&quot;#机器学习&quot; class=&quot;headerlink&quot; title=&quot;机器学习&quot;&gt;&lt;/a&gt;机器学习&lt;/h1&gt;&lt;p&gt;机器学习看作一个从有限</summary>
      
    
    
    
    <category term="深度学习基础-邱锡鹏" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%82%B1%E9%94%A1%E9%B9%8F/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="邱锡鹏" scheme="http://example.com/tags/%E9%82%B1%E9%94%A1%E9%B9%8F/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络与深度学习 学习笔记（第一章）</title>
    <link href="http://example.com/2021/09/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%89/"/>
    <id>http://example.com/2021/09/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%89/</id>
    <published>2021-09-10T08:42:01.000Z</published>
    <updated>2021-09-13T04:58:33.835Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="神经网络与深度学习 邱锡鹏 学习笔记（第一章）">TOC</a></p><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="浅层学习"><a href="#浅层学习" class="headerlink" title="浅层学习"></a>浅层学习</h2><blockquote><p>学习一个预测模型．一般需要首先将数据表示为一组特征（Feature），特征的表示形式可以是连续的数值、离散的符号或其他形式．然后将这些特征输入到预测模型，并输出预测结果．这类机器学习可以看作浅层学习（Shallow  Learning）</p></blockquote><h2 id="机器学习的步骤"><a href="#机器学习的步骤" class="headerlink" title="机器学习的步骤"></a>机器学习的步骤</h2><ol><li>数据预处理：对数据的原始形式进行初步的数据清理（比如去掉一些有缺失特征的样本，或去掉一些冗余的数据特征等）和加工（对数值特征进行缩放和归一化等），并构建成可用于训练机器学习模型的数据集．</li><li>特征提取：从数据的原始特征中提取一些对特定机器学习任务有用的高质量特征．比如在图像分类中提取边缘、尺度不变特征变换（Scale Invariant<br>Feature Transform，SIFT）特征，在文本分类中去除停用词等．</li><li>特征转换：对特征进行进一步的加工，比如降维和升维． 很多特征转换方法也都是机器学习方法．降维包括特征抽取（Feature Extraction）和特征选择（FeatureSelection）两种途径．常用的特征转换方法有主成分分析（Principal Components Analysis，PCA）、 线性判别分析（Linear Discriminant Analysis，LDA）等．</li><li>预测：机器学习的核心部分，学习一个函数并进行预测．</li></ol><p><img src="https://img-blog.csdnimg.cn/f3d73fb28f1342f48a6ca08638b8da68.png#pic_center" alt="传统机器学习的数据处理流程"><br>注：很多的机器学习问题变成了特征工程（Feature Engineering）问题．开发一个机器学习系统的主要工作量都消耗在了预处理、特征提取以及特征转换上。</p><h1 id="表示学习"><a href="#表示学习" class="headerlink" title="表示学习"></a>表示学习</h1><p>在表示学习中，有两个核心问题：</p><ul><li>一是“什么是一个好的表示”；即表示 需要包含更高层的语义信息</li><li>二是“如何学习到好的表示”．</li></ul><p>传统的特征学习一般是通过<font color="red">人为地设计一些准则</font>，然后根据这些准则来选取有效的特征。 所以 特征的学习是和最终预测模型的学习分开进行的，<font color="red">因此学习到的特征不一定可以提升最终模型的性能．</font></p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>将输入信息转换为有效的特征。<br>如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作表示学习（Representation Learning）．</p><h2 id="语义鸿沟"><a href="#语义鸿沟" class="headerlink" title="语义鸿沟"></a>语义鸿沟</h2><p>语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性。</p><blockquote><p>比如车，图片中每辆车的颜色和形状等属性都不尽相同，因此不同图片在像素级别上的表示（即底层特征）差异性也会非常大．但是我们理解这些图片是建立在比较抽象的高层语义概念上的</p></blockquote><h2 id="表示特征的方式"><a href="#表示特征的方式" class="headerlink" title="表示特征的方式"></a>表示特征的方式</h2><ol><li>局部表示：例如，one-hot向量 表示颜色。 缺点在于多个颜色就多个列或者行 </li><li>分布式表示：RGB 表示颜色</li></ol><p><img src="https://img-blog.csdnimg.cn/aa86242673364da09f8245d248914cbd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="局部表示和分布式表示示例"></p><h2 id="嵌入"><a href="#嵌入" class="headerlink" title="嵌入"></a>嵌入</h2><p>嵌入通常指将一个度量空间中的一些对象映射到另一个低维的度量空间中，并尽可能保持不同对象之间的拓扑关系．<font color="blue">比如自然语言中词的分布式表示，也经常叫作词嵌入 </font></p><p><font color="red"><strong>例如：3维one-hot向量空间和一个2维嵌入空间的对比</strong></font><br><img src="https://img-blog.csdnimg.cn/007e99163fd044348e7061ebebc1af33.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_14,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="one-hot向量空间与嵌入空间"><br>在低维的嵌入空间中，每个样本都不在坐标轴上，样本之间可以计算相似度．</p><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="什么是深度"><a href="#什么是深度" class="headerlink" title="什么是深度"></a>什么是深度</h2><p>“深度”是指原始数据进行非线性特征转换的次数</p><h2 id="深度学习的优点"><a href="#深度学习的优点" class="headerlink" title="深度学习的优点"></a>深度学习的优点</h2><p>深度学习，其主要目的是从数据中<font color="red">自动学习到有效的特征表示 </font><br>其抽象在 数据通过多层的特征转换，学习到的表示可以代替人工设计的特征，从而避免“特征工程”<br><img src="https://img-blog.csdnimg.cn/f678fb27e01c4d989d099cfa06afe038.png#pic_center" alt="深度学习的数据处理流程"><br>其是 一种 <font color="red"><strong>端到端的学习方式</strong></font>   在学习过程中<strong>不进行分模块或分阶段训练</strong>，直接优化任务的总体目标．在端到端学习中，一般不需要明确地给出不同模块或阶段的功能，中间过程不需要人为干预</p><h2 id="深度学习的关键问题"><a href="#深度学习的关键问题" class="headerlink" title="深度学习的关键问题"></a>深度学习的关键问题</h2><p>深度学习需要解决的关键问题是 <strong>贡献度分配问题</strong>，即一个系统中不同的 <strong>组件</strong> 或其 <strong>参数</strong> 对最终系统输出结果的贡献或影响 </p><p>目前，深度学习采用的模型主要是神经网络模型，其主要原因是神经网络模型可以使用 <strong>误差反向传播算法</strong> ，从而可以比较好地解决贡献度分配问题</p><h1 id="深度学习相关的学术会议"><a href="#深度学习相关的学术会议" class="headerlink" title="深度学习相关的学术会议"></a>深度学习相关的学术会议</h1><ul><li>国际表示学习会议  ICLR ：主要聚焦于深度学习</li><li>神经信息处理系统年会 NeurIPS ：交叉学科会议，但偏重于机器学习</li><li>国际机器学习会议 ICML：机器学习顶级会议</li><li>国际人工智能联合会议 IJCAI ：人工智能领域最顶尖的综合性会议</li><li>美国人工智能协会年会  AAAI ：人工智能领域的顶级会议</li></ul><h2 id="计算机视觉领域"><a href="#计算机视觉领域" class="headerlink" title="计算机视觉领域"></a>计算机视觉领域</h2><ul><li>计算机视觉与模式识别大会  CVPR</li><li>国际计算机视觉会议  ICCV</li></ul><h2 id="自然语言处理领域"><a href="#自然语言处理领域" class="headerlink" title="自然语言处理领域"></a>自然语言处理领域</h2><ul><li>计算语言学年会 ACL</li><li>自然语言处理实证方法大会  EMNLP</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;神经网络与深度学习 邱锡鹏 学习笔记（第一章）&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;机器学习&quot;&gt;&lt;a href=&quot;#机器学习&quot; class=&quot;headerlink&quot; title=&quot;机器学习&quot;&gt;&lt;/a&gt;机器学习&lt;/h1&gt;&lt;h2 id=&quot;浅层学习&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="深度学习基础-邱锡鹏" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%82%B1%E9%94%A1%E9%B9%8F/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="邱锡鹏" scheme="http://example.com/tags/%E9%82%B1%E9%94%A1%E9%B9%8F/"/>
    
    <category term="神经网络" scheme="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 实现 线性回归模型 —— 刘二</title>
    <link href="http://example.com/2021/09/08/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/"/>
    <id>http://example.com/2021/09/08/Pytorch%20%E5%AE%9E%E7%8E%B0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%20%E2%80%94%E2%80%94%20%E5%88%98%E4%BA%8C/</id>
    <published>2021-09-08T02:40:01.000Z</published>
    <updated>2021-09-17T12:28:47.682Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Pytorch 实现 线性回归模型 —— 刘二">TOC</a></p><p>用Pytorch 实现 线性模型。整个流程图可以如下图所示：<br><img src="https://img-blog.csdnimg.cn/0728aba4a28542838fdb11f7bf4a668c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>大致的设计步骤 分为5步 如下所示：<br><img src="https://img-blog.csdnimg.cn/e1aa1c61b89b4e46adebe9d44c61ff2b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>第五步：是进行 评估模型并预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><h1 id="Step1：准备数据"><a href="#Step1：准备数据" class="headerlink" title="Step1：准备数据"></a>Step1：准备数据</h1><p><img src="https://img-blog.csdnimg.cn/91dc702ff784417993b5d0e5f63f75ed.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br></pre></td></tr></table></figure><h1 id="Step2：设计模型"><a href="#Step2：设计模型" class="headerlink" title="Step2：设计模型"></a>Step2：设计模型</h1><p>这边的线性模型比较简单 为 $y=Wx+b$ 只有两个超参数 W 和 b<br><img src="https://img-blog.csdnimg.cn/51e148a09e1d452e8a4e9794e3c4035a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>要通过输入的维度和 输出的维度，才能明确 $W$ 和 $b$ 的维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个线性模型类 所有的模型类都必须继承torch.nn.Module</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"><span class="comment"># 构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 调用父类的构造 这步必须得有</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel,self).__init__()</span><br><span class="line">        <span class="comment"># Linear 是一个模型类 这边实例化他给 对象linear</span></span><br><span class="line">        <span class="comment"># w 权重 = 1  b 偏执 = 1</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 方法重写</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">    <span class="comment"># 预测值 </span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure><h2 id="torch-nn-Linear-in-features-out-features-bias-True-方法"><a href="#torch-nn-Linear-in-features-out-features-bias-True-方法" class="headerlink" title="torch.nn.Linear(in_features,out_features,bias=True) 方法"></a>torch.nn.Linear(in_features,out_features,bias=True) 方法</h2><p><img src="https://img-blog.csdnimg.cn/0e75527101244e56a7687acb4eaa9315.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><p>实例化这个模型为 model</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化这个模型</span></span><br><span class="line">model = LinearModel()</span><br></pre></td></tr></table></figure><h1 id="Step3：构建损失函数和优化器"><a href="#Step3：构建损失函数和优化器" class="headerlink" title="Step3：构建损失函数和优化器"></a>Step3：构建损失函数和优化器</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数：mean squared error</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>) <span class="comment"># size_average = True 的话 就 乘以 1/N  默认为true</span></span><br><span class="line"><span class="comment"># model.parameters() 可以找到模型所有需要训练的参数   优化器：SGD</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>) </span><br></pre></td></tr></table></figure><h2 id="torch-nn-MSELoss-size-average-False-方法"><a href="#torch-nn-MSELoss-size-average-False-方法" class="headerlink" title="torch.nn.MSELoss(size_average=False) 方法"></a>torch.nn.MSELoss(size_average=False) 方法</h2><p>用于创建一个MSE损失函数<br><img src="https://img-blog.csdnimg.cn/4921387ab8ea4db6a6aaca5ed892fef5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h2 id="torch-optim-SGD-…-方法"><a href="#torch-optim-SGD-…-方法" class="headerlink" title="torch.optim.SGD(…) 方法"></a>torch.optim.SGD(…) 方法</h2><p>优化器选择 SGD   可调整学习率<br>$w^{*} = w - α\frac{\partial L}{\partial W}$<br><img src="https://img-blog.csdnimg.cn/aab4fe33c3844ea386a7c14cc9350865.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="Step4-训练模型"><a href="#Step4-训练模型" class="headerlink" title="Step4: 训练模型"></a>Step4: 训练模型</h1><p> 前两步 就是正向传播 forward</p><ol><li>预测 标签</li><li>预测 与 实际 算出损失值</li><li>反向传播 backward 优化参数</li><li>更新参数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"> <span class="comment">#数据跑 99次  range是不到那个数字的</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br></pre></td></tr></table></figure><h1 id="Step5-评估模型并预测"><a href="#Step5-评估模型并预测" class="headerlink" title="Step5: 评估模型并预测"></a>Step5: 评估模型并预测</h1><p>这边没有准备 测试集及其标签</p><p>输出 超参数  权重w 和 偏置b<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出 超参数  权重w 和 偏置b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>,model.linear.weight.item() )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>,model.linear.bias.item())</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测 </span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x为4.0 预测的 y值为：&#x27;</span>,y_test.data)</span><br></pre></td></tr></table></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><font color="red"> 这边少了 怎么输出 准确率？ </font><br><img src="https://img-blog.csdnimg.cn/c0013d6d060148b8ac02605d0fa0d163.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_15,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="实现源码"><a href="#实现源码" class="headerlink" title="实现源码"></a>实现源码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步 设计模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 调用父类的构造 这步必须得有</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel,self).__init__()</span><br><span class="line">        <span class="comment"># Linear 是一个模型类 这边实例化他给 linear</span></span><br><span class="line">        <span class="comment"># w 权重 = 1  b 偏执 = 1</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化这个模型</span></span><br><span class="line">model = LinearModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步 构建损失函数和优化器</span></span><br><span class="line"><span class="comment"># mean squared error</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>) <span class="comment"># size_average = True 的话 就 乘以 1/N  默认为true</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>) <span class="comment">#model.parameters() 可以找到</span></span><br><span class="line"><span class="comment"># 模型所有需要训练的参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步 训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 前两步 就是正向传播 forward</span></span><br><span class="line"><span class="string">    1. 预测 标签</span></span><br><span class="line"><span class="string">    2. 预测 与 实际 算出损失值</span></span><br><span class="line"><span class="string">    3. 反向传播 backward 优化参数</span></span><br><span class="line"><span class="string">    4. 更新参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;迭代次数:&#x27;</span>,epoch,<span class="string">&quot;  损失值:&quot;</span>,loss)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()   <span class="comment">#更新参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五步 评估模型</span></span><br><span class="line"><span class="comment"># 输出 超参数  权重w 和 偏置b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>,model.linear.weight.item() )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>,model.linear.bias.item())</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x为4.0 预测的 y值为：&#x27;</span>,y_test.data)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Pytorch 实现 线性回归模型 —— 刘二&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;用Pytorch 实现 线性模型。整个流程图可以如下图所示：&lt;br&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/0728aba4a285428</summary>
      
    
    
    
    <category term="Pytorch" scheme="http://example.com/categories/Pytorch/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="刘二" scheme="http://example.com/tags/%E5%88%98%E4%BA%8C/"/>
    
    <category term="Pytorch" scheme="http://example.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Keras  RNN 实现 MNIST 手写数字识别</title>
    <link href="http://example.com/2021/09/04/Keras%20%20RNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    <id>http://example.com/2021/09/04/Keras%20%20RNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/</id>
    <published>2021-09-04T02:40:01.000Z</published>
    <updated>2021-09-17T12:25:56.550Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras  RNN 实现 MNIST 手写数字识别">TOC</a></p><p>我们就以 MNIST数据集的手写识别 为例子</p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers.recurrent <span class="keyword">import</span> SimpleRNN</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure><h1 id="载入MNIST-数据"><a href="#载入MNIST-数据" class="headerlink" title="载入MNIST 数据"></a>载入MNIST 数据</h1><p>该数据集一共有训练集 6w 张，测试集 1w 张<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_images,train_labels),(test_images,test_labels)= mnist.load_data()</span><br></pre></td></tr></table></figure></p><p>可以查看一下图像和标签  是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数字5</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签5</span></span><br><span class="line"><span class="built_in">print</span>(train_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 数字7</span></span><br><span class="line"><span class="built_in">print</span>(test_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签7</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以 打印 一下图片 看看是什么样子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(test_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/a745fc12bef94f93b93f80b2e89bb0d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="图片数据处理"><a href="#图片数据处理" class="headerlink" title="图片数据处理"></a>图片数据处理</h1><h2 id="查看图片原有-shape"><a href="#查看图片原有-shape" class="headerlink" title="查看图片原有 shape"></a>查看图片原有 shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_images (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images&#x27;</span>, train_images.shape)</span><br><span class="line"><span class="comment"># train_labels (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><h2 id="图片数据处理：归一化"><a href="#图片数据处理：归一化" class="headerlink" title="图片数据处理：归一化"></a>图片数据处理：归一化</h2><p>除以255.0 是为了归一化，使得元素点 全部变为在 0-1 之间<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment">#  (60000, 28, 28) </span></span><br><span class="line">train_images_scale = train_images/<span class="number">255.0</span></span><br><span class="line">test_images_scale = test_images/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_images变换后 (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images变换后&#x27;</span>, train_images_scale.shape)</span><br><span class="line"><span class="comment"># test_images变换后 (10000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_images变换后&#x27;</span>, test_images_scale.shape)</span><br></pre></td></tr></table></figure></p><h2 id="标签数据处理：转换成-one-hot-格式"><a href="#标签数据处理：转换成-one-hot-格式" class="headerlink" title="标签数据处理：转换成 one hot 格式"></a>标签数据处理：转换成 one hot 格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换 one hot 格式  共十个分类</span></span><br><span class="line"><span class="comment"># np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</span></span><br><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br><span class="line">test_labels_hot = np_utils.to_categorical(test_labels,num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_labels (60000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>,train_labels_hot.shape)</span><br><span class="line"><span class="comment"># test_labels (10000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_labels&#x27;</span>,test_labels_hot.shape)</span><br></pre></td></tr></table></figure><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><p>先定义 RNN 所需的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义rnn 的参数</span></span><br><span class="line"><span class="comment"># 数据长度 一行一共有28个元素</span></span><br><span class="line">input_size = <span class="number">28</span></span><br><span class="line"><span class="comment"># 序列长度 一共有28个序列 也就是28行</span></span><br><span class="line">time_steps = <span class="number">28</span></span><br><span class="line"><span class="comment"># 隐藏层cell个数</span></span><br><span class="line">cell_size = <span class="number">50</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>再定义 RNN 模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment">#循环神经网络</span></span><br><span class="line">model.add(SimpleRNN(</span><br><span class="line">    units=cell_size, <span class="comment">#输出</span></span><br><span class="line">    input_shape=(time_steps,input_size) <span class="comment">#输入</span></span><br><span class="line">))</span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line">model.add(Dense(input_dim=cell_size,units=<span class="number">10</span>,activation=<span class="string">&quot;softmax&quot;</span>))</span><br></pre></td></tr></table></figure><br>优化器使用 Adam， 损失函数 选择 交叉熵   并编译</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化器  10的 -4次方</span></span><br><span class="line">adam = Adam(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= adam,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p> 一共 60000张训练图片 按批次训练 一批次64张   一共6w/64 个批次<br>训练完一轮6w张，表示一个epoch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一共 60000张训练图片 按批次训练 一批次64张   </span></span><br><span class="line"><span class="comment"># 训练完一轮6w张，表示一个epoch</span></span><br><span class="line">model.fit(train_images_scale,train_labels_hot,batch_size=<span class="number">64</span>,epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/35cafae3dc0e431086f33cdf4a23dc52.png#pic_center" alt="在这里插入图片描述"></p><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br>也可以看一下 在训练集上的表现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(train_images_scale,train_labels_hot)</span><br></pre></td></tr></table></figure><p>下面是 结果（上训练集 下测试集）<br><img src="https://img-blog.csdnimg.cn/d93ee2873e72460e96c0c4a9a663985b.png#pic_center" alt="在这里插入图片描述"></p><h1 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h1><font color="red">**有问题 输入的 数据维度不对 不知道错哪了？？？**</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看下输入的形状</span></span><br><span class="line"><span class="comment"># # 预测数据</span></span><br><span class="line"><span class="built_in">print</span>(test_images_scale[<span class="number">0</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images_scale[<span class="number">0</span>]/<span class="number">255.0</span>)))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images_scale[<span class="number">0</span>]/<span class="number">255.0</span>))))</span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输入的图像 变换后形状<br><img src="https://img-blog.csdnimg.cn/c6e4d5008f9c416ba48bea577a86c238.png#pic_center" alt="在这里插入图片描述"><br>模型预测 数据 输出的概率分类结果<br><img src="https://img-blog.csdnimg.cn/ba872c35347041b592db07d82b429d35.png#pic_center" alt="在这里插入图片描述"><br>选出其中最大的 以及 实际图片标签 均为 数字7<br><img src="https://img-blog.csdnimg.cn/5a585701d439431aa1853283c8592b46.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras  RNN 实现 MNIST 手写数字识别&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们就以 MNIST数据集的手写识别 为例子&lt;/p&gt;
&lt;h1 id=&quot;导入需要的包&quot;&gt;&lt;a href=&quot;#导入需要的包&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="Keras" scheme="http://example.com/categories/Keras/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
    <category term="RNN" scheme="http://example.com/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>Keras  CNN 实现 MNIST 手写数字识别</title>
    <link href="http://example.com/2021/09/03/Keras%20%20CNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    <id>http://example.com/2021/09/03/Keras%20%20CNN%20%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/</id>
    <published>2021-09-03T02:40:01.000Z</published>
    <updated>2021-09-17T12:26:02.683Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras  CNN 实现 MNIST 手写数字识别">TOC</a></p><p>我们就以 MNIST数据集的手写识别 为例子</p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Dropout,Convolution2D,MaxPooling2D,Flatten</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure><h1 id="载入MNIST-数据"><a href="#载入MNIST-数据" class="headerlink" title="载入MNIST 数据"></a>载入MNIST 数据</h1><p>该数据集一共有训练集 6w 张，测试集 1w 张<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_images,train_labels),(test_images,test_labels)= mnist.load_data()</span><br></pre></td></tr></table></figure></p><p>可以查看一下图像和标签  是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数字5</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签5</span></span><br><span class="line"><span class="built_in">print</span>(train_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 数字7</span></span><br><span class="line"><span class="built_in">print</span>(test_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签7</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以 打印 一下图片 看看是什么样子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(test_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/a745fc12bef94f93b93f80b2e89bb0d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="图片数据处理"><a href="#图片数据处理" class="headerlink" title="图片数据处理"></a>图片数据处理</h1><h2 id="查看图片原有-shape"><a href="#查看图片原有-shape" class="headerlink" title="查看图片原有 shape"></a>查看图片原有 shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_images (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images&#x27;</span>, train_images.shape)</span><br><span class="line"><span class="comment"># train_labels (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><h2 id="图片数据处理：变成四维-，并归一化"><a href="#图片数据处理：变成四维-，并归一化" class="headerlink" title="图片数据处理：变成四维 ，并归一化"></a>图片数据处理：变成四维 ，并归一化</h2><p>变维度的 -1 是个通配符，系统会自动完成应该变成多少<br>除以255.0 是为了归一化，使得元素点 全部变为在 0-1 之间<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment"># CNN 输入的是一张图片</span></span><br><span class="line"><span class="comment"># 将 (60000, 28, 28) -&gt; (60000, 28, 28, 1)  变成四维</span></span><br><span class="line"><span class="comment"># 第四个 1  表示的为深度   黑白图像为 1   彩色图像为 3</span></span><br><span class="line">train_images_scale = train_images.reshape(-<span class="number">1</span>, train_images.shape[<span class="number">1</span>] ,train_images.shape[<span class="number">2</span>],<span class="number">1</span>)/<span class="number">255.0</span></span><br><span class="line">test_images_scale = test_images.reshape(-<span class="number">1</span>,test_images.shape[<span class="number">1</span>], test_images.shape[<span class="number">2</span>],<span class="number">1</span>)/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_images变换后 (60000, 28, 28, 1)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images变换后&#x27;</span>, train_images_scale.shape)</span><br><span class="line"><span class="comment"># test_images变换后 (10000, 28, 28, 1)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_images变换后&#x27;</span>, test_images_scale.shape)</span><br></pre></td></tr></table></figure></p><h2 id="标签数据处理：转换成-one-hot-格式"><a href="#标签数据处理：转换成-one-hot-格式" class="headerlink" title="标签数据处理：转换成 one hot 格式"></a>标签数据处理：转换成 one hot 格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换 one hot 格式  共十个分类</span></span><br><span class="line"><span class="comment"># np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</span></span><br><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br><span class="line">test_labels_hot = np_utils.to_categorical(test_labels,num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_labels (60000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>,train_labels_hot.shape)</span><br><span class="line"><span class="comment"># test_labels (10000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_labels&#x27;</span>,test_labels_hot.shape)</span><br></pre></td></tr></table></figure><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><p>思路是 卷积 再池化 重复步骤  再压平给 全身神经网络<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment">#第一个卷积层</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    input shape 激入平面</span></span><br><span class="line"><span class="string">    filters 卷积核/过滤器 个数</span></span><br><span class="line"><span class="string">    kernel_size 卷积窗口大小</span></span><br><span class="line"><span class="string">    strides 步长</span></span><br><span class="line"><span class="string">    padding（边界填充）padding方式 same/valid</span></span><br><span class="line"><span class="string">    activation 激活函数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 用same 保持整个还是 28 × 28</span></span><br><span class="line">model.add(Convolution2D(</span><br><span class="line">    input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>),filters=<span class="number">32</span>,kernel_size=<span class="number">5</span>,strides=<span class="number">1</span>,padding=<span class="string">&#x27;same&#x27;</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment">#第一个池化层</span></span><br><span class="line">model.add(MaxPooling2D(</span><br><span class="line">    pool_size=<span class="number">2</span>,strides=<span class="number">2</span>,padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line"><span class="comment">#第二卷积层</span></span><br><span class="line">model.add(Convolution2D(<span class="number">64</span>,<span class="number">5</span>,strides=<span class="number">1</span>,padding=<span class="string">&#x27;same&#x27;</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment">#第二个池化层</span></span><br><span class="line">model.add(MaxPooling2D(</span><br><span class="line">    pool_size=<span class="number">2</span>,strides=<span class="number">2</span>,padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#把第二个池化层的前出扁平化为1维</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line"><span class="comment">#第一个全连接层</span></span><br><span class="line">model.add(Dense(units=<span class="number">1024</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"><span class="comment">#Dropout</span></span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line"><span class="comment">#第二个全连接层</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">1024</span>,activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure></p><p>优化器使用 Adam， 损失函数 选择 交叉熵   并编译</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义优化器  10的 -4次方</span></span><br><span class="line">adam = Adam(learning_rate=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= adam,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p> 一共 60000张训练图片 按批次训练 一批次64张   一共6w/64 个批次<br>训练完一轮6w张，表示一个epoch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一共 60000张训练图片 按批次训练 一批次64张   </span></span><br><span class="line"><span class="comment"># 训练完一轮6w张，表示一个epoch</span></span><br><span class="line">model.fit(train_images_scale,train_labels_hot,batch_size=<span class="number">64</span>,epochs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/35cafae3dc0e431086f33cdf4a23dc52.png#pic_center" alt="在这里插入图片描述"><br>上图只跑了一次 epoch，因为笔记本太慢了<br><strong>注意： 最好用 GPU 来跑 ，不然笔记本非常的慢</strong></p><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br>也可以看一下 在训练集上的表现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(train_images_scale,train_labels_hot)</span><br></pre></td></tr></table></figure><p>下面是 结果（上训练集 下测试集）<br><img src="https://img-blog.csdnimg.cn/d93ee2873e72460e96c0c4a9a663985b.png#pic_center" alt="在这里插入图片描述"></p><h1 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看下输入的形状</span></span><br><span class="line"><span class="built_in">print</span>(test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>))))</span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))))</span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输入的图像 变换后形状<br><img src="https://img-blog.csdnimg.cn/c6e4d5008f9c416ba48bea577a86c238.png#pic_center" alt="在这里插入图片描述"><br>模型预测 数据 输出的概率分类结果<br><img src="https://img-blog.csdnimg.cn/ba872c35347041b592db07d82b429d35.png#pic_center" alt="在这里插入图片描述"><br>选出其中最大的 以及 实际图片标签 均为 数字7<br><img src="https://img-blog.csdnimg.cn/5a585701d439431aa1853283c8592b46.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras  CNN 实现 MNIST 手写数字识别&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们就以 MNIST数据集的手写识别 为例子&lt;/p&gt;
&lt;h1 id=&quot;导入需要的包&quot;&gt;&lt;a href=&quot;#导入需要的包&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="Keras" scheme="http://example.com/categories/Keras/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
    <category term="CNN" scheme="http://example.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>Keras MNIST 过拟合问题解决：Dropout 与 正则化</title>
    <link href="http://example.com/2021/09/02/Keras%20MNIST%20%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%EF%BC%9ADropout%20%E4%B8%8E%20%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <id>http://example.com/2021/09/02/Keras%20MNIST%20%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%EF%BC%9ADropout%20%E4%B8%8E%20%E6%AD%A3%E5%88%99%E5%8C%96/</id>
    <published>2021-09-02T02:40:01.000Z</published>
    <updated>2021-09-17T12:26:09.961Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras MNIST 过拟合问题解决：Dropout 与 正则化">TOC</a></p><p>我们就以 MNIST数据集的手写识别 为例子<br>做 过拟合问题的应用  包括 Dropout 和 正则化</p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><h2 id="使用Dropout-需要导入的包"><a href="#使用Dropout-需要导入的包" class="headerlink" title="使用Dropout 需要导入的包"></a>使用Dropout 需要导入的包</h2><p>需要 导入 另一个包 keras.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure></p><h2 id="使用-正则化-需要导入的包"><a href="#使用-正则化-需要导入的包" class="headerlink" title="使用 正则化  需要导入的包"></a>使用 正则化  需要导入的包</h2><p>layers 层中 引入 keras.regularizers   中  l2 范式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.regularizers <span class="keyword">import</span> l2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure></p><h1 id="载入MNIST-数据"><a href="#载入MNIST-数据" class="headerlink" title="载入MNIST 数据"></a>载入MNIST 数据</h1><p>该数据集一共有训练集 6w 张，测试集 1w 张<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_images,train_labels),(test_images,test_labels)= mnist.load_data()</span><br></pre></td></tr></table></figure></p><p>可以查看一下图像和标签  是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数字5</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签5</span></span><br><span class="line"><span class="built_in">print</span>(train_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 数字7</span></span><br><span class="line"><span class="built_in">print</span>(test_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签7</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以 打印 一下图片 看看是什么样子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(test_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/a745fc12bef94f93b93f80b2e89bb0d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="图片数据处理"><a href="#图片数据处理" class="headerlink" title="图片数据处理"></a>图片数据处理</h1><h2 id="查看图片原有-shape"><a href="#查看图片原有-shape" class="headerlink" title="查看图片原有 shape"></a>查看图片原有 shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_images (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images&#x27;</span>, train_images.shape)</span><br><span class="line"><span class="comment"># train_labels (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><h2 id="图片数据处理：将图片压平"><a href="#图片数据处理：将图片压平" class="headerlink" title="图片数据处理：将图片压平"></a>图片数据处理：将图片压平</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment"># 将 (60000, 28, 28) -&gt; (60000, 784)  压平图片</span></span><br><span class="line">train_images_scale = train_images.reshape(train_images.shape[<span class="number">0</span>], train_images.shape[<span class="number">1</span>] * train_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line">test_images_scale = test_images.reshape(test_images.shape[<span class="number">0</span>], test_images.shape[<span class="number">1</span>] * test_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_images变换后 (60000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images变换后&#x27;</span>, train_images_scale.shape)</span><br><span class="line"><span class="comment"># test_images变换后 (10000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_images变换后&#x27;</span>, test_images_scale.shape)</span><br></pre></td></tr></table></figure><h2 id="标签数据处理：转换成-one-hot-格式"><a href="#标签数据处理：转换成-one-hot-格式" class="headerlink" title="标签数据处理：转换成 one hot 格式"></a>标签数据处理：转换成 one hot 格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换 one hot 格式  共十个分类</span></span><br><span class="line"><span class="comment"># np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</span></span><br><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br><span class="line">test_labels_hot = np_utils.to_categorical(test_labels,num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_labels (60000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>,train_labels_hot.shape)</span><br><span class="line"><span class="comment"># test_labels (10000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_labels&#x27;</span>,test_labels_hot.shape)</span><br></pre></td></tr></table></figure><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><h2 id="添加-Dropout"><a href="#添加-Dropout" class="headerlink" title="添加 Dropout"></a>添加 Dropout</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 输入压平图像 维度为784   输出为 10分类</span></span><br><span class="line"><span class="comment"># 加个隐层</span></span><br><span class="line">model.add(Dense(units=<span class="number">200</span>,input_dim=<span class="number">784</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;tanh&quot;</span>))</span><br><span class="line"><span class="comment"># 上层40%的神经元不工作</span></span><br><span class="line">model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">100</span>,input_dim=<span class="number">200</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;tanh&quot;</span>))</span><br><span class="line"><span class="comment"># 上层40%的神经元不工作</span></span><br><span class="line">model.add(Dropout(<span class="number">0.4</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">100</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;softmax&quot;</span>))</span><br></pre></td></tr></table></figure><p>优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新定义 sgd 优化器  加速一下学习率</span></span><br><span class="line">sgd = SGD(learning_rate=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="添加-正则化项"><a href="#添加-正则化项" class="headerlink" title="添加 正则化项"></a>添加 正则化项</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 输入压平图像 维度为784   输出为 10分类</span></span><br><span class="line"><span class="comment"># 加个隐层</span></span><br><span class="line">model.add(Dense(units=<span class="number">200</span>,input_dim=<span class="number">784</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;tanh&quot;</span>,kernel_initializer=l2(<span class="number">0.003</span>)))</span><br><span class="line">model.add(Dense(units=<span class="number">100</span>,input_dim=<span class="number">200</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;tanh&quot;</span>,kernel_initializer=l2(<span class="number">0.003</span>)))</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">100</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;softmax&quot;</span>,kernel_initializer=l2(<span class="number">0.003</span>)))</span><br></pre></td></tr></table></figure><p>优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新定义 sgd 优化器  加速一下学习率</span></span><br><span class="line">sgd = SGD(learning_rate=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p> 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次<br>训练完一轮6w张，表示一个epoch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次</span></span><br><span class="line"><span class="comment"># 训练完一轮6w张，表示一个epoch</span></span><br><span class="line">model.fit(train_images_scale,train_labels_hot,batch_size=<span class="number">32</span>,epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/576f033299d04330a371c33eb6a2abac.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="训练过程"></p><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><h2 id="Dropout的结果"><a href="#Dropout的结果" class="headerlink" title="Dropout的结果"></a>Dropout的结果</h2><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br>也可以看一下 在训练集上的表现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(train_images_scale,train_labels_hot)</span><br></pre></td></tr></table></figure><p>下面是 结果（上训练集 下测试集）<br><img src="https://img-blog.csdnimg.cn/7c46786df7d74ef383b14a1627e0cf9d.png#pic_center" alt="在这里插入图片描述"></p><h2 id="正则化的结果"><a href="#正则化的结果" class="headerlink" title="正则化的结果"></a>正则化的结果</h2><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br>也可以看一下 在训练集上的表现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(train_images_scale,train_labels_hot)</span><br></pre></td></tr></table></figure><p>下面是 结果（上训练集 下测试集）</p><p><font color="red"><strong>出现了错误 不知道是哪里的问题哎？</strong></font><br><img src="https://img-blog.csdnimg.cn/44c6138733e4436ba6d847fec0f88cfd.png#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: __call__() got an unexpected keyword argument <span class="string">&#x27;dtype&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/e71199da4cfb4c2aaf69d39fc4dacfd3.png#pic_center" alt="在这里插入图片描述"></p><h1 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看下输入的形状</span></span><br><span class="line"><span class="built_in">print</span>(test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>).shape)</span><br><span class="line"><span class="comment"># 模型预测  输出每个分类的 概率</span></span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>))))</span><br><span class="line"><span class="comment"># 选取最大的那个 就是预测的标签</span></span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>)))))</span><br><span class="line"><span class="comment"># 实际该图片的 标签</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输入的图像 变换后形状<br><img src="https://img-blog.csdnimg.cn/4302df2ba0e44c04897aaf00f6955dad.png#pic_center" alt="在这里插入图片描述"><br>模型预测 数据 输出的概率分类结果<br><img src="https://img-blog.csdnimg.cn/3ffa135e578d486c8404588781678e0c.png#pic_center" alt="在这里插入图片描述"><br>选出其中最大的 以及 实际图片标签 均为 数字7<br><img src="https://img-blog.csdnimg.cn/5a585701d439431aa1853283c8592b46.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras MNIST 过拟合问题解决：Dropout 与 正则化&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们就以 MNIST数据集的手写识别 为例子&lt;br&gt;做 过拟合问题的应用  包括 Dropout 和 正则化&lt;/p&gt;
&lt;h1 id=&quot;导入需要的包&quot;&gt;&lt;a</summary>
      
    
    
    
    <category term="Keras" scheme="http://example.com/categories/Keras/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>Keras  MNIST（数字识别）数据集分类（普通全神经网络）</title>
    <link href="http://example.com/2021/08/30/Keras%20%20MNIST%EF%BC%88%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB%EF%BC%88%E6%99%AE%E9%80%9A%E5%85%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89/"/>
    <id>http://example.com/2021/08/30/Keras%20%20MNIST%EF%BC%88%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%89%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E7%B1%BB%EF%BC%88%E6%99%AE%E9%80%9A%E5%85%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89/</id>
    <published>2021-08-30T02:40:01.000Z</published>
    <updated>2021-09-17T12:26:21.379Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras  MNIST（数字识别）数据集分类">TOC</a></p><h1 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h1><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span>  Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(matplotlib.get_backend())</span><br></pre></td></tr></table></figure><h1 id="载入MNIST-数据"><a href="#载入MNIST-数据" class="headerlink" title="载入MNIST 数据"></a>载入MNIST 数据</h1><p>该数据集一共有训练集 6w 张，测试集 1w 张<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_images,train_labels),(test_images,test_labels)= mnist.load_data()</span><br></pre></td></tr></table></figure></p><p>可以查看一下图像和标签  是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数字5</span></span><br><span class="line"><span class="built_in">print</span>(train_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签5</span></span><br><span class="line"><span class="built_in">print</span>(train_labels[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 数字7</span></span><br><span class="line"><span class="built_in">print</span>(test_images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 标签7</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>可以 打印 一下图片 看看是什么样子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(train_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(test_images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/a745fc12bef94f93b93f80b2e89bb0d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h1 id="图片数据处理"><a href="#图片数据处理" class="headerlink" title="图片数据处理"></a>图片数据处理</h1><h2 id="查看图片原有-shape"><a href="#查看图片原有-shape" class="headerlink" title="查看图片原有 shape"></a>查看图片原有 shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_images (60000, 28, 28)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images&#x27;</span>, train_images.shape)</span><br><span class="line"><span class="comment"># train_labels (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><h2 id="图片数据处理：将图片压平"><a href="#图片数据处理：将图片压平" class="headerlink" title="图片数据处理：将图片压平"></a>图片数据处理：将图片压平</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line"><span class="comment"># 将 (60000, 28, 28) -&gt; (60000, 784)  压平图片</span></span><br><span class="line">train_images_scale = train_images.reshape(train_images.shape[<span class="number">0</span>], train_images.shape[<span class="number">1</span>] * train_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line">test_images_scale = test_images.reshape(test_images.shape[<span class="number">0</span>], test_images.shape[<span class="number">1</span>] * test_images.shape[<span class="number">2</span>])/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_images变换后 (60000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_images变换后&#x27;</span>, train_images_scale.shape)</span><br><span class="line"><span class="comment"># test_images变换后 (10000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_images变换后&#x27;</span>, test_images_scale.shape)</span><br></pre></td></tr></table></figure><h2 id="标签数据处理：转换成-one-hot-格式"><a href="#标签数据处理：转换成-one-hot-格式" class="headerlink" title="标签数据处理：转换成 one hot 格式"></a>标签数据处理：转换成 one hot 格式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 换 one hot 格式  共十个分类</span></span><br><span class="line"><span class="comment"># np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</span></span><br><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br><span class="line">test_labels_hot = np_utils.to_categorical(test_labels,num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_labels (60000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_labels&#x27;</span>,train_labels_hot.shape)</span><br><span class="line"><span class="comment"># test_labels (10000, 10)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_labels&#x27;</span>,test_labels_hot.shape)</span><br></pre></td></tr></table></figure><h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><p>只有输入和输出层， 输入压平图像 维度为784   输出为 10分类<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 输入压平图像 维度为784   输出为 10分类</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">784</span>,bias_initializer=<span class="string">&quot;one&quot;</span>,activation=<span class="string">&quot;softmax&quot;</span>))</span><br></pre></td></tr></table></figure><br>优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新定义 sgd 优化器  加速一下学习率</span></span><br><span class="line">sgd = SGD(learning_rate=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 优化器使用加速学习率的 sgd ， 损失函数 选择 交叉熵</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,metrics=<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p> 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次<br>训练完一轮6w张，表示一个epoch<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一共 60000张训练图片 按批次训练 一批次32张   一共6w/32 = 1875 个批次</span></span><br><span class="line"><span class="comment"># 训练完一轮6w张，表示一个epoch</span></span><br><span class="line">model.fit(train_images_scale,train_labels_hot,batch_size=<span class="number">32</span>,epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/576f033299d04330a371c33eb6a2abac.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAamtzODg5OTU2NTY=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="训练过程"></p><h1 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h1><p>就是在 测试集上的表现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss,accuracy = model.evaluate(test_images_scale,test_labels_hot)</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/eb52f38d3e944137a3ade6e5b9d1e1a9.png#pic_center" alt="在这里插入图片描述"></p><h1 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看下输入的形状</span></span><br><span class="line"><span class="built_in">print</span>(test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>).shape)</span><br><span class="line"><span class="comment"># 模型预测  输出每个分类的 概率</span></span><br><span class="line"><span class="built_in">print</span>(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>))))</span><br><span class="line"><span class="comment"># 选取最大的那个 就是预测的标签</span></span><br><span class="line"><span class="built_in">print</span>(np.argmax(model.predict((test_images_scale[<span class="number">0</span>].reshape(-<span class="number">1</span>,<span class="number">784</span>)))))</span><br><span class="line"><span class="comment"># 实际该图片的 标签</span></span><br><span class="line"><span class="built_in">print</span>(test_labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输入的图像 变换后形状<br><img src="https://img-blog.csdnimg.cn/4302df2ba0e44c04897aaf00f6955dad.png#pic_center" alt="在这里插入图片描述"><br>模型预测 数据 输出的概率分类结果<br><img src="https://img-blog.csdnimg.cn/3ffa135e578d486c8404588781678e0c.png#pic_center" alt="在这里插入图片描述"><br>选出其中最大的 以及 实际图片标签 均为 数字7<br><img src="https://img-blog.csdnimg.cn/5a585701d439431aa1853283c8592b46.png#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras  MNIST（数字识别）数据集分类&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;导入需要的包&quot;&gt;&lt;a href=&quot;#导入需要的包&quot; class=&quot;headerlink&quot; title=&quot;导入需要的包&quot;&gt;&lt;/a&gt;导入需要的包&lt;/h1&gt;&lt;p&gt;首先导入</summary>
      
    
    
    
    <category term="Keras" scheme="http://example.com/categories/Keras/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>keras 包方法集合</title>
    <link href="http://example.com/2021/08/29/keras%20%E5%8C%85%E6%96%B9%E6%B3%95%E9%9B%86%E5%90%88/"/>
    <id>http://example.com/2021/08/29/keras%20%E5%8C%85%E6%96%B9%E6%B3%95%E9%9B%86%E5%90%88/</id>
    <published>2021-08-29T02:18:01.000Z</published>
    <updated>2021-09-17T12:26:15.853Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="keras 包方法集合">TOC</a></p><h1 id="keras-util-库-np-utils"><a href="#keras-util-库-np-utils" class="headerlink" title="keras.util 库  np_utils"></a>keras.util 库  np_utils</h1><h2 id="np-utils-to-categorical"><a href="#np-utils-to-categorical" class="headerlink" title="np_utils.to_categorical"></a>np_utils.to_categorical</h2><p>np_utils.to_categorical用于将标签转化为形如(nb_samples, nb_classes)的二值序列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels_hot = np_utils.to_categorical(train_labels,num_classes=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>如将 $[1,2,3,……4]$ 转化成：<br><img src="https://img-blog.csdnimg.cn/379535ca64bc4e5987673ecd12af0c75.png#pic_center" alt="在这里插入图片描述"><br>这样的形态。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;keras 包方法集合&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;keras-util-库-np-utils&quot;&gt;&lt;a href=&quot;#keras-util-库-np-utils&quot; class=&quot;headerlink&quot; title=&quot;keras.util 库</summary>
      
    
    
    
    <category term="Keras" scheme="http://example.com/categories/Keras/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>各类 优化器(调参工具) 详解与选择</title>
    <link href="http://example.com/2021/08/28/%E5%90%84%E7%B1%BB%20%E4%BC%98%E5%8C%96%E5%99%A8(%E8%B0%83%E5%8F%82%E5%B7%A5%E5%85%B7)%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>http://example.com/2021/08/28/%E5%90%84%E7%B1%BB%20%E4%BC%98%E5%8C%96%E5%99%A8(%E8%B0%83%E5%8F%82%E5%B7%A5%E5%85%B7)%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2021-08-28T02:40:01.000Z</published>
    <updated>2021-08-30T02:52:24.871Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="各类 优化器 详解与选择">TOC</a></p><h1 id="sgd"><a href="#sgd" class="headerlink" title="sgd"></a>sgd</h1><h1 id="adam"><a href="#adam" class="headerlink" title="adam"></a>adam</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;各类 优化器 详解与选择&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;sgd&quot;&gt;&lt;a href=&quot;#sgd&quot; class=&quot;headerlink&quot; title=&quot;sgd&quot;&gt;&lt;/a&gt;sgd&lt;/h1&gt;&lt;h1 id=&quot;adam&quot;&gt;&lt;a href=&quot;#adam&quot; </summary>
      
    
    
    
    <category term="优化器" scheme="http://example.com/categories/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    
    
    <category term="优化器" scheme="http://example.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>Keras 构建 线性模型和非线性模型</title>
    <link href="http://example.com/2021/08/24/Keras%20%E6%9E%84%E5%BB%BA%20%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2021/08/24/Keras%20%E6%9E%84%E5%BB%BA%20%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-08-24T09:22:01.000Z</published>
    <updated>2021-09-17T12:26:29.678Z</updated>
    
    <content type="html"><![CDATA[<p>@<a href="Keras 构建 线性模型和非线性模型">TOC</a></p><h1 id="预测线性模型"><a href="#预测线性模型" class="headerlink" title="预测线性模型"></a>预测线性模型</h1><p>使用的数据 是我们随机生成的<br>、<br>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># matplotlib.use(&#x27;TkAgg&#x27;)</span></span><br><span class="line"><span class="comment"># print(matplotlib.get_backend())</span></span><br><span class="line"><span class="comment"># Sequential按顺序构成的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="comment"># Dense全连接层</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br></pre></td></tr></table></figure><p>先准备我们需要的数据。随机生成100个随机值x，并随机产生100个噪声值。我们按 $y=0.1x+0.2$ 的公式，得到对应的y标签值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用numpy 生成100个 随机点</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 测试集，但其实都是随机的 用x_data 也可以</span></span><br><span class="line">x_pre = np.random.rand(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 噪声 使得每个点不是 均匀在一条直线上</span></span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.01</span>,x_data.shape)</span><br><span class="line">y_data = x_data * <span class="number">0.1</span> + <span class="number">0.2</span> + noise</span><br></pre></td></tr></table></figure><p>可以将 100个点的分布图画出。 注意图的显示可能有问题，自行解决一下哦。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><blockquote><p>Q：为什么我们要引入 噪声呢？<br>答：引入噪声可以让我们的数据更加的离散分布 在 我们设计的线性模型上。 使得假设的数据更加的合理。 如下图所示 。 <font color="orange">橙色</font>的是我们设定的线性模型，<font color="blue">蓝色</font>的是 加入噪声以后的数据分布<br><img src="https://img-blog.csdnimg.cn/92fd1e153f2845aba178b80a7845b8cb.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></blockquote><p>使用 keras 中的 Sequential （顺序构成的模型） 构建模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个顺序模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 在模型中添加一个全连接层</span></span><br><span class="line"><span class="comment"># units 输出的维度</span></span><br><span class="line"><span class="comment"># input_dim 输入的维度</span></span><br><span class="line">model.add(Dense(units=<span class="number">1</span>,input_dim=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># sgd 随机梯度下降法</span></span><br><span class="line"><span class="comment"># mse Mean Squared Error 均方误差</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;sgd&#x27;</span>,loss=<span class="string">&#x27;mse&#x27;</span>)</span><br></pre></td></tr></table></figure><p>之后我们就按照批次训练。 共训练3001个批次。有两种写法。<br>方法一：<br>用一个循环体，循环3001次； 每500次 打印一次 损失值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练3001个批次</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3001</span>):</span><br><span class="line">    <span class="comment">#每次训练一个批次</span></span><br><span class="line">    cost = model.train_on_batch(x_data,y_data)</span><br><span class="line">    <span class="comment"># 每500个 batch 打印一次 cost值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;cost:&quot;</span>,cost)</span><br></pre></td></tr></table></figure><br>方法二： 直接使用 model.fit () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x_data,y_data,epochs=<span class="number">3001</span>)</span><br></pre></td></tr></table></figure><p> 可以查看 参数值 W （权重）和 b（偏置值）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W,b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;W：&#x27;</span>,W,<span class="string">&#x27;b:&#x27;</span>,b)</span><br></pre></td></tr></table></figure><p>预测 测试集的 结果 使用 model.predict () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集 输入网络中，得到预测值 y_pred</span></span><br><span class="line">y_pred = model.predict(x_pre)</span><br></pre></td></tr></table></figure><p>可以再 把预测的 图打出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_pre,y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>我们的训练 使用方法二 <strong>结果如图所示：</strong><br><img src="https://img-blog.csdnimg.cn/45c3db30524a45a48e919881f6c36710.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="预测非线性模型"><a href="#预测非线性模型" class="headerlink" title="预测非线性模型"></a>预测非线性模型</h1><p>使用的数据 也是我们随机生成的</p><p>首先导入我们需要的包（直接把Sequential 和 Dense 直接导入 这样之后方便） 注意SGD 需要 tensorflow.keras.optimizers 导入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">matplotlib.use(<span class="string">&#x27;TkAgg&#x27;</span>)</span><br><span class="line"><span class="comment"># Sequential按顺序构成的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="comment"># Dense全连接层</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense,Activation</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> SGD</span><br></pre></td></tr></table></figure><p>先准备我们需要的数据。用等差数列生成200个值x，并随机产生200个噪声值。我们按 $y=x^{2}$ 的公式，得到对应的y标签值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用numpy 生成 200 个随机点</span></span><br><span class="line">x_data = np.linspace(-<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">200</span>)</span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">x_pre = np.linspace(-<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">200</span>)</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.02</span>,x_data.shape)</span><br><span class="line">y_data = np.square(x_data) + noise</span><br></pre></td></tr></table></figure><p>可以将 200个点的分布图画出。 注意图的显示可能有问题，自行解决一下哦。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_data,y_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>使用 keras 中的 Sequential （顺序构成的模型） 构建模型。 <font color="red">与线性模型的区别在，我们需要 增加激活函数，并且增加一个 中间层（含有10个神经元）。</font>  <font color="blue">并且增加一点 sgd 的学习率，不然学习度太慢，需要的训练次数就会非常大。</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个顺序模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 在模型中添加一个全连接层</span></span><br><span class="line"><span class="comment"># units 输出的维度  维度就是神经元个数</span></span><br><span class="line"><span class="comment"># input_dim 输入的维度</span></span><br><span class="line"><span class="comment"># 需要的神经模型为 1-10-1</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,input_dim=<span class="number">1</span>,activation=<span class="string">&#x27;tanh&#x27;</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">1</span>,input_dim=<span class="number">10</span>,activation=<span class="string">&#x27;tanh&#x27;</span>))</span><br><span class="line"><span class="comment"># sgd 随机梯度下降法</span></span><br><span class="line"><span class="comment"># mse Mean Squared Error 均方误差</span></span><br><span class="line"><span class="comment"># sgd 的学习率太小 训练次数可能非常多</span></span><br><span class="line"><span class="comment"># 需要修改一下 sgd的学习率</span></span><br><span class="line">sgd = SGD(lr=<span class="number">0.3</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer= sgd,loss=<span class="string">&#x27;mse&#x27;</span>)</span><br></pre></td></tr></table></figure><p>之后我们就按照批次训练。 共训练3001个批次。有两种写法。<br>方法一：<br>用一个循环体，循环3001次； 每500次 打印一次 损失值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练3001个批次</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3001</span>):</span><br><span class="line">    <span class="comment">#每次训练一个批次</span></span><br><span class="line">    cost = model.train_on_batch(x_data,y_data)</span><br><span class="line">    <span class="comment"># 每500个 batch 打印一次 cost值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;cost:&quot;</span>,cost)</span><br></pre></td></tr></table></figure><br>方法二： 直接使用 model.fit () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x_data,y_data,epochs=<span class="number">3001</span>)</span><br></pre></td></tr></table></figure><p> 可以查看 参数值 W （权重）和 b（偏置值）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W,b = model.layers[<span class="number">0</span>].get_weights()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;W：&#x27;</span>,W,<span class="string">&#x27;b:&#x27;</span>,b)</span><br></pre></td></tr></table></figure><p>预测 测试集的 结果 使用 model.predict () 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集 输入网络中，得到预测值 y_pred</span></span><br><span class="line">y_pred = model.predict(x_pre)</span><br></pre></td></tr></table></figure><p>可以再 把预测的 图打出来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x_pre,y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>我们的训练 使用方法二 <strong>结果如图所示：</strong><br><img src="https://img-blog.csdnimg.cn/79b89f1dec6e4327a2679c2c9882d6c7.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@&lt;a href=&quot;Keras 构建 线性模型和非线性模型&quot;&gt;TOC&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;预测线性模型&quot;&gt;&lt;a href=&quot;#预测线性模型&quot; class=&quot;headerlink&quot; title=&quot;预测线性模型&quot;&gt;&lt;/a&gt;预测线性模型&lt;/h1&gt;&lt;p&gt;使用的数据 是我</summary>
      
    
    
    
    <category term="Keras" scheme="http://example.com/categories/Keras/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="Tensorflow" scheme="http://example.com/tags/Tensorflow/"/>
    
    <category term="Keras" scheme="http://example.com/tags/Keras/"/>
    
    <category term="线性模型" scheme="http://example.com/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="非线性模型" scheme="http://example.com/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
</feed>
