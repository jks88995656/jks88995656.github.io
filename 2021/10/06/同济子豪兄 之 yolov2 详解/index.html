<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>同济子豪兄 之 yolov2 详解 | 一只柴犬</title><meta name="keywords" content="深度学习,yolo"><meta name="author" content="凯凯超人"><meta name="copyright" content="凯凯超人"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="@TOC 蓝色： 表示还不明白什么意思   v1存在 一些性能和原理上的问题：  mAP相比 R-CNN 系列比较低 定位性能比较差，定位错误占总错误的比例很大 Recall比较低，就是把全部目标全部检测出来的能力比较差 检测密集和小目标的能力比较差  这篇文章其实主要有两个模型，即Yolov2 和 Yolo9000. 9000这个只是一个想法，不太实用。Yolov2 是在此基础上做了很多的 tr">
<meta property="og:type" content="article">
<meta property="og:title" content="同济子豪兄 之 yolov2 详解">
<meta property="og:url" content="http://example.com/2021/10/06/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov2%20%E8%AF%A6%E8%A7%A3/index.html">
<meta property="og:site_name" content="一只柴犬">
<meta property="og:description" content="@TOC 蓝色： 表示还不明白什么意思   v1存在 一些性能和原理上的问题：  mAP相比 R-CNN 系列比较低 定位性能比较差，定位错误占总错误的比例很大 Recall比较低，就是把全部目标全部检测出来的能力比较差 检测密集和小目标的能力比较差  这篇文章其实主要有两个模型，即Yolov2 和 Yolo9000. 9000这个只是一个想法，不太实用。Yolov2 是在此基础上做了很多的 tr">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/106_16.jpg">
<meta property="article:published_time" content="2021-10-06T08:09:01.000Z">
<meta property="article:modified_time" content="2021-10-06T08:24:42.013Z">
<meta property="article:author" content="凯凯超人">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="yolo">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/106_16.jpg"><link rel="shortcut icon" href="/img/favicon2.png"><link rel="canonical" href="http://example.com/2021/10/06/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov2%20%E8%AF%A6%E8%A7%A3/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '同济子豪兄 之 yolov2 详解',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-10-06 16:24:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mycss.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/pool.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/iconfont.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/css/flink.min.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="一只柴犬" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/admin.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">41</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/106_16.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">一只柴犬</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">同济子豪兄 之 yolov2 详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-10-06T08:09:01.000Z" title="发表于 2021-10-06 16:09:01">2021-10-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-06T08:24:42.013Z" title="更新于 2021-10-06 16:24:42">2021-10-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Yolo/">Yolo</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="同济子豪兄 之 yolov2 详解"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>@<a href="同济子豪兄 之 yolov2 详解">TOC</a></p>
<font color="blue">蓝色： 表示还不明白什么意思</font> 

<p>v1存在 一些性能和原理上的问题：</p>
<ol>
<li>mAP相比 R-CNN 系列比较低</li>
<li>定位性能比较差，定位错误占总错误的比例很大</li>
<li>Recall比较低，就是把全部目标全部检测出来的能力比较差</li>
<li>检测密集和小目标的能力比较差</li>
</ol>
<p>这篇文章其实主要有两个模型，即Yolov2 和 Yolo9000. 9000这个只是一个想法，不太实用。<br>Yolov2 是在此基础上做了很多的 tricks，作者分别归为了三个类，分别是Better（更准确），Faster（更快的），Stronger（类别更多的）。其中Stronger这块 作者说他可以预测9000种，但其实效果不好的，他只是提供一个想法。我们的重点在 前面的Better（更准确）和Faster（更快）（换用了Darknet-19网络）上。</p>
<blockquote>
<p>总结来看，虽然YOLOv2做的改进，基本都是借鉴其它论文的一些tricks，比如Faster R-CNN的anchor box，YOLOv2采用anchor box和卷积做预测，这基本上与SSD模型（单尺度特征图的SSD）非常类似了，而且SSD也是借鉴了Faster R-CNN的RPN网络。<br>从某种意义上来说，YOLOv2和SSD这两个one-stage模型与RPN网络本质上无异，<strong>只不过RPN不做类别的预测，只是简单地区分物体与背景</strong>。在two-stage方法中，RPN起到的作用是给出region proposals，其实就是作出粗糙的检测，所以另外增加了一个stage，即采用R-CNN网络来进一步提升检测的准确度（包括给出类别预测）。<br>而对于one-stage方法，它们想要一步到位，直接采用“RPN”网络作出精确的预测，要因此要在网络设计上做很多的tricks。<br>YOLOv2的一大创新是采用Multi-Scale Training策略，这样同一个模型其实就可以适应多种大小的图片了。</p>
</blockquote>
<h1 id="Better（更准确）"><a href="#Better（更准确）" class="headerlink" title="Better（更准确）"></a>Better（更准确）</h1><p>作者为使yolo的精度更高，使用了很多个tricks。包括<br><strong>Batch Normalization</strong>、High Resolution Classifer、<strong>Anchor</strong>、Dimension Cluster、Direct location prediction、Fine-Graind Features、Muti-Scale Training</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p>
<blockquote>
<p>Batch Normalization 的详细内容请看<br><a target="_blank" rel="noopener" href="https://jks88995656.github.io/2021/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20-%20Batch%20Normalization/">一只柴犬 深度学习基础 - Batch Normalization </a></p>
</blockquote>
<h2 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h2><p>目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分（CNN特征提取器），由于历史原因，ImageNet分类模型基本采用大小为 224×224 的图片作为输入，分辨率相对较低，不利于检测模型。</p>
<p>  Yolov1中 </p>
<ul>
<li>所以 YOLOv1 在采用 224×224 分类模型预训练后，将分辨率增加至448×448，并使用这个高分辨率在检测数据集上finetune。<strong>但是直接切换分辨率，检测模型可能难以快速适应高分辨率。</strong></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/img_convert/803c6a4d557bf242831be696587816d0.png#pic_center" alt="Yolov1中作者模型训练和测试 输入图片大小情况"></p>
<p> Yolov2中</p>
<ul>
<li><strong>YOLOv2增加了在ImageNet数据集上使用 448×448 输入来finetune分类网络这一中间过程（10 epochs）</strong>（也就是他先是224×224 分辨率训练了一会，再用 448×448<br>分辨率上训练了10个epoch，），这可以使得模型在检测数据集上finetune之前已经适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</li>
</ul>
<h2 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a>Convolutional With Anchor Boxes</h2><p>  Yolov1中</p>
<ul>
<li>在YOLOv1中，输入图片最终被划分为 7×7 个 grid cell，每个grid cell预测2个边界框（bounding box）。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中<strong>存在不同尺度和长宽比（scales and ratios）的物体</strong>，YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在<strong>精确定位方面表现较差</strong>。In another word, v1中并没有对框和对象的关系做匹配，也就是我的框可能都是长的或者都是宽的没个标准，这样匹配起来就比较麻烦。<br><img src="https://img-blog.csdnimg.cn/img_convert/4059677c62015c45885ecfff1025240b.png" alt="v1中的bounding box 野蛮生长"><br>比如上图所示，长的那个符合人这个物体，但对车就不太符合了。</li>
<li>对于YOLOv1，每个grid cell都预测2个 bounding box，每个box包含5个值： (中心点横坐标，中心店纵坐标，框宽度，框高度，置信度)，最后一个置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。<strong>但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。</strong></li>
</ul>
<p> Yolov2</p>
<ul>
<li>v2中引入了先验参考框这个概念。（其实可以理解为一个公共的模板bounding box集合，生成的bounding box 都是这个集合里的）所有的预测框其实都是 先验参考框的<strong>偏移</strong>。每个anchor都对应一个预测框，每个预测框只要 预测出 其相对于Anchor的偏移量。例如下图所示，就是5个先验参考框：<br><img src="https://img-blog.csdnimg.cn/img_convert/0152b5d1eba3e64713e8bf34f771fc17.png#pic_center" alt="5个先验参考框（anchor box）"></li>
<li><p>所以<strong>YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框(bounding box)</strong>。为了使检测所用的特征图分辨率更高，<strong>移除</strong>其中的一个<strong>pool层</strong>。在检测模型中，YOLOv2不是采用 448×448 图片作为输入，而是采用 <strong>416×416 大小</strong>。因为YOLOv2模型下采样的总步长为 32，对于 416×416 大小的图片，<font color="red">最终得到的特征图大小为 13×13 ，维度是<strong>奇数</strong></font>，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。<strong>所以在YOLOv2设计中要保证最终的特征图有奇数个位置。</strong><br><img src="https://img-blog.csdnimg.cn/img_convert/6b72bf553115ff5febd1abb903f7d010.png#pic_center" alt="使用anchor box 假如为2（因为其实就2类物体）"></p>
</li>
<li><p>YOLOv2使用了anchor box之后，每个位置的各个anchor box都单独预测一套分类概率值，这和SSD比较类似（但SSD没有预测置信度，而是把background作为一个类别来处理）。</p>
</li>
<li>在YOLOv2中，<strong>每个grid cell 预测 5个bounding box</strong> （也就是对应的 5个anchor box）。为什么选5呢后面会说（用的聚类）。所以网络结构输出的特征结果 变为了 <strong>13×13×（5+20）× 5  = 13×13×125</strong> ，不是一个grid cell 的bounding box 共享一套分类概率值（条件概率），现在是每个bounding box 都要有自己单独的一套。如下图所示：<br><img src="https://img-blog.csdnimg.cn/img_convert/a56fa6771fcd8496670697f9005ee125.png" alt="v1与v2特征输出的区别"></li>
</ul>
<h2 id="anchor-box-在v2的效果"><a href="#anchor-box-在v2的效果" class="headerlink" title="anchor box 在v2的效果"></a>anchor box 在v2的效果</h2><p>使用anchor boxes之后，<strong>YOLOv2的mAP有稍微下降</strong>（这里下降的原因，有博主猜想是YOLOv2虽然使用了anchor boxes，但是依然采用YOLOv1的训练方法）。YOLOv1只能预测98个边界框（ 7×7×2 ），而YOLOv2使用anchor box之后可以预测更多个边界框（ 13×13×5 ）。所以使用anchor box之后，<strong>YOLOv2的召回率recall大大提升，由原来的81%升至88%</strong>。也就是说yolo检测出物体的能力更强了，但准确性下降了一些。</p>
<h2 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h2><p> 在Faster R-CNN和SSD中，先验框的维度（长和宽）都是<strong>手动设定</strong>的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。</p>
<font color="red">这里的这个功能就是 为了选取 对于所训练的数据集，我选择anchor box 为几个？ 才能最好的效果。</font>

<blockquote>
<p>那这里 总结一下 v2为什么选择 anchor box =5 呢？<br>答：<strong>所谓的anchor box 的数量 说白了 就是 你的数据集到底有多少个类</strong>，把物体对象框大小差不多的归为一类。<br>v2 采用 的是k-means聚类方法对训练集中的边界框标签做了聚类分析。因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标：</p>
<script type="math/tex; mode=display">d(box,centroid) = 1 - IoU(box,centroid)</script><p>在<strong>VOC和COCO数据集上的聚类分析结果</strong>，随着聚类中心数目的增加，平均IOU值（各个边界框与聚类中心的IOU的平均值）是增加的，但是综合考虑模型复杂度和召回率，作者最终选取<strong>5个聚类中心作为先验框</strong> (也就是 anchor box = 5)<br>但是这里先验框的大小具体指什么作者并没有说明，但肯定不是像素点，从代码实现上看，应该是相对于预测的特征图大小（ 13×13）。对比两个数据集，也可以看到COCO数据集上的物体相对小点。这个策略作者并没有单独做实验，但是作者对比了采用聚类分析得到的先验框与手动设置的先验框在平均IOU上的差异，发现前者的平均IOU值更高，因此模型更容易训练学习。<br><img src="https://img-blog.csdnimg.cn/img_convert/5f91f5d74220601c5e3f6d7cb34b81c1.png#pic_center" alt="数据集VOC和COCO上的边界框聚类分析结果  右侧中 黑框是voc2007 的 长宽比聚类  蓝色的是 coco 的长宽比聚类"></p>
<font color="blue">聚类中心怎么看的呢？这边的先验框大小到底是什么意思呢？</font>

</blockquote>
<h2 id="特征提取器-New-Network-Darknet-19"><a href="#特征提取器-New-Network-Darknet-19" class="headerlink" title="特征提取器 New Network: Darknet-19"></a>特征提取器 New Network: Darknet-19</h2><p>YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如下图所示。<br><img src="https://img-blog.csdnimg.cn/img_convert/10c9041bf90f9c2d7a3de23f4b571706.png" alt="Darknet-19 的分类模型 预训练输入的为 224×224"><br>Darknet-19与VGG16模型设计原则是一致的，主要采用 3×3 卷积，采用 2×2 的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用<strong>global avgpooling做预测</strong>，并且在 3×3 卷积之间使用 1×1 卷积来压缩特征图channles以降低模型计算量和参数。Darkxnet-19每个卷积层后面同样使用了Batch Normalization层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。</p>
<h2 id="Direct-location-prediction-直接位置预测"><a href="#Direct-location-prediction-直接位置预测" class="headerlink" title="Direct location prediction 直接位置预测"></a>Direct location prediction 直接位置预测</h2><p><font color="red"><strong>已知 YOLOv2借鉴RPN网络使用anchor box来预测边界框bounding box相对先验框anchor box的偏移量offsets。</strong></font><br>预测边界框 bounding box 的中心位置 $(x,y)$ ，需要根据预测的坐标偏移值 $(t<em>{x},t</em>{y})$，<font color="blue">先验框 anchor box 的尺度 $(w<em>{a},h</em>{a})$</font> 以及中心坐标 $(x<em>{a},y</em>{a})$ （特征图（边长是奇数）每个位置的中心点）来计算：</p>
<p><font color="blue">（不知道他这边怎么理解的 我是改了一下）</font></p>
<pre><code>$$x = x_&#123;a&#125;-(t_&#123;x&#125; × w_&#123;a&#125;)$$ $$y=y_&#123;a&#125;-(t_&#123;y&#125;×h_&#123;a&#125;)$$
</code></pre><p>这个公式是<strong>无约束的，预测的边界框很容易向任何方向偏移</strong>，如当 $t<em>{x}=1$ 时边界框将向左偏移先验框的一个宽度大小，而当 $t</em>{x}=-1$ 时边界框将向右偏移先验框的一个宽度大小，因此每个位置<strong>预测的边界框可以落在图片任何位置</strong>，可能移动幅度过大，这导致模型的<strong>不稳定性</strong>，在训练时需要很长时间来预测出正确的offsets。<br>所以，YOLOv2弃用了这种预测w方式，而是沿用YOLOv1的方法，就是<strong>预测边界框中心点相对于对应cell左上角位置$(c<em>{x},c</em>{y})$的相对偏移值</strong>，<font color="blue">为了将边界框 bounding box 中心点约束在当前cell中 </font>，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets $t<em>{x},t</em>{y},t<em>{w},t</em>{h}$ ，可以按如下公式计算出边界框实际位置和大小$(b<em>{x},b</em>{y},b<em>{w},b</em>{h})$：<br><img src="https://img-blog.csdnimg.cn/img_convert/920c51b2db6727365725e3fb831d027e.png#pic_center" alt=""><br>其中 $(c<em>{x},c</em>{y})$ 为grid cell的左上角坐标，如下图所示。<br><img src="https://img-blog.csdnimg.cn/img_convert/f3be09eb2cb40eb39ee6ce782070fc22.png#pic_center" alt="边界框位置与大小的计算示例图"><br>在计算时每个grid cell的尺度为1，所以当前grid cell的左上角坐标为$(1,1)$。由于 $sigmoid函数$ 的处理，<strong>边界框的中心位置会约束在当前grid cell内部，防止偏移过多</strong>。而 $p<em>{w}$ 和 $p</em>{h}$ 是先验框 anchor box 的宽度与长度，前面说过它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1。这里记特征图的大小为 $(W,H)$ （v2中为 (13,13) )，这样我们可以将边界框相对于整张图片的位置和大小计算出来  <font color="blue">（4个值均在0和1之间）?? 什么四个值  是下图的4个 这是左上角是（1,1）右下角是（0,0）？？么</font><br><img src="https://img-blog.csdnimg.cn/img_convert/b7643a7e41364c7a09b018e58a010571.png#pic_center" alt="得到的四个 尺度值"><br><strong>如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的最终位置和大小了</strong>。这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。</p>
<h2 id="Fine-Grained-Features-细粒度特征图"><a href="#Fine-Grained-Features-细粒度特征图" class="headerlink" title="Fine-Grained Features 细粒度特征图"></a>Fine-Grained Features 细粒度特征图</h2><p>YOLOv2的输入图片大小为 416×416 ，经过5次maxpooling之后得到 13×13大小的特征图，并以此特征图采用卷积做预测。 13×13大小的特征图对检测大物体是足够了，但是<strong>对于小物体还需要更精细的特征图（Fine-Grained Features）</strong>。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种pass through层来利用更精细的特征图。如下图为整体的网络结构：<br><img src="https://img-blog.csdnimg.cn/img_convert/989dde6af769da5bd9ba6012e8bef92e.png#pic_center" alt="Darknet19检测模型+passthrough操作"><br>passthrough层与ResNet网络的shortcut类似，以前面<strong>更高分辨率的特征图为输入</strong>，然后将其<strong>连接到后面的低分辨率特征图</strong>上。<br>前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个 2×2 的局部区域，然后将其转化为channel维度，对于26×26×512 的特征图，经passthrough层处理之后就变成了 13×13×2048 的新特征图（特征图大小降低4倍，而channles增加4倍。操作如下图所示：</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/4051f9f715281e7658ab01fb6557807b.png#pic_center" alt="passthrough 增加通道数的操作栗子"><br>这样也就得到了 13×13×2048 的输出，可以与后面的 13×13×1024 低分辨率特征图连接在一起形成 13×13×3072 大小的特征图，然后在此特征图基础上卷积做预测。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/8a808bf40d6b819b196d7d348c755dca.png" alt="就是这样"></p>
<p><font color="blue">在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层：</font><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out = tf.extract_image_patches(<span class="keyword">in</span>, [<span class="number">1</span>, stride, stride, <span class="number">1</span>], [<span class="number">1</span>, stride, stride, <span class="number">1</span>], [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">&quot;VALID&quot;</span>)</span><br><span class="line">// <span class="keyword">or</span> use tf.space_to_depth</span><br><span class="line">out = tf.space_to_depth(<span class="keyword">in</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<p>这是v2论文中刚开始的想法，作者后期借鉴了ResNet网络，<strong>不是直接对高分辨特征图处理</strong>，而是增加了一个<strong>中间1×1卷积层</strong>，先采用64个1×1 卷积核进行卷积，然后再进行passthrough处理，这样 26×26×512的特征图得到 13×13×256 的特征图。最后再高和低分辨率合并，得到13×13×1280的特征图做预测。这算是实现上的一个trick。使用Fine-Grained Features之后YOLOv2的性能有1%的提升。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/cd777947365d746debcde922ad285dfb.png#pic_center" alt="作者改进后的 也就是真实的代码设计"></p>
<h2 id="Multi-Scale-Training-多尺度训练"><a href="#Multi-Scale-Training-多尺度训练" class="headerlink" title="Multi-Scale Training 多尺度训练"></a>Multi-Scale Training 多尺度训练</h2><p>YOLOv2的一大创新是采用Multi-Scale Training策略，<strong>这样同一个模型其实就可以适应多种大小的图片</strong>。</p>
<p><font color="blue">由于YOLOv2模型中采用了global average pooling 全局平均池化层 ，所以YOLOv2的输入可以不限于 416×416 大小的图片。</font>为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔10个 iterations 之后改变模型的输入图片大小。由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值：{320,352，…608}。在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。<br><img src="https://img-blog.csdnimg.cn/img_convert/8a03850706857e9f5a4cd666bae33301.png#pic_center" alt="Multi-Scale Training 多尺度输入训练"><br>YOLOv2在 VOC2007和2012的训练集上，可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于 544×544 ，mAP高达78.6%。<strong>注意，这只是训练时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。</strong><br>这里也可以看出，多尺度训练的一个副作用：如果你输入的是一个高分辨率的大图片 yolo会预测的比较慢但是比较准 如果你输入的是低分辨率的小图片，yolo会预测的非常快但是精度没有那么高。因此yolo可以通过输入图片的大小尺度，来做到精度和速度的权衡。</p>
<h1 id="YOLOv2的训练"><a href="#YOLOv2的训练" class="headerlink" title="YOLOv2的训练"></a>YOLOv2的训练</h1><p>YOLOv2的训练主要包括三个阶段。</p>
<ol>
<li>第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为  ，共训练160个epochs。</li>
<li>第二阶段将网络的输入调整为 448×448 ，继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。</li>
<li><strong>第三个阶段就是修改Darknet-19分类模型为检测模型</strong>，并在检测数据集上继续finetune网络。网络修改包括（网路结构可视化）：<font color="red">移除最后一个卷积层、global average pooling层以及softmax层，并且新增了三个 3×3×1024 卷积层，同时增加了一个passthrough层，最后使用 1×1 卷积层输出预测结果，输出的channels数为： $anchor个数 × (5+类别个数)$（在v2中 为 5×（5+20）） ，和训练采用的数据集有关系。</font>由于anchors数为5，对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425（因为COCO是80个分类）。</li>
</ol>
<p>这里以VOC数据集为例，最终的预测矩阵为 $T$（shape为 （batch_size,13,13,125）），可以先将其reshape为 （batch_size,13,13,5,25），<font color="blue">其中 $T[:,:,:,:,0:4]$ 为边界框的位置和大小 $t_x,t_y,t_w,t_h$， $T[:,:,:,:,4]$ 为边界框的置信度，而 $T[:,:,:,:,5:]$为类别预测值。 </font><br><img src="https://img-blog.csdnimg.cn/img_convert/533f8c8dcc9da79ea03a5ca90bd00f4a.png#pic_center" alt="YOLOv2训练的三个阶段"><br><img src="https://img-blog.csdnimg.cn/img_convert/89c0c117e6648f93753cc6416d0bbadf.png#pic_center" alt="YOLOv2 检测网络结构示意图"></p>
<h2 id="YOLOv2的损失函数"><a href="#YOLOv2的损失函数" class="headerlink" title="YOLOv2的损失函数"></a>YOLOv2的损失函数</h2><p>v2的预测框和类别的想法其实与v1是一致的，对于训练图片中的ground truth，若其中心点落在某个grid cell内，那么该grid cell内的5个先验框 anchor box 所对应的预测框 bounding box 负责预测它，具体是哪个bounding box预测它，需要在训练中确定，即由那个与ground truth的IoU最大的 bounding box 预测它，而剩余的4个 bounding box 不与该ground truth匹配。<br><img src="https://img-blog.csdnimg.cn/img_convert/68b2ccd52bf5d18fd4a234ac07c12057.png#pic_center" alt="YOLOv2的损失函数"></p>
<ol>
<li>首先 $W、H$分别指的是特征图（ 13×13 ）的宽13与高13，而 $A$ 指的是先验框anchor box数目（这里是5），各个 $λ$ 值是各个 loss 部分的权重系数（即超参数）。<br><img src="https://img-blog.csdnimg.cn/img_convert/2b4666868ab3c549bf793cfa3acf9e59.png#pic_center" alt="在这里插入图片描述"></li>
<li><font color="red">第一项loss是计算 该预测框不负责检测物体(background) 的置信度误差(越小越好)</font>，但是哪些预测框不负责检测物体对象而预测背景呢，需要先计算各个预测框和<strong>所有ground truth的IOU值</strong>，并且取<strong>最大值Max_IOU</strong>，如果该值<strong>小于</strong>一定的<strong>阈值</strong>（YOLOv2使用的是0.6），那么这个预测框就标记为background，<font color="blue">需要计算$λ_{noobj}$ 的置信度误差。</font><br><img src="https://img-blog.csdnimg.cn/img_convert/b0b4307ec6cdad8bde6ec679a55b6d8b.png#pic_center" alt="在这里插入图片描述"></li>
<li><p><font color="red">第二项是计算先验框anchor box与预测框bounding box的坐标误差</font>，<font color="blue">但是只在前12800个iterations间计算，该博客博主认为 这项应该是在训练前期使预测框快速学习到先验框的位置和形状。</font><br><img src="https://img-blog.csdnimg.cn/img_convert/fdd54195cbe12c0fc17d3ac02a61a179.png#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p>Q1: 为什么是 前12800个iterations？</p>
</blockquote>
</li>
<li><p><font color="red">第三大项计算与某个ground truth匹配的预测框各部分loss值，包括定位（坐标）误差、置信度误差以及分类误差。</font>先说一下匹配原则，对于某个ground truth，首先要确定其中心点要落在哪个grid cell上，然后计算这个grid cell的5个先验框 anchor box 与ground truth的IoU值<font color="blue">（YOLOv2中bias_match=1）</font>，<strong>计算IoU值时不考虑坐标，只考虑形状</strong>，所以先将先验框与ground truth的中心点都偏移到同一位置（原点），然后计算出对应的IoU值，IoU值最大的那个先验框与ground truth匹配，对应形状的预测框用来预测这个ground truth。 说白了就是 我先看看 anchorbox 模板里面哪个和 ground truth形状比较像，这样我的框比较对嘛。然后选出这个框 找到 这个grid cell 里面 5个bounding box 和这个anchor 形状一样的那个框 去匹配ground truth。 说白了就是 弄了个中间商去选 形状 匹配。<br><font color="blue">在计算obj置信度时，target=1，但与YOLOv1一样而增加了一个控制参数rescore，当其为1时，target取预测框与ground truth的真实IOU值（cfg文件中默认采用这种方式）。 ???什么意思</font><br>对于那些没有与ground truth匹配的先验框（与预测框对应），除去那些Max_IOU低于阈值的，其它的就全部忽略，不计算任何误差。这点在YOLOv3论文中也有相关说明：<strong>YOLO中一个ground truth只会与一个先验框匹配（IOU值最好的）</strong>，对于那些IOU值超过一定阈值的先验框，其预测结果就忽略了。</p>
</li>
</ol>
<p> 尽管YOLOv2和YOLOv1计算loss处理上有不同，但都是采用均方差来计算loss。另外需要注意的一点是，在计算boxes的 $w$ 和 $h$ 误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，<font color="blue">但是根据ground truth的大小对权重系数进行修正：l.coord_scale <em> (2 - truth.w</em>truth.h)（这里w和h都归一化到(0,1))</font>，这样对于尺度较小的boxes其权重系数会更大一些，可以放大误差，起到和YOLOv1计算平方根相似的效果（参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/YiXiaoZhou/p/7429481.html">YOLO v2 损失函数源码分析</a>）。 </p>
<p>最终的YOLOv2模型在速度上比YOLOv1还快（采用了计算量更少的Darknet-19模型），而且模型的准确度比YOLOv1有显著提升，详情见paper。</p>
<h1 id="Yolov2的预测"><a href="#Yolov2的预测" class="headerlink" title="Yolov2的预测"></a>Yolov2的预测</h1><p>同Yolov1 的预测 同样是消除多余的框。可以看一下Yolov1 是怎么 在网络提取出特征图 后 如何 消除多余的框的。 <a target="_blank" rel="noopener" href="https://jks88995656.github.io/2021/09/26/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov1%20%E8%AF%A6%E8%A7%A3/">Yololv1的详解</a></p>
<h1 id="Yolo9000"><a href="#Yolo9000" class="headerlink" title="Yolo9000"></a>Yolo9000</h1><p>YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。<br>ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p>
<p>这个装逼不成的作者 <strong>选择在COCO和ImageNet数据集上进行联合训练</strong>，但是遇到的第一问题是<font color="red">两者的类别并不是完全互斥的，</font>比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：<br><img src="https://img-blog.csdnimg.cn/img_convert/e081b98313870f03c620f2d8fb0ddb7e.png#pic_center" alt="基于COCO和ImageNet数据集建立的WordTree"><br>WordTree中的根节点为”physical object”，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个path，然后计算path上各个节点的概率之积。<br><img src="https://img-blog.csdnimg.cn/img_convert/63cf274736bb7b43970301aa7d192431.png#pic_center" alt="ImageNet与WordTree预测的对比"><br><img src="https://img-blog.csdnimg.cn/img_convert/e6f7edd74a243934d58718fd27e46d23.png#pic_center" alt="Yolo9000"><br>在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是 $Pr(physical    -object)$ ，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。</p>
<p>通过联合训练策略，YOLO9000可以快速检测出超过9000个类别的物体，总体mAP值为19.7%。效果一般般，很多类别检测不出来，特别是物体类。<br><img src="https://img-blog.csdnimg.cn/img_convert/ebf757ed8dcdec3f000ab814355f10b3.png#pic_center" alt="在这里插入图片描述"><br>作者提出的 yolo9000 算是一个开创的想法，但是效果不佳（不然他早就在youtube上装逼了）</p>
<h1 id="参考论文和博客"><a href="#参考论文和博客" class="headerlink" title="参考论文和博客"></a>参考论文和博客</h1><p>参考博客：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35325884">转载来源 目标检测|YOLOv2原理与实现</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/YiXiaoZhou/p/7429481.html">YOLO v2 损失函数源码分析 源码c版的</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54073204">Batch Normalization的通俗解释</a></li>
</ul>
<p>参考视频：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Q64y1s74K?p=1">同济子豪兄v2算法讲解</a></li>
<li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Q64y1s74K?p=2">同济子豪兄v2论文精讲</a></p>
<p>keras代码 github：YAD2K-master<br>参考论文：YOLO9000 Better, Faster, Stronger</p>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">凯凯超人</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2021/10/06/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov2%20%E8%AF%A6%E8%A7%A3/">http://example.com/2021/10/06/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov2%20%E8%AF%A6%E8%A7%A3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">一只柴犬</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/yolo/">yolo</a></div><div class="post_share"><div class="social-share" data-image="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/106_16.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20-%20Batch%20Normalization/"><img class="next-cover" src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/105_5.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度学习基础 - Batch Normalization</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/09/26/同济子豪兄 之 yolov1 详解/" title="同济子豪兄 之 yolov1 详解"><img class="cover" src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/223234.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-26</div><div class="title">同济子豪兄 之 yolov1 详解</div></div></a></div><div><a href="/2021/08/10/为什么要Deep？深而不是宽/" title="为什么要Deep？深而不是宽"><img class="cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-10</div><div class="title">为什么要Deep？深而不是宽</div></div></a></div><div><a href="/2021/09/20/梯度消失和梯度爆炸的理解/" title="梯度消失和梯度爆炸的理解"><img class="cover" src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/20151107191711_H5LwP.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-20</div><div class="title">梯度消失和梯度爆炸的理解</div></div></a></div><div><a href="/2021/08/07/深度学习 Deep Learning 基础/" title="深度学习 Deep Learning 基础"><img class="cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-07</div><div class="title">深度学习 Deep Learning 基础</div></div></a></div><div><a href="/2021/10/05/深度学习基础 - Batch Normalization/" title="深度学习基础 - Batch Normalization"><img class="cover" src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/105_5.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-05</div><div class="title">深度学习基础 - Batch Normalization</div></div></a></div><div><a href="/2021/10/04/目标检测的中的指标的含义及其实现/" title="目标检测的中的指标的含义及其实现"><img class="cover" src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/shakhasdk.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-04</div><div class="title">目标检测的中的指标的含义及其实现</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/admin.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">凯凯超人</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">41</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xxxxx" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xxxxxx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://gitee.com/×××" target="_blank" title="Gitee"><i class="iconfont gitee"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Better%EF%BC%88%E6%9B%B4%E5%87%86%E7%A1%AE%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">Better（更准确）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">1.1.</span> <span class="toc-text">Batch Normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#High-Resolution-Classifier"><span class="toc-number">1.2.</span> <span class="toc-text">High Resolution Classifier</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Convolutional-With-Anchor-Boxes"><span class="toc-number">1.3.</span> <span class="toc-text">Convolutional With Anchor Boxes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#anchor-box-%E5%9C%A8v2%E7%9A%84%E6%95%88%E6%9E%9C"><span class="toc-number">1.4.</span> <span class="toc-text">anchor box 在v2的效果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dimension-Clusters"><span class="toc-number">1.5.</span> <span class="toc-text">Dimension Clusters</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8-New-Network-Darknet-19"><span class="toc-number">1.6.</span> <span class="toc-text">特征提取器 New Network: Darknet-19</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Direct-location-prediction-%E7%9B%B4%E6%8E%A5%E4%BD%8D%E7%BD%AE%E9%A2%84%E6%B5%8B"><span class="toc-number">1.7.</span> <span class="toc-text">Direct location prediction 直接位置预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fine-Grained-Features-%E7%BB%86%E7%B2%92%E5%BA%A6%E7%89%B9%E5%BE%81%E5%9B%BE"><span class="toc-number">1.8.</span> <span class="toc-text">Fine-Grained Features 细粒度特征图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-Scale-Training-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="toc-number">1.9.</span> <span class="toc-text">Multi-Scale Training 多尺度训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#YOLOv2%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="toc-number">2.</span> <span class="toc-text">YOLOv2的训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLOv2%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">YOLOv2的损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Yolov2%E7%9A%84%E9%A2%84%E6%B5%8B"><span class="toc-number">3.</span> <span class="toc-text">Yolov2的预测</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Yolo9000"><span class="toc-number">4.</span> <span class="toc-text">Yolo9000</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%AE%BA%E6%96%87%E5%92%8C%E5%8D%9A%E5%AE%A2"><span class="toc-number">5.</span> <span class="toc-text">参考论文和博客</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/06/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov2%20%E8%AF%A6%E8%A7%A3/" title="同济子豪兄 之 yolov2 详解"><img src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/106_16.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="同济子豪兄 之 yolov2 详解"/></a><div class="content"><a class="title" href="/2021/10/06/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov2%20%E8%AF%A6%E8%A7%A3/" title="同济子豪兄 之 yolov2 详解">同济子豪兄 之 yolov2 详解</a><time datetime="2021-10-06T08:09:01.000Z" title="发表于 2021-10-06 16:09:01">2021-10-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20-%20Batch%20Normalization/" title="深度学习基础 - Batch Normalization"><img src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/105_5.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深度学习基础 - Batch Normalization"/></a><div class="content"><a class="title" href="/2021/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%20-%20Batch%20Normalization/" title="深度学习基础 - Batch Normalization">深度学习基础 - Batch Normalization</a><time datetime="2021-10-05T07:29:01.000Z" title="发表于 2021-10-05 15:29:01">2021-10-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/04/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E4%B8%AD%E7%9A%84%E6%8C%87%E6%A0%87%E7%9A%84%E5%90%AB%E4%B9%89%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/" title="目标检测的中的指标的含义及其实现"><img src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/shakhasdk.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="目标检测的中的指标的含义及其实现"/></a><div class="content"><a class="title" href="/2021/10/04/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E4%B8%AD%E7%9A%84%E6%8C%87%E6%A0%87%E7%9A%84%E5%90%AB%E4%B9%89%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/" title="目标检测的中的指标的含义及其实现">目标检测的中的指标的含义及其实现</a><time datetime="2021-10-04T14:01:01.000Z" title="发表于 2021-10-04 22:01:01">2021-10-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/26/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov1%20%E8%AF%A6%E8%A7%A3/" title="同济子豪兄 之 yolov1 详解"><img src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/223234.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="同济子豪兄 之 yolov1 详解"/></a><div class="content"><a class="title" href="/2021/09/26/%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%20%E4%B9%8B%20yolov1%20%E8%AF%A6%E8%A7%A3/" title="同济子豪兄 之 yolov1 详解">同济子豪兄 之 yolov1 详解</a><time datetime="2021-09-26T14:01:01.000Z" title="发表于 2021-09-26 22:01:01">2021-09-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/25/%E5%90%B4%E6%81%A9%E8%BE%BEdeeplearning.ai%E5%AD%A6%E4%B9%A0%20%20%E4%B9%8B%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" title="吴恩达deeplearning.ai学习  之 目标检测"><img src="http://kyle-pic.oss-cn-hangzhou.aliyuncs.com/img/cat1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="吴恩达deeplearning.ai学习  之 目标检测"/></a><div class="content"><a class="title" href="/2021/09/25/%E5%90%B4%E6%81%A9%E8%BE%BEdeeplearning.ai%E5%AD%A6%E4%B9%A0%20%20%E4%B9%8B%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" title="吴恩达deeplearning.ai学习  之 目标检测">吴恩达deeplearning.ai学习  之 目标检测</a><time datetime="2021-09-25T08:37:01.000Z" title="发表于 2021-09-25 16:37:01">2021-09-25</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 凯凯超人</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/js/pool.min.js"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"><script src="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/js/pool.min.js"></script><script src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"></script><script src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/hideMobileSidebar.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>