<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>深度学习 Deep Learning 模型优化 | 一只柴犬</title><meta name="keywords" content="深度学习,李宏毅,模型优化"><meta name="author" content="凯凯超人"><meta name="copyright" content="凯凯超人"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="@TOC 深度学习  怎么评价效果与改进？  先检查 训练阶段 是否有比较好的结果training优化方法： 　 换激活函数（Sigmoid、ReLU、Maxout、Tanh、Softmax） 优化器——优化梯度下降和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）   training没问题了，再检查testing是否有比较好的结果　testing（过拟合）优">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习 Deep Learning 模型优化">
<meta property="og:url" content="http://example.com/2021/08/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/index.html">
<meta property="og:site_name" content="一只柴犬">
<meta property="og:description" content="@TOC 深度学习  怎么评价效果与改进？  先检查 训练阶段 是否有比较好的结果training优化方法： 　 换激活函数（Sigmoid、ReLU、Maxout、Tanh、Softmax） 优化器——优化梯度下降和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）   training没问题了，再检查testing是否有比较好的结果　testing（过拟合）优">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/3.jpg">
<meta property="article:published_time" content="2021-08-09T14:36:01.000Z">
<meta property="article:modified_time" content="2021-08-09T14:37:14.552Z">
<meta property="article:author" content="凯凯超人">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="李宏毅">
<meta property="article:tag" content="模型优化">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/3.jpg"><link rel="shortcut icon" href="/img/favicon2.png"><link rel="canonical" href="http://example.com/2021/08/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习 Deep Learning 模型优化',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-08-09 22:37:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mycss.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/pool.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/iconfont.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/css/flink.min.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="一只柴犬" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/admin.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/3.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">一只柴犬</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E9%9F%B3%E4%B9%90"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E7%94%B5%E5%BD%B1"><i class="fa-fw /movies/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E7%85%A7%E7%89%87"><i class="fa-fw /Gallery/"></i><span> 2</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习 Deep Learning 模型优化</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-09T14:36:01.000Z" title="发表于 2021-08-09 22:36:01">2021-08-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-08-09T14:37:14.552Z" title="更新于 2021-08-09 22:37:14">2021-08-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%9D%8E%E5%AE%8F%E6%AF%85/">机器学习基础-李宏毅</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习 Deep Learning 模型优化"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>@<a href="深度学习 Deep Learning 后续优化">TOC</a></p>
<p><strong>深度学习  怎么评价效果与改进？</strong></p>
<ol>
<li>先检查 训练阶段 是否有比较好的结果<br>training优化方法：
　<ol>
<li>换激活函数（Sigmoid、ReLU、Maxout、Tanh、Softmax）</li>
<li>优化器——优化梯度下降和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）</li>
</ol>
</li>
<li>training没问题了，再检查testing是否有比较好的结果<br>　testing（过拟合）优化：<ol>
<li>参数在过拟合之前就停止更新</li>
<li>正则化Regularization</li>
<li>dropout</li>
</ol>
</li>
</ol>
<h1 id="如何优化模型"><a href="#如何优化模型" class="headerlink" title="如何优化模型"></a>如何优化模型</h1><h2 id="谈什么才是overfitting？"><a href="#谈什么才是overfitting？" class="headerlink" title="谈什么才是overfitting？"></a>谈什么才是overfitting？</h2><p>首先我们在明确一下，深度学习的流程，其与机器学习基本一致。如下图所示：<img src="https://img-blog.csdnimg.cn/0f42918289e14d23a7c8eada3056de07.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>判断  <em>过拟合</em> 需要看两个数据集上的结果（training set → good， testing set → bad）。<br><strong>在试图解决overfitting之后仍要再看一下training set上的结果！</strong></p>
<blockquote>
<p><strong>误区：</strong><br>不能看见所有不好的 <em>performance</em> 都归因到 <em>overfitting</em>。如只看下右图，不能断言56-layer有 <em>overfitting</em>，要看模型在training set上的表现。根据下左图，可以发现原因是训练的时候没有训练好，即这个层次设计可能本身就是有问题的，不然为啥20层的挺好，56层没道理差啊（这不能叫 <em>underfitting</em>，<em>underfitting</em>：参数不够多，模型能力不足）。<img src="https://img-blog.csdnimg.cn/d9f2b9e2e027466286e85d6b3998b713.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="对症下药：训练error，测试error分别用什么方法"><a href="#对症下药：训练error，测试error分别用什么方法" class="headerlink" title="对症下药：训练error，测试error分别用什么方法"></a>对症下药：训练error，测试error分别用什么方法</h2><p>在读到深度学习的方法时，要思考该方法是解决什么问题。<br><strong>是解决training set上的performance不好，还是解决testing set上的performance不好。</strong>比如，Dropout是为了解决testing set上结果不好的问题，如果是training set上结果不好而用Dropout，不会有好的结果。<br>下图是 分别解决各问题，可以采用的方法：<img src="https://img-blog.csdnimg.cn/f74d3b5a294e40648e9f9646b7c8e063.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="模型Train阶段Error-具体解决"><a href="#模型Train阶段Error-具体解决" class="headerlink" title="模型Train阶段Error 具体解决"></a>模型Train阶段Error 具体解决</h2><p>如下图是，MNIST手写数字识别，激活函数用sigmoid，training data上的accuracy与层数的关系曲线：<img src="https://img-blog.csdnimg.cn/91b2987e4b43432f88b04cd027a9ec56.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>层数&gt;7时，performance下降，<strong>原因不是 <em>overfitting</em>! 因为train的时候就没train好。</strong><br>那模型压根没有训练好，可以采用的方法上面也给出了包括：换激活函数（Sigmoid、ReLU、Maxout、Tanh、Softmax）。激活函数可能会带来什么样的问题？以sigmoid为例说：会出现梯度消失。</p>
<h3 id="方法一：换激活函数的问题-——-梯度消失"><a href="#方法一：换激活函数的问题-——-梯度消失" class="headerlink" title="方法一：换激活函数的问题 —— 梯度消失"></a>方法一：换激活函数的问题 —— 梯度消失</h3><h4 id="什么是梯度消失"><a href="#什么是梯度消失" class="headerlink" title="什么是梯度消失"></a>什么是梯度消失</h4></blockquote>
<p>有梯度时，参数才会往梯度最小的地方改变；没有梯度了，参数就停止更新了。<br>前面层的学习速率明显低于后面层（后向传播），这就是梯度消失。<img src="https://img-blog.csdnimg.cn/f82182495a2f4ec5bd000941331d346a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h4 id="为什么会有梯度消失？"><a href="#为什么会有梯度消失？" class="headerlink" title="为什么会有梯度消失？"></a>为什么会有梯度消失？</h4><blockquote>
<p>角度一： 用sigmoid会出现梯度消失的问题（<strong>参数的变化经过sigmoid会逐层衰减，对最后的loss影响很小</strong>）<br><img src="https://img-blog.csdnimg.cn/052d1db3b2ef4b628a984db51778bf5a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如上图所示，我刚开始增加的△w，由于 <em>sigmoid函数</em> ，自变量越大，改变量越小的缘故。数的变化经过sigmoid会逐层衰减，对最后的loss影响很小。也就是对C的偏导会越来越小，导致梯度接近于0，而消失。</p>
<p>角度二：<br>假设现在存在一个网络结构：<br><img src="https://img-blog.csdnimg.cn/2de3a4da8a8c4ad1b0c6ca4fe6f5787d.png#pic_center" alt="在这里插入图片描述"><br>其整个数学模型可以表示为：<img src="https://img-blog.csdnimg.cn/f0e9bab2192e48d5b30ff91f0cb7ac58.png#pic_center" alt="在这里插入图片描述"><br>若要对于 <em>w1</em><br>求梯度，根据链式求导法则，得到的解为：<img src="https://img-blog.csdnimg.cn/f685ef885b0541198b35f65f257e4296.png#pic_center" alt="在这里插入图片描述"><br>这明显数学模型的值，随着隐层的增加，越来越小。不过这个前提是 原先的w（权重）并不是很大，原本相乘就是小于1的。如果w太大的话，第一项相乘就大于1，这就会导致最后的梯度爆炸。</p>
</blockquote>
<h4 id="如何解决梯度消失问题-——-换ReLU激活函数"><a href="#如何解决梯度消失问题-——-换ReLU激活函数" class="headerlink" title="如何解决梯度消失问题 —— 换ReLU激活函数"></a>如何解决梯度消失问题 —— 换ReLU激活函数</h4><h5 id="ReLU-与-MaxOut"><a href="#ReLU-与-MaxOut" class="headerlink" title="ReLU 与 MaxOut"></a>ReLU 与 MaxOut</h5><p>梯度消失是因为 <em>sigmoid</em> 引起的，要解决当然要换一个激活函数。采用的方法是换 <em>ReLU激活函数</em>（原型 input<0时，输出为0，input>0,输出为原值；可变型）<img src="https://img-blog.csdnimg.cn/44d11711c43a4495af5a232ba87f8df6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>ReLU输出0或x，输出0的ReLU神经元相当于不存在，网络变得瘦长，但是整个网络仍然是<strong>非线性的</strong>，只有当input改变十分微小的时候才是线性的，因为input不同，输出为0的ReLU神经元也不同。</p>
<blockquote>
<p><strong>Q : 为什么ReLU是非线性？明明两端都是线性直线啊？</strong><br>因为ReLU在0点没有导数定义，线性讲究的是在整个定义域内。而在0点处，不可微，所以整体是非线性的。</p>
</blockquote>
<p> ReLU是<strong>Maxout</strong>的特例，Maxout可以学出激活函数。<br><img src="https://img-blog.csdnimg.cn/436e424d5bf8460a9eaecb263ffdc73d.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5 id="MaxOut-的训练是怎样的？好在哪？"><a href="#MaxOut-的训练是怎样的？好在哪？" class="headerlink" title="MaxOut 的训练是怎样的？好在哪？"></a>MaxOut 的训练是怎样的？好在哪？</h5><p>下面是一个神经网络的栗子，我们将激活函数换成 <em>MaxOut</em>激活函数。<br><img src="https://img-blog.csdnimg.cn/6b39a8e35978476e96d1eb7d9580c49d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上面的图 可以删除不需要的边和点，变为如下的神经网络：<br><img src="https://img-blog.csdnimg.cn/92fa0460f9a84b0689ef391fc257ade0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p><strong>Q ：如果和上图所示，那我去掉的那些边不是啥用没有？也就是这个权值压根和神经网络没关系呢，不是吗？</strong><br>答 ：当然有关系，因为我们的任务是调参，每个权值和偏值得变化，都会导致 <em>MaxOut激活函数</em> 选择的不同。比如说改变一下 <em>w</em>，他可能会选择 <em>z2</em>呢。</p>
</blockquote>
<h3 id="方法二：优化器-——-优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）"><a href="#方法二：优化器-——-优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）" class="headerlink" title="方法二：优化器 —— 优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）"></a>方法二：优化器 —— 优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）</h3><p>寻找最佳参数的方法，调节学习率的方法有（<strong>SGD、Adagrad、RMSProp、Momentum、Adam</strong>）<br>SGD、Adagrad 可以看之前关于 梯度算法进阶的部分 <a href="#">Post not found: 梯度下降算法 进阶</a><br>这边主要讲解一下 RMSProp 和 Momentum</p>
<h4 id="关于-RMSProp"><a href="#关于-RMSProp" class="headerlink" title="关于 RMSProp"></a>关于 RMSProp</h4><h5 id="为什么要使用RMSProp"><a href="#为什么要使用RMSProp" class="headerlink" title="为什么要使用RMSProp"></a>为什么要使用RMSProp</h5><p>　在 <em>Adagrad</em> 中，<font color="red">学习率是跟损失函数对 <em>w</em> 的<strong>二次微分</strong>有关。</font>那么对于图中蓝绿相交的一点来说，因为 <em>w1</em> 所在的曲率相对于 <em>w2</em> 要小，所以 <em>w1</em> 的学习率会比 <em>w2</em> 大。现在单考虑 <em>w1</em>（只看横向），那么二次微分是固定的（碗状），也就是说 <em>w1</em>是根据固定的规则去自动调整 <em>η</em> 的。但是现实中同一方向的二次微分是不固定的，因此对于同一方向去 <em>w1</em>，需要不同的规则去调 <em>η</em>。</p>
<font color=" turquoise">对于一个参数来说，*Adagrad* 是用固定的规则去调 *η*，*RMSProp* 是用变化的规则去调 *η*</font>![在这里插入图片描述](https://img-blog.csdnimg.cn/cc6becd55c104daaa6f8eddadc55eb9d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)
##### 如何实现RMSProp
在原来分母这一项中，在过去梯度平方和前面加上权值 *a*，现有的梯度平方加上 *1-a*。
<font color=" turquoise">其在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得**陡峭的方向变得平缓**，从而加快训练速度）</font>
![在这里插入图片描述](https://img-blog.csdnimg.cn/1377f4da7b564c9a89ede8f17577f241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)
#### 关于 Momentum
##### Momentum（动量）是用来解决什么问题的？
<font color="red">**这个是用来解决局部最优解的问题的**</font>

<p>说白了就是要延续他的惯性<br><img src="https://img-blog.csdnimg.cn/85f0e0d5d9a540eaa4f68d7c2e13c4ab.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h5 id="如何实现Momentum"><a href="#如何实现Momentum" class="headerlink" title="如何实现Momentum"></a>如何实现Momentum</h5><p>考虑他此时的方向，并保留他的惯性。来调整下一步，最大可能的避免局部最优解。<br><img src="https://img-blog.csdnimg.cn/0201be68528b443bb4b5596d5f27b58d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h4 id="关于-Adam"><a href="#关于-Adam" class="headerlink" title="关于 Adam"></a>关于 Adam</h4><p>他其实是一种 <em>Momentum + RMSProp</em> 结合的方法。其具体算法可以看如下图所示：<img src="https://img-blog.csdnimg.cn/c3aff899691d4155b8e47e51420c868e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="模型Train阶段OK-Test阶段Error，即过拟合"><a href="#模型Train阶段OK-Test阶段Error，即过拟合" class="headerlink" title="模型Train阶段OK Test阶段Error，即过拟合"></a>模型Train阶段OK Test阶段Error，即过拟合</h2><h3 id="方法一-：参数在过拟合之前就停止更新（Early-Stopping）"><a href="#方法一-：参数在过拟合之前就停止更新（Early-Stopping）" class="headerlink" title="方法一 ：参数在过拟合之前就停止更新（Early Stopping）"></a>方法一 ：参数在过拟合之前就停止更新（Early Stopping）</h3><font color="red">这里的testing set指的是有label的testing set（即validation set ）。</font>
如果learning rate设得对的话，training set的loss会逐渐降低，而testing set与training set可能分布不同，所以testing set的loss可能先降后升，这时就不要一直train下去，而是要在testing loss最小的地方停止train。这里的testing set 实际指的是**validation set**。

### 方法二 ：正则化Regularization

> **Q ： 首先理解什么是范数，L1（范数为1）和L2（范数为2）是什么？**
> 范数：向量在不同空间中“长度”的计算公式
> L1：绝对值之和
> L2：平方和

#### L2正则化（权值衰减）
![在这里插入图片描述](https://img-blog.csdnimg.cn/8a8facf4f6bf42ebb56f4f657102b5ed.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)
> **Q ： 为什么通常 不考虑 bias？**
> L2正则化让function更平滑，而bias与函数平滑程度没有关系。

![在这里插入图片描述](https://img-blog.csdnimg.cn/35cbdf14ab7049bd9967fcabc1223ae2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)

参数更新：
因为η、λ、n都是正的，**所以 *1−ηλ*小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来**。**当然考虑到后面的导数项，w最终的值可能增大也可能减小**。

<font color="red">正则化在NN中虽有用但不明显。</font>
NN参数初始值一般接近0，update参数即是要参数原理0。L2正则化（让参数接近0）的作用可能与early stopping类似。

> **Q : 为什么参数w衰减能防止过拟合?**
> 答 ：模型过于复杂会导致过拟合。那么越小的w（可以想象成0理解），表示网络复杂度低，越简单的网络结构，就越不会过拟合。比如模型  *y=w1×w1 + w2×w2* 的平方  中把 *w2=0* 代入，模型就会简化，就不会引起过拟合。![在这里插入图片描述](https://img-blog.csdnimg.cn/c8fc90454614466ebee4a2d5a549a25e.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)
> **Q : 对正则化解决过拟合问题的一些本质理解？**
> 正则化防止过拟合的本质：减少“没用参数”的权值（防止过拟合），同时也减少“有用参数”的权值（会增加bias误差）
> **Q : 什么是有用参数，什么是没用参数？如上图中，怎么就把 *x2* 删去，不把 *x1* 删去呢？**
> 我们姑且假设 *w1* 是有用参数， *w2* 是无用参数，由公式知参数更新值跟权值、梯度值两个因素有关，实际上，无论是 *x1* 还是 *x2* ，权值都会衰减，每update一次参数，权值 *w* 就会衰减一次，但如果是下图的情况，*损失函数Loss* 的减少跟 *w2* 没关系的，所以对其偏导为0，那么 *w2* 的参数更新只跟权值有关了，随着更新次数叠加，权值就会逐渐衰减接近0；对于***有用参数 w1*** ，虽然它权值衰减，但是它其作用的是后面的偏导值，所以它还是不会变成0的。
![在这里插入图片描述](https://img-blog.csdnimg.cn/50e060cb7bc24fbe896ef111e287092b.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)
#### L1正则化
L1是在Loss函数加上绝对值之和，求偏导后比原始的更新规则多出了η ×λ ×sgn(w)这一项。<font color="turquoise">**当w为正时，更新后的w变小。当w为负时，更新后的w变大（一加一减）——因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合**</font>
![在这里插入图片描述](https://img-blog.csdnimg.cn/113c9ac61f5946c783a434a07f8d52e9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)

#### L1和L2相比
L1 update的速度是*|ηλ||ηλ|* （划横线那项每次改变量是一定的）。而 L2 update 的速度与w的t有关（如果wt  很大，那么改变量也很大）。
用L1做training，结果比较sparse(稀疏的)，参数中有很多接近0的值，也有很多很大的值。
L2 learn的结果：参数值平均来讲比较小。

### Dropout 
**Dropout也是为了简化神经网络结构的目的**，但L1、L2正则化是通过修改代价函数来实现的，而Dropout则是通过修改神经网络本身来实现的。
#### dropout是如何实现的
在training的时候，每次update参数之前，对每一个Neuron（包括input_layer）做sampling，决定这个Neuron按一定几率p丢掉，跟它相连的weight也被丢掉，结果得到一个细长的Network。（每一次update一个mini-batch之前，拿来traing的Network structure是不一样的）。

换句话说input layer中每个element也算是一个neuron.
每次更新参数之前都要resample.
**用dropout，在training上的结果会变差，但在testing上的结果会变好。**
![在这里插入图片描述](https://img-blog.csdnimg.cn/196bfc65730b4cb6b5877bcbc0a40729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)
<font color="red">在**testing的时候不做dropout，所有neuron都要用**。
如果training时的删除神经元的概率为p%，则在testing时，所有的weight都要乘以（1-p）%</font>
![在这里插入图片描述](https://img-blog.csdnimg.cn/2c2491b39cef4452b88c9c465a8a1cbe.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)
#### Dropout的原理 
![在这里插入图片描述](https://img-blog.csdnimg.cn/27c81c02d95847c2a5776da769696c54.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)

> **Q : 为什么当在做 *testing* 的时候，weights需要都乘以 *(1-p)%*  (p是Dropout的概率)？**
>
> 答：如下图所示，左侧是在 *Traing* 阶段，*w2* 与 *w4* 由于丢失几率和丢掉(几率是这里是0.5)，假设神经元此时都是1，这时输出的应该是 *w1+w3*；右侧是在 *Testing* 阶段，在这个阶段要保证所有的神经元都不做 *Dropout* 。所以变成了w1+w2+w3+w4，约等于变成了左侧的两倍；所有取所有weight都要乘以（1-0.5）。

实际上，dropout是利用ensemble思想，把一个复杂神经网络的训练转化为，训练很多个简单的神经网络，然后再把多个简单神经网络训练出来的参数做平均。
![在这里插入图片描述](https://img-blog.csdnimg.cn/96b24fb760ee4d65b18862bc7bcf4ee0.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)
一个复杂model, bias准，但variance大，把多个复杂model ensemble起来，variance变小。
每个dropout后的结构由一个batch来train，但是权重是共享的，每个权重是由多个batch 来train的。
![在这里插入图片描述](https://img-blog.csdnimg.cn/32d269be253c4f1790664d3b5c88caca.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center)
<font color="red">**在testing的时候，把多个结构的结果取平均，与把所有参数乘以(1 - p%)，效果是近似的。**
**Dropout用在ReLU、Maxout上效果较好。**</font>

<blockquote>
<p><strong>Q : 为什么 Dropout 在testing的时候，把多个结构的结果取平均，与把所有参数乘以(1 - p%)，效果是近似的？ 而不是相同</strong><br>答：因为神经元之间的层次，使用的激活函数不一定都是线性的，只有线性的情况下才会是相同的。</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">凯凯超人</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2021/08/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/">http://example.com/2021/08/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">一只柴犬</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/">李宏毅</a><a class="post-meta__tags" href="/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/">模型优化</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/3.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/10/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81Deep%EF%BC%9F%E6%B7%B1%E8%80%8C%E4%B8%8D%E6%98%AF%E5%AE%BD/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/4.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">为什么要Deep？深而不是宽</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20Deep%20Learning%20%E5%9F%BA%E7%A1%80/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度学习 Deep Learning 基础</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/08/10/为什么要Deep？深而不是宽/" title="为什么要Deep？深而不是宽"><img class="cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-10</div><div class="title">为什么要Deep？深而不是宽</div></div></a></div><div><a href="/2021/08/07/深度学习 Deep Learning 基础/" title="深度学习 Deep Learning 基础"><img class="cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-07</div><div class="title">深度学习 Deep Learning 基础</div></div></a></div><div><a href="/2021/08/17/半监督学习 Semi-Supervised/" title="半监督学习 Semi-Supervised"><img class="cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/5.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-17</div><div class="title">半监督学习 Semi-Supervised</div></div></a></div><div><a href="/2021/08/12/卷积神经网络CNN（Convolutional Neural Network）/" title="卷积神经网络CNN（Convolutional Neural Network）"><img class="cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/14.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-12</div><div class="title">卷积神经网络CNN（Convolutional Neural Network）</div></div></a></div><div><a href="/2021/08/19/无监督学习（Unsupervised Learning）之 聚类与降维/" title="无监督学习（Unsupervised Learning）之 聚类与降维"><img class="cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-19</div><div class="title">无监督学习（Unsupervised Learning）之 聚类与降维</div></div></a></div><div><a href="/2021/08/04/分类问题 Classification 案例一：神奇宝贝是水系还是普通系？/" title="分类问题 Classification 案例一：神奇宝贝是水系还是普通系？"><img class="cover" src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/15.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-04</div><div class="title">分类问题 Classification 案例一：神奇宝贝是水系还是普通系？</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/admin.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">凯凯超人</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xxxxx" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xxxxxx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://gitee.com/×××" target="_blank" title="Gitee"><i class="iconfont gitee"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">如何优化模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%88%E4%BB%80%E4%B9%88%E6%89%8D%E6%98%AFoverfitting%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">谈什么才是overfitting？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E7%97%87%E4%B8%8B%E8%8D%AF%EF%BC%9A%E8%AE%AD%E7%BB%83error%EF%BC%8C%E6%B5%8B%E8%AF%95error%E5%88%86%E5%88%AB%E7%94%A8%E4%BB%80%E4%B9%88%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">对症下药：训练error，测试error分别用什么方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8BTrain%E9%98%B6%E6%AE%B5Error-%E5%85%B7%E4%BD%93%E8%A7%A3%E5%86%B3"><span class="toc-number">1.3.</span> <span class="toc-text">模型Train阶段Error 具体解决</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E6%8D%A2%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E9%97%AE%E9%A2%98-%E2%80%94%E2%80%94-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="toc-number">1.3.1.</span> <span class="toc-text">方法一：换激活函数的问题 —— 梯度消失</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">什么是梯度消失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">为什么会有梯度消失？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98-%E2%80%94%E2%80%94-%E6%8D%A2ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">如何解决梯度消失问题 —— 换ReLU激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#ReLU-%E4%B8%8E-MaxOut"><span class="toc-number">1.3.1.3.1.</span> <span class="toc-text">ReLU 与 MaxOut</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MaxOut-%E7%9A%84%E8%AE%AD%E7%BB%83%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F%E5%A5%BD%E5%9C%A8%E5%93%AA%EF%BC%9F"><span class="toc-number">1.3.1.3.2.</span> <span class="toc-text">MaxOut 的训练是怎样的？好在哪？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E4%BC%98%E5%8C%96%E5%99%A8-%E2%80%94%E2%80%94-%E4%BC%98%E5%8C%96gd%E5%92%8C%E8%87%AA%E9%80%82%E5%BA%94%E8%B0%83%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%88SGD%E3%80%81Adagrad%E3%80%81RMSProp%E3%80%81Momentum%E3%80%81Adam%EF%BC%89"><span class="toc-number">1.3.2.</span> <span class="toc-text">方法二：优化器 —— 优化gd和自适应调学习率（SGD、Adagrad、RMSProp、Momentum、Adam）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E-RMSProp"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">关于 RMSProp</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8RMSProp"><span class="toc-number">1.3.2.1.1.</span> <span class="toc-text">为什么要使用RMSProp</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0Momentum"><span class="toc-number">1.3.2.1.2.</span> <span class="toc-text">如何实现Momentum</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E-Adam"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">关于 Adam</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8BTrain%E9%98%B6%E6%AE%B5OK-Test%E9%98%B6%E6%AE%B5Error%EF%BC%8C%E5%8D%B3%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.</span> <span class="toc-text">模型Train阶段OK Test阶段Error，即过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80-%EF%BC%9A%E5%8F%82%E6%95%B0%E5%9C%A8%E8%BF%87%E6%8B%9F%E5%90%88%E4%B9%8B%E5%89%8D%E5%B0%B1%E5%81%9C%E6%AD%A2%E6%9B%B4%E6%96%B0%EF%BC%88Early-Stopping%EF%BC%89"><span class="toc-number">1.4.1.</span> <span class="toc-text">方法一 ：参数在过拟合之前就停止更新（Early Stopping）</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/08/22/Tensorflow%20%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/" title="Tensorflow 代码学习"><img src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/11.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Tensorflow 代码学习"/></a><div class="content"><a class="title" href="/2021/08/22/Tensorflow%20%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/" title="Tensorflow 代码学习">Tensorflow 代码学习</a><time datetime="2021-08-22T06:05:01.000Z" title="发表于 2021-08-22 14:05:01">2021-08-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/08/21/Numpy%E3%80%81Pandas%E3%80%81Matplotlib%20%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/" title="Numpy、Pandas、Matplotlib  常用代码"><img src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Numpy、Pandas、Matplotlib  常用代码"/></a><div class="content"><a class="title" href="/2021/08/21/Numpy%E3%80%81Pandas%E3%80%81Matplotlib%20%E5%B8%B8%E7%94%A8%E4%BB%A3%E7%A0%81/" title="Numpy、Pandas、Matplotlib  常用代码">Numpy、Pandas、Matplotlib  常用代码</a><time datetime="2021-08-21T02:18:01.000Z" title="发表于 2021-08-21 10:18:01">2021-08-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/08/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Unsupervised%20Learning%EF%BC%89%E4%B9%8B%20%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/" title="无监督学习（Unsupervised Learning）之 聚类与降维"><img src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无监督学习（Unsupervised Learning）之 聚类与降维"/></a><div class="content"><a class="title" href="/2021/08/19/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Unsupervised%20Learning%EF%BC%89%E4%B9%8B%20%E8%81%9A%E7%B1%BB%E4%B8%8E%E9%99%8D%E7%BB%B4/" title="无监督学习（Unsupervised Learning）之 聚类与降维">无监督学习（Unsupervised Learning）之 聚类与降维</a><time datetime="2021-08-19T14:36:01.000Z" title="发表于 2021-08-19 22:36:01">2021-08-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/08/17/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20Semi-Supervised/" title="半监督学习 Semi-Supervised"><img src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="半监督学习 Semi-Supervised"/></a><div class="content"><a class="title" href="/2021/08/17/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20Semi-Supervised/" title="半监督学习 Semi-Supervised">半监督学习 Semi-Supervised</a><time datetime="2021-08-17T14:36:01.000Z" title="发表于 2021-08-17 22:36:01">2021-08-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/08/12/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%EF%BC%88Convolutional%20Neural%20Network%EF%BC%89/" title="卷积神经网络CNN（Convolutional Neural Network）"><img src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/14.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="卷积神经网络CNN（Convolutional Neural Network）"/></a><div class="content"><a class="title" href="/2021/08/12/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%EF%BC%88Convolutional%20Neural%20Network%EF%BC%89/" title="卷积神经网络CNN（Convolutional Neural Network）">卷积神经网络CNN（Convolutional Neural Network）</a><time datetime="2021-08-12T14:36:01.000Z" title="发表于 2021-08-12 22:36:01">2021-08-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 凯凯超人</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/js/pool.min.js"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"><script src="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/js/pool.min.js"></script><script src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"></script><script src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/hideMobileSidebar.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>